[{"authors":["admin"],"categories":null,"content":"I am a teaching-focused Senior Lecturer and the Deputy Director Education for the School of Psychology and Neuroscience at the University of Glasgow. My research predominantly focuses on lecture capture, how it can be used as an effective study tool by students and the impact on students from widening participation backgrounds as well as those with disabilities and neurodivergent conditions. In all my work, I draw on theories of learning from cognitive science and self-regulation, as well as theories of belonging and self-efficacy.\nMy leadership roles have centered around supporting those on the learning, teaching, and scholarship track and I am centre head for the Pedagogy and Education Research Unit in the School of Psychology and Neuroscience, as well as the lead of the College of MVLS LTS Network.\nMy teaching is varied although centres on cognitive psychology and beginner data skills in R. I am also Year Lead for our Level 1 undergraduate cohort, an admin role that I have held for the majority of my career and that has informed my research practice greatly.\nFinally, I am a vocal advocate of open science and open educational resources and strive to incorporate them into both my teaching and my research design. As a member of the PsyTeachR team I have authored several open-access data skills books and tutorials.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/emily-nordmann/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/emily-nordmann/","section":"authors","summary":"I am a teaching-focused Senior Lecturer and the Deputy Director Education for the School of Psychology and Neuroscience at the University of Glasgow. My research predominantly focuses on lecture capture, how it can be used as an effective study tool by students and the impact on students from widening participation backgrounds as well as those with disabilities and neurodivergent conditions.","tags":null,"title":"Emily Nordmann","type":"authors"},{"authors":[],"categories":[],"content":"\rThis website accompanies the Psychology of Swearing talk held on Friday 19th April at UWS.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:\nBooks\n\rBenjamin Bergen – What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves. Amazon link.\rSteven Pinker - The Stuff of Thought. Amazon link.\rEmma Byrne - Swearing is Good For You. Amazon link.\r\rWebsites\nhttps://stronglang.wordpress.com/\nhttps://twitter.com/stronglang\nJournal papers \u0026amp; pre-prints\nBeattie, H., Robertson, O., Horlin, C., Spaeth, E., \u0026amp; Nordmann, E. (2022). Gendered attitudes towards swearing: a trans-inclusive mixed methods study of social identity, swearing norms, and behaviour.Preprint\nFeldman, G., Lian, H., Kosinski, M., \u0026amp; Stillwell, D. (2017) Frankly, we do give a damn: The relationship between profanity and honesty. Social Psychological and Personality Science\nHarris, C. L., Aycicegi, A., \u0026amp; Gleason, J. B. (2003). Taboo words and reprimands elicit greater autonomic reactivity in a first language than in a second language. Applied Psycholinguistics, 24(04), 561-579.\nJay, T. (2009). The utility and ubiquity of taboo words. Perspectives on Psychological Science, 4(2), 153-161.\nJay, T., \u0026amp; Janschewitz, K. (2008). The pragmatics of swearing. Journal of Politeness Research. Language, Behaviour, Culture, 4(2), 267-288.\nJay, K. L., \u0026amp; Jay, T. B. (2013). A child’s garden of curses: A gender, historical, and age-related evaluation of the taboo lexicon. The American Journal of Psychology, 126(4), 459-475.\nJay, K. L., \u0026amp; Jay, T. B. (2015). Taboo word fluency and knowledge of slurs and general pejoratives: deconstructing the poverty-of-vocabulary myth. Language Sciences, 52, 251-259.\nMackay, D. G., Shafto, M., Taylor, J. K., Marian, D. E., Abrams, L., \u0026amp; Dyer, J. R. (2004). Relations between emotion, memory, and attention: Evidence from taboo Stroop, lexical decision, and immediate memory tasks. Memory \u0026amp; Cognition, 32(3), 474-488.\nStephens, R., Atkins, J., \u0026amp; Kingston, A. (2009). Swearing as a response to pain. Neuroreport, 20(12), 1056-1060.\nVan Lancker, D., \u0026amp; Cummings, J. L. (1999). Expletives: Neurolinguistic and neurobehavioral perspectives on swearing. Brain research reviews, 31(1), 83-104.\n\rRelationship between age and vocabulary\rThe correlation between age and animals is r = -0.23, p = 0.129.\nThe correlation between age and swears is r = -0.12, p = 0.323.\nThe correlation between swears and animals is r = 0.3, p = 0.036.\n\rVocabulary scores by gender\rGeneral fluency\r\r\rgender\raverage_score\r\r\r\rMan\r9.30\r\rWoman\r10.39\r\rNonbinary\r12.00\r\r\r\r\n\rSwearing\r\r\rgender\raverage_score\r\r\r\rMan\r7.00\r\rWoman\r6.61\r\rNonbinary\r8.50\r\r\r\r\n\r\rMost frequent swear words\r\rWord cloud\r\r","date":1713312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713357097,"objectID":"064df933342e13a8abfa08415d5b85ac","permalink":"/post/psychology-of-swearing-uws/","publishdate":"2024-04-17T00:00:00Z","relpermalink":"/post/psychology-of-swearing-uws/","section":"post","summary":"This website accompanies the Psychology of Swearing talk held on Friday 19th April at UWS.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:","tags":[],"title":"Psychology of Swearing UWS","type":"post"},{"authors":[],"categories":[],"content":"\rThis website accompanies the Psychology of Swearing talk held on Wednesday 20th March at Mary Immaculate College.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:\nBooks\n\rBenjamin Bergen – What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves. Amazon link.\rSteven Pinker - The Stuff of Thought. Amazon link.\rEmma Byrne - Swearing is Good For You. Amazon link.\r\rWebsites\nhttps://stronglang.wordpress.com/\nhttps://twitter.com/stronglang\nJournal papers \u0026amp; pre-prints\nBeattie, H., Robertson, O., Horlin, C., Spaeth, E., \u0026amp; Nordmann, E. (2022). Gendered attitudes towards swearing: a trans-inclusive mixed methods study of social identity, swearing norms, and behaviour.Preprint\nFeldman, G., Lian, H., Kosinski, M., \u0026amp; Stillwell, D. (2017) Frankly, we do give a damn: The relationship between profanity and honesty. Social Psychological and Personality Science\nHarris, C. L., Aycicegi, A., \u0026amp; Gleason, J. B. (2003). Taboo words and reprimands elicit greater autonomic reactivity in a first language than in a second language. Applied Psycholinguistics, 24(04), 561-579.\nJay, T. (2009). The utility and ubiquity of taboo words. Perspectives on Psychological Science, 4(2), 153-161.\nJay, T., \u0026amp; Janschewitz, K. (2008). The pragmatics of swearing. Journal of Politeness Research. Language, Behaviour, Culture, 4(2), 267-288.\nJay, K. L., \u0026amp; Jay, T. B. (2013). A child’s garden of curses: A gender, historical, and age-related evaluation of the taboo lexicon. The American Journal of Psychology, 126(4), 459-475.\nJay, K. L., \u0026amp; Jay, T. B. (2015). Taboo word fluency and knowledge of slurs and general pejoratives: deconstructing the poverty-of-vocabulary myth. Language Sciences, 52, 251-259.\nMackay, D. G., Shafto, M., Taylor, J. K., Marian, D. E., Abrams, L., \u0026amp; Dyer, J. R. (2004). Relations between emotion, memory, and attention: Evidence from taboo Stroop, lexical decision, and immediate memory tasks. Memory \u0026amp; Cognition, 32(3), 474-488.\nStephens, R., Atkins, J., \u0026amp; Kingston, A. (2009). Swearing as a response to pain. Neuroreport, 20(12), 1056-1060.\nVan Lancker, D., \u0026amp; Cummings, J. L. (1999). Expletives: Neurolinguistic and neurobehavioral perspectives on swearing. Brain research reviews, 31(1), 83-104.\n\rRelationship between age and vocabulary\rThe correlation between age and animals is r = -0.4, p = 0.218.\nThe correlation between age and swears is r = -0.12, p = 0.652.\nThe correlation between swears and animals is r = 0.47, p = 0.106.\n\rVocabulary scores by gender\rGeneral fluency\r\r\rgender\raverage_score\r\r\r\rMan\r13.57\r\rWoman\r13.90\r\rNonbinary\r15.50\r\r\r\r\n\rSwearing\r\r\rgender\raverage_score\r\r\r\rMan\r9.29\r\rWoman\r7.70\r\rNonbinary\r6.50\r\r\r\r\n\r\rMost frequent swear words\r\rWord cloud\r\r","date":1710892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710942671,"objectID":"3e95db982e597e33bfff08ef3721b476","permalink":"/post/psychology-of-swearing-mic/","publishdate":"2024-03-20T00:00:00Z","relpermalink":"/post/psychology-of-swearing-mic/","section":"post","summary":"This website accompanies the Psychology of Swearing talk held on Wednesday 20th March at Mary Immaculate College.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:","tags":[],"title":"Psychology of Swearing  - MIC","type":"post"},{"authors":[],"categories":[],"content":"\rThis website accompanies the Psychology of Swearing talk held on Wednesday 28th February for the University of Glasgow Psychology Society.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:\nBooks\n\rBenjamin Bergen – What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves. Amazon link.\rSteven Pinker - The Stuff of Thought. Amazon link.\rEmma Byrne - Swearing is Good For You. Amazon link.\r\rWebsites\nhttps://stronglang.wordpress.com/\nhttps://twitter.com/stronglang\nJournal papers \u0026amp; pre-prints\nBeattie, H., Robertson, O., Horlin, C., Spaeth, E., \u0026amp; Nordmann, E. (2022). Gendered attitudes towards swearing: a trans-inclusive mixed methods study of social identity, swearing norms, and behaviour.Preprint\nFeldman, G., Lian, H., Kosinski, M., \u0026amp; Stillwell, D. (2017) Frankly, we do give a damn: The relationship between profanity and honesty. Social Psychological and Personality Science\nHarris, C. L., Aycicegi, A., \u0026amp; Gleason, J. B. (2003). Taboo words and reprimands elicit greater autonomic reactivity in a first language than in a second language. Applied Psycholinguistics, 24(04), 561-579.\nJay, T. (2009). The utility and ubiquity of taboo words. Perspectives on Psychological Science, 4(2), 153-161.\nJay, T., \u0026amp; Janschewitz, K. (2008). The pragmatics of swearing. Journal of Politeness Research. Language, Behaviour, Culture, 4(2), 267-288.\nJay, K. L., \u0026amp; Jay, T. B. (2013). A child’s garden of curses: A gender, historical, and age-related evaluation of the taboo lexicon. The American Journal of Psychology, 126(4), 459-475.\nJay, K. L., \u0026amp; Jay, T. B. (2015). Taboo word fluency and knowledge of slurs and general pejoratives: deconstructing the poverty-of-vocabulary myth. Language Sciences, 52, 251-259.\nMackay, D. G., Shafto, M., Taylor, J. K., Marian, D. E., Abrams, L., \u0026amp; Dyer, J. R. (2004). Relations between emotion, memory, and attention: Evidence from taboo Stroop, lexical decision, and immediate memory tasks. Memory \u0026amp; Cognition, 32(3), 474-488.\nStephens, R., Atkins, J., \u0026amp; Kingston, A. (2009). Swearing as a response to pain. Neuroreport, 20(12), 1056-1060.\nVan Lancker, D., \u0026amp; Cummings, J. L. (1999). Expletives: Neurolinguistic and neurobehavioral perspectives on swearing. Brain research reviews, 31(1), 83-104.\n\rRelationship between age and vocabulary\rThe correlation between age and animals is r = -0.2, p = 0.619.\nThe correlation between age and swears is r = 0.11, p = 0.619.\nThe correlation between swears and animals is r = 0.26, p = 0.556.\n\rVocabulary scores by gender\rGeneral fluency\r\r\rgender\raverage_score\r\r\r\rMan\r14.60\r\rWoman\r17.23\r\r\r\r\n\rSwearing\r\r\rgender\raverage_score\r\r\r\rMan\r10.20\r\rWoman\r9.14\r\r\r\r\n\r\rMost frequent swear words\r\rWord cloud\r\r","date":1708992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709048058,"objectID":"b6e732ce4a48550730231298214cd18d","permalink":"/post/psychology-of-swearing-feb-2024/","publishdate":"2024-02-27T00:00:00Z","relpermalink":"/post/psychology-of-swearing-feb-2024/","section":"post","summary":"This website accompanies the Psychology of Swearing talk held on Wednesday 28th February for the University of Glasgow Psychology Society.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:","tags":[],"title":"Psychology of Swearing - Feb 2024","type":"post"},{"authors":[],"categories":["chatgpt"],"content":"\rPart 1: Using ChatGPT to create teaching materials: Data simulation \u0026amp; MCQs\nPart 2: Using ChatGPT to create teaching materials: marking criteria \u0026amp; rubrics\nPart 3: ChatGPT: Student guidance for multiple-choice assessments\nPart 4: Using ChatGPT for learning \u0026amp; teaching: Correcting auto-transcripts\nYou could argue that these blogs are an exercise in procrastination and if you spend any time with me in real life, you’ll be questioning why I’m writing all these whilst constantly moaning about workload. If I’m completely honest they are part-procrastination, but, genuinely, I’m much further along than if I hadn’t spent the time on these blogs as the process of writing has helped shaped my thoughts.\nI’ve been working on the guidance for my Level 1 essay and in some ways, writing guidance for MCQs was more problematic in that ChatGPT can get them all right very quickly. But in other ways, what is and is not academic misconduct was so clear with MCQs and it’s so much harder to draw the lines in the sand when it comes to an essay and provide good advice on how students can and should use AI. I’m so much less sure of myself with the essay guidance. I’ve structured it under three subheadings although this may well change between now and September (and feedback and ideas are most welcome!). I think it’s starting to look ok but my sense of unease remains.\nWhat use of AI would be regarded as academic misconduct for this assessment?\r\rUsing AI to write any part of your essay for you from scratch is academic misconduct and if we suspect you have used it in this way, we will report you for academic misconduct.\n\r\rAn easy rule of thumb to follow for the essay is do not copy and paste anything from an AI into your essay. This doesn’t avoid every problem, and there are exceptions to the rule, but if you want a simple rule to follow, that’s a good starting point.\n\r\rUse AI to help you refine and edit your essay, do not use it to write the essay. When in doubt, come and ask talk to us in office hours - you will not get in trouble for asking how to do things right!\n\rIt’s all shades of grey but I wanted to provide as much clarity as I could. The “don’t copy and paste” rule isn’t perfect but it probably works well enough to give the right idea. By explicitly noting they may not be sure and could reach out for help, I also wanted to highlight that this isn’t an exact art and it’s not wrong for them to have questions.\n\rHow can I use AI to help support my learning for this assessment?\r\rYou can use specific AI tools to help with your literature search (but do not use ChatGPT for this! See below for more info):\n\r\rTools like Elicit or Connected Papers can find relevant papers without perfect keyword match, summarise takeaways from the paper specific to your question, and extract key information from the papers. It’s not academic misconduct to use Elicit because you still have to read the papers, understand the information, and integrate it into your essay.\n\r\rYou can use tools like ChatGPT to get feedback and use it to help you refine and edit (but not write!) your essay. For example, you can use it to get feedback on your draft essay plan. This is a good use of AI because you’re using it to get feedback but you’re playing an absolutely active role in the creation of your essay.\n\r\rExample prompt: Act as an expert psychology tutor. I am a first year undergraduate psychology student and have to write an essay on the question [give it your essay question]. Here is the marking criteria [give it the marking criteria]. And here is a draft structure I have written to plan my essay [give it your essay plan]. I want you to give me feedback on how I can improve my essay plan, but before you give any feedback, ask me up to 5 questions about my plan and use my answers to inform your feedback.\n\r\rYou can also use it to give you feedback on your draft essay before you submit it. As with using it for feedback on your plan, this is a good use of AI because you’re using it in an active way and any changes to your essay will be your own work, you’re just getting direction form it.\n\r\rExample prompt: Act as an expert psychology tutor. am a first year undergraduate psychology student and have to write an essay on the question [give it your essay question]. Here is the marking criteria [give it the marking criteria]. Give me feedback on my essay for each of the marking criteria.\n\rTo test out using it for feedback I got ChatGPT to write me a draft essay plan and essay and then I fed the snake it’s own tail and asked for feedback in a new chat window. The feedback it produced was not always up to what a human would produce (although let’s face it, that depends on the human) and there’s a need to urge caution but as a supplement rather than a replacement, I think this would be a useful form of formative feedback - here’s an example of the feedback it gave aligned with the marking criteria:\n\r3.2 Ensure that the essay has a logical structure and that the paragraphs flow together to form a coherent narrative: The structure of your essay is good, with an introduction, main body, and conclusion. The narrative flow between paragraphs is generally effective, but there are a few places where transitions could be smoother. Additionally, some paragraphs may be too long or too short, affecting the overall coherence of the essay. Consider reorganizing and editing for better flow and readability.\n\rIt is quite general but I would prefer general to overly specific and the important thing here is that ChatGPT isn’t doing any of their work, they still have to action the feedback. I would also be really interested to know if there’s a difference in how students respond to AI vs human feedback - I would imagine they’d be more likely to assume the human was right but also coming from an AI probably strips the feedback of its emotion which might make them more likely to process it? Interesting times ahead.\nI also wanted to address the use of ChatGPT for editing. Most of my thoughts on this came out of trying to use it for correcting auto-transcripts where I really struggled to get it to do exactly what I wanted. My key concern here is students giving it their essay and then whether they wanted it to or not, ChatGPT editing the entire thing and changing the meaning of what they’ve written and stripping their writing of any individuality. I want them to make active, conscious choices and to learn from the editing process not just take what it gives them without thinking.\n\rFinally, you can use it to help you proof-read and edit. Be very careful to ask it to provide you with line-by-line changes with the comparison so that you can see what it has changed and why, because it means you’re making an active choice and it will also help you learn about how to improve your writing.\n\r\rBefore you use AI for editing, remember that it is important that you develop your own voice as a writer and if you constantly use AI to edit you’ll end up with a bland, generic writing style. If everyone does this we’re going to end up with a world written in beige. Don’t be beige. Also remember that just because the AI suggests an edit, doesn’t mean it is better than what you’ve written! Even with the below prompts it may edit more than you ask it to - if it does this and edits huge paragraphs rather than individual sentences, ignore it and click regenerate response - you want it to give you the edits line-by-line so that you can compare them and choose the ones you think are actually an improvement.\n\r\rExample prompt: Act as an expert academic writing tutor. I have written an undergraduate university essay and I would like you to review it with regards to spelling, grammar, concise writing, APA formatting, and academic tone. Provide suggestions for edits I can make sentence-by-sentence. Provide the original and your edited suggestion so that I can compare them. Explain why you have suggested the edit [give it your essay]:\n\r\rExample prompt if English is not your first language: Act as an expert academic writing tutor. My first language is [your first language]. I have written an undergraduate university essay and I would like you to review it with regards to ensuring that the writing follows the grammar rules and conventions of British English. Provide suggestions for edits I can make sentence-by-sentence. Provide the original and your edited suggestion so that I can compare them. Explain why you have suggested the edit [give it your essay]\n\rI am terrified that I am pointing them towards a tool that will potentially harm their development as writers, either because they will blindly copy and paste and end up changing the meaning of what they intended, or that we’ll end up in a world where everyone has the same generic A-corrected writing.\nBut, abstinence as a policy isn’t going to work - if we don’t guide them they’re going to do it anyway, my logic here is that at least this way I can make them aware of the potential pitfalls.\nI am also very likely romanticising the need to develop an individual writing voice based on my own feelings about writing. As I write this I’ve got George Orwell’s rules for writing swirling around in my head, the fact that I know them by heart should tell me I might have crossed the line into wanky academic. Maybe losing some individuality is the price we pay for massively improving the clarity of most people’s writing (I vehemently don’t believe this but I feel like I should at least provide the other side of the argument)?.\n\rWhat are the limitations of using AI for this assessment and what should I be careful of aside from academic misconduct?\rIn addition to the above caveats, I have a standard heading about limitations to try and get them thinking more critically about AI and not just focus on the issue of misconduct.\n\rDo not use AI tools like ChatGPT to do your research. Use tools like Scopus, Web of Science, Google Scholar, ad Elicit and read the papers yourself. The reasons for this are that:\ra) if you rely on AI summaries you won’t develop the depth of understanding you need at university level and your essay will be shallow\rb) reading papers helps you develop an academic tone in your writing\rc) sometimes it tells you things that are simply wrong, including making up references that don’t exist\rd) it doesn’t have access to the most recent research.\n\r\rIf you use it for feedback, it may give you advice that isn’t very good and if you’re not super careful about ensuring you’ve given it all the guidance from the labs and your tutor, it may end up telling you to do something that contradicts what we’ve told you to do. If you do use AI to help refine your structure, we’d strongly encourage you to come to office hours or GTA sessions to check you’re on the right track.\n\r\rAI writing is frequently formulaic, uninspiring, and often doesn’t have the right tone.\n\r\rWhen you ask it to suggest edits for your work, you have to be really careful to review its suggestions because sometimes it will change the core meaning of what you’ve written rather than just correcting the grammar or tone. Always ask it to give you suggestions sentence-by-sentence and if it doesn’t do this, try changing the prompt or regenerate the response until it does. Remember that your version might be better than than the AI - trust yourself!\n\r\rAnother use case we’ve seen is using AI to correct your references into APA formatting, but having tried this, it didn’t do a very good job so we’d suggest using a reference manager like Zotero instead if you want something to help automate that process (although you still need to check them).\n\rIs this the best approach? I’m not sure. But I am fairly sure that right now, at scale, honesty and transparency is the only tool we’ve got that’s of any use. I know that some students will abuse it, and some will likely take my guidance to abuse it even more effectively. But I have always believed that we shouldn’t design learning \u0026amp; teaching around the lowest common denominator and that the overwhelming majority of students want to, and do, do things as intended so we should give them what information we do have whilst we’re still figuring it out on our side.\nGod I hope I’m right.\nAbout halfway through writing this blog I got the song “Walk through the fire” from the Buffy musical stuck in my head like the millennial I am so let’s end with that.\nWill this do a thing to change it?\rAm I leading them to danger?\rAre we all too far gone to care?\n\r\r","date":1690848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690921256,"objectID":"3fe2609ddd85f03c05041be819cf8052","permalink":"/post/chatgpt-student-guidance-for-essays/","publishdate":"2023-08-01T00:00:00Z","relpermalink":"/post/chatgpt-student-guidance-for-essays/","section":"post","summary":"Part 1: Using ChatGPT to create teaching materials: Data simulation \u0026amp; MCQs\nPart 2: Using ChatGPT to create teaching materials: marking criteria \u0026amp; rubrics\nPart 3: ChatGPT: Student guidance for multiple-choice assessments","tags":["chatgpt"],"title":"ChatGPT: Student guidance for essays","type":"post"},{"authors":[],"categories":[],"content":"\rAlongside Wil Tovio and Rachel O’Neill, I’ve written before in the\rTimes\rHigher\rabout the problems that requiring academics to produce corrected\rcaptions introduces. If you don’t do it, you disadvantage a wide range\rof students and effectively ruin the education of those who are d/Deaf\rand hard-of-hearing. If you do, you put a much higher workload burden on\rany lecturer with a “non-standard” accent, those with poorer quality\rrecording equipment, and let’s honest, those who just care a bit more. I\rsay all of this whilst recognising my own privilege: most recording\rsoftware generally picks up what I am saying accurately and through both\rwork and personal means, I invested in a high-quality mic at the start\rof covid.\nCorrecting transcripts\rBut anyway. It just occurred to me that one use of ChatGPT that falls\runder the category “let’s make the admin associated with teaching\reasier” is that we might be able to use it to edit transcriptions for\rus. And for once, I actually have the means to evaluate how it’s done in\rthe form of an original automated-transcription, the corrected version a\rhuman spent hours on, and the audio. The video in question was a\rwalkthrough of a data skills tutorial in R, and platforms like Zoom and\rEcho360 tend to struggle with the technical terminology.\nI downloaded the original transcript as a .txt file from Echo360 and put\rthe following prompt into ChatGPT. I’ve got a paid subscription and\raccess to GPT4.0 but I chose to use GPT3.5 as that’s the one most people\rhave access to.\n\rAct like an expert in the programming language R. I have recorded a\rlecture which is a walkthrough video of a tutorial in R. The learning\routcomes are: Be able to reshape data between long and wide formats;\rSeparate, change, reorder, and rename columns; Use pipes to chain\rtogether functions. Recording software has automatically transcribed\rthe recording but there are many errors. Below is the transcript,\rcorrect the transcription errors.\n\rThe full transcript is much longer than ChatGPT can take in one response\rso you either have to do it bit by bit, or tell it you’re going to split\rthe document and not to proceed until you tell it you’re finished. For\rthe purposes of this tutorial, I just copied and pasted what I thought\rwould fit.\nOnce it produced the corrected transcript I then asked:\n\rTell me what you changed and give me the original and the corrected\rversion so I can easily compare them.\n\rIt gave me a list of 7 changes it had made, I won’t list them all but\rhow did it do?\n\rOriginal: “So now that we have looked that in and will open up a\rnew a junk…”\n\rCorrected by ChatGPT: “So now that we have loaded that in, let’s\ropen up a new chunk…”\n\rCorrected by human: “So now that we have locked that in, I will\ropen up a new a chunk”\r\rListening to the audio, you could sensibly conclude it was either\r“loaded” or “locked”, (the human doing the editing wasn’t the human\rdoing the lecture) and both make sense in context. For the second part\rof the sentence, the audio is a bit disfluent, both the human and\rChatGPT have corrected it to what was clearer the intent of the sentence\rand whilst they’re different, they both work.\n\rOriginal: “Let’s compare this to a version of this table, but in\rcompletely tiny format”\rCorrected by ChatGPT: “Let’s compare this to a version of this\rtable but in a completely tidy format”\n\rCorrected by human: “Let’s compare this to a version of this\rtable, but in completely tidy format.”\r\rChatGPT and the human produced the same correction.\n\rOriginal:“…we now have three columns here where each column is\rits own variable one just four items, one just for the price per\ritem and one just for the total price, and they are distinguished by\rthis for scoring here.”\n\rCorrected by ChatGPT: “…we now have three columns here: one\rjust for items, one just for the price per item, and one just for\rthe total price, and they are distinguished by this ‘year’ column.”\n\rCorrected by human: “we now have three columns here where each\rcolumn is its own variable one just for items, one just for the\rprice per item and one just for the total price, and they are\rdistinguished by this fourth column here - year.\r\rChatGPT has removed a few words “here each column is its own variable”\rwhich in this case doesn’t really affect the overall meaning in the\rcontext of the wider section but if it’s going to do that you’d want to\rmake sure you’re checking each correction.\n\rThe whole truth and nothing but the truth\rWhilst it gave me 7 corrections, I could see that it had made more than\rthe initial list it gave so I followed up:\n\rThese aren’t all the changes, tell me everything you changed\n\rAnd it then produced a list of 18 changes, apologising for the\roversight. So I probed further:\n\rIs this a complete list of all changes now or are there more?\n\rIt then gave me an additional extra change. Hmmm. The text file dumps\rthe transcription into one big block of text, but Echo360 also gives the\roption to download the .VTT file that has it line-by-line with\rtime-stamps e.g.:\n\r00:00:21.110 –\u0026gt; 00:00:24.680 \u0026lt;v Speaker 0\u0026gt;So as usual, we’re going\rto start off by creating\n\r\rNOTE CONF {“raw”:[100,100,100,52,93,100,100,100,100,100]} 00:00:24.680\r–\u0026gt; 00:00:27.860 \u0026lt;v Speaker 0\u0026gt;a new project for this chapter so\rthat we can\n\r\rNOTE CONF {“raw”:[100,100,94,100,100,100,100,100,100,100]}\r00:00:27.860 –\u0026gt; 00:00:29.570 \u0026lt;v Speaker 0\u0026gt;work through things\rtogether.\n\rI thought that maybe chunking the text a bit would help it be able to\ridentify the changes but all it did was render the correction completely\ruseless. Splitting up the text with the time stamps appears to stop it\rbeing able to parse it properly which is interesting in a way that makes\rme realise I’ve got no idea what’s going on under the hood.\n\rNo single truth\rMy internet acted up and I couldn’t access the chat I was having for\rthis blog so I redid the prompt in a new chat, with the same prompt and\rsection of the script.\nThis time it gave me 33 changes. Some of them were the same, some of\rthem were different. Which is not surprising because that’s how ChatGPT\rworks, it’s all prediction and you can use the regenerate response\roption to get a slightly different version if you’re not happy with\rwhatever it has produced. But in the context of transcription, it’s a\rreally useful reminder that it isn’t “correcting” it, it’s doing what it\rnormally does which is predicting what word should come next. It doesn’t\rhave the source audio, it’s not doing what an underpaid human would be\rdoing.\nIt’s possible that had I initially asked it “give me a complete list of\rall changes, leave nothing out”, it wouldn’t have missed any. But also,\rit is well-known that you can “trick” ChatGPT into thinking it’s wrong\rjust by telling it that it is:\nSo it could be that my follow-up prompts insisting it had missed\rsomething resulted in it making up new prompts to satisfy the monkey at\rits typewriter. In a nutshell, you can’t use ChatGPT to verify what\rChatGPT has produced. The snake will eat its own tail.\nMore worryingly, in additional attempts both with 3.5 and GPT4.0, it\rstarted editing more than you’d want for a transcription correction. For\rexample:\n\rBefore we go any further here, I’m going to to switch to year. As you\rcan see, we’re going to switch to this year. So hopefully you are now\rseeing my internet browser,\n\rConsulting the audio, this should be “I’m going to switch to share, as\ryou can see, I’m going to switch to this here. So hopefully you are now\rseeing my internet browser”. It’s not a sentence that makes a great deal\rof sense without the video (which is describing changing what is being\rshared on the screen) but that’s what you’d want the transcript to say\rbecause alongside the video it does make sense.\nThis is what ChatGPT changed it to:\n\rBefore we delve deeper, I’m going to switch screens. You should now\rsee my internet browser.\n\rWhich makes a lot more sense except for the fact it doesn’t actually\rrepresent what was said.\n\rMaybe we’re asking the wrong questions\rThis feels like a task ChatGPT should be able to perform so I became\rslightly obsessed and starting trying different prompts, convinced that\rmaybe the issue was that I wasn’t being specific enough:\n\rAct like a video editor who is an expert in the programming language R\rwho has been asked to correct a transcript for the subtitles of a\rrecorded lecture which is a walkthrough video of a tutorial in R.\n\r\rThe learning outcomes are: Be able to reshape data between long and\rwide formats; Separate, change, reorder, and rename columns; Use pipes\rto chain together functions.\n\r\rRecording software has automatically transcribed the recording but\rthere are many errors where the transcription software has not\raccurately assessed what word has been said. Below is the transcript,\redit all words that are likely to be transcription errors so that they\rcan be used as subtitles. Do not edit anything that is not likely to\rbe an error and do not paraphrase or change the meaning.\n\rThis seemed to keep to the brief of not changing the meaning a lot\rbetter although it was perhaps a little too conservative (but if the\roption is change too much or too little, perhaps that’s for the best).\rAdditionally it didn’t get everything right (e.g., 2 and 4 aren’t right\rbut I suppose they’re no more wrong than the original automated\rtranscript so it is at least not changing things that aren’t wrong).\n\rIs this any use?\rThe question is then, given all these issues, is this any use? The edits\rit produced on my first attempt were really very impressive and reading\rthrough the edited transcript, it all made sense and I was getting very\rexcited. But as I kept going I got more and more cautious. In some cases\rit’s not necessarily problematic that it wasn’t a one-to-one correction,\rthe human also made some choices that deviated from an exact script to\rmake it make sense, but without a lot of work on the prompt in some\rcases ChatGPT was paraphrasing way beyond the original intent and\rmeaning. I was forced to remind myself that it’s not “correcting” words\rand it doesn’t have access to the audio. Additionally, it’s very\rdifficult to get it to tell you everything it changed so you absolutely\rcouldn’t use this without verifying it.\nOn my first attempt, the amount it got right would hugely cut-down on\rthe time it takes to correct a transcript and a more specific prompt\rseemed to solve some of the issues with paraphrasing. It was certainly\rstill better than the automated transcript, so one possible option could\rbe to take the original, run it through ChatGPT, and then get a human to\rcorrect the ChatGPT version. That way, you make the workload more\rmanageable, but you still have human eyes on it.\nI think whether or not it is worth it probably depends on how much\rtranscription you have to do. If you have hours and hours of recorded\rcontent to transcribe then it is probably worth training ChatGPT to do\rexactly what you want and to take the time to build the prompts and find\ra balance you’re happy with because in the long-run it will still save\rhuge amounts of time. However, if I had a single video, I’m not sure I\rwould currently bother as it probably takes enough work to get it right\rthan it does just to do it manually.\nThis blog feels like a stream-of-consciousness but what this process has\rdone is change the way I would approach using ChatGPT to edit anything.\rI’m currently working on guidance for essay writing for students and I\rthink my experience here has taught me that I wouldn’t ask it to edit\ranything directly but instead to give suggestions alongside the\roriginal. For the purposes of transcription correction, that process\rmakes it time-consuming but for an essay or any other piece of writing,\rit would ensure you’re making active choices.\nAnother consideration is privacy. If you upload your transcripts, you’re\ressentially giving OpenAI your lecture to help train its LLM unless you\rchange the default\rsettings.\rWhether you care about that is up to you, but make a conscious choice.\nAnd finally, none of this changes the fact that the problem with the\rworkload involved with transcription will still be higher for people\rworking in their second language and those who have regional accents and\rthat academic workload modelling is a complete joke.\n\r","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690124529,"objectID":"757465e4eaff2f271a8fc3644ad18413","permalink":"/post/using-chatgpt-for-learning-teaching-correcting-auto-transcripts/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/post/using-chatgpt-for-learning-teaching-correcting-auto-transcripts/","section":"post","summary":"Alongside Wil Tovio and Rachel O’Neill, I’ve written before in the\rTimes\rHigher\rabout the problems that requiring academics to produce corrected\rcaptions introduces. If you don’t do it, you disadvantage a wide range\rof students and effectively ruin the education of those who are d/Deaf\rand hard-of-hearing.","tags":[],"title":"Using ChatGPT for learning \u0026 teaching: Correcting auto-transcripts","type":"post"},{"authors":[],"categories":[],"content":"\rPart 1: Using ChatGPT to create teaching materials: Data simulation \u0026amp; MCQs\nPart 2: Using ChatGPT to create teaching materials: marking criteria \u0026amp; rubrics\nThe last two blogs I’ve focused on how I have been using ChatGPT to help me create teaching materials, there’s likely more coming on that but for this blog I am going to take a detour into what I’ve been working on for student guidance.\nI’d like to state something before accidentally being taken too seriously, I hope this is already clear but it’s worth saying out loud - whilst I am hopefully an expert in learning and teaching, I am not an expert in generative AI, I just have techno-joy and have been playing around and musing.\nWhen trying to create guidance for my students, I had two underlying ideas guiding my approach:\nAs per the Russell Group principles, I strongly believe it’s my job as an individual educator and our job as a sector to guide students how to use AI appropriately. Putting our head in the sand and telling them not to use it is shortsighted and stupid. I very much enjoyed reading this paper “A New Era of Learning: Considerations for ChatGPT as a Tool to Enhance Statistics and Data Science Education” that likens ChatGPT to the introduction of the calculator. Don’t be the person saying we shouldn’t use calculators.\rI am not changing any of my assessments this year, I am just updating the guidance. There are two reasons for this. First, I am still not over the last three consecutive years I had to rip up my course and start again. I was a much younger woman when covid hit and I just don’t have the energy for another reimgaining. Second, and the reason I’m probably more likely to state in rooms where I could get fired, I don’t think I, or the majority of people working in academia, understand the ramifications of generative AI enough to redesign our assessments right now. I could put a huge amount of work into redesigning something that was just as susceptible to AI use and/or the capabilities of AI could change overnight. My tactic instead is to upskill both myself and my students and take the next year to figure out a sensible approach and what is and what is not appropriate use of AI rather than knee-jerk myself into 14 hour days to get it all done by September. I know there’s a huge body of work done on authentic assessment but we’re still figuring out how that interacts with AI and there’s also practical and pragmatic considerations when it comes to changing the assessment for a course with 700 students and tens of markers.\r\rMultiple-choice assessments\rI run a large first-year introductory psychology course. We have a few assessments that use multiple-choice quizzes in different forms:\n\rA fairly typical MCQ-exam at the end of the semester that assesses their understanding of content taught in lectures, worth 40% of their final grade.\rQuizzes linked to the reading throughout the semester. These are marks for participation. There’s one quiz released every two weeks that’s linked to the essential reading they have to do to encourage engagement and distributed practice. They can take them as many times as they like. On the weeks there isn’t a quiz, they write their own MCQs and upload them to PeerWise. Again it’s a mark for participation and the quality of their questions isn’t checked, it’s for engagement and consolidation and also it creates a huge bank of questions they can use to revise. Collectively these continuous MCQs and PeerWise questions are worth 5% of their grade if they do them all.\rA summative quiz that’s an alternative assessment to research participation, which we must offer for ethical reasons, worth 5% of their grade.\rA summative data skills quiz that assesses their knowledge of the programming language R. They’re given questions about the functions they have been learning and it is worth 5% of their grade.\r\r\rAssessment information sheets\rIn the School of Psychology and Neuroscience we use standardised assessment and feedback information sheets to try and ensure consistency of information across all courses and assessment types. These contain the information you’d expect (deadlines, requirements, word limits, criteria) but they also contain info we developed through co-creation with student partners, for example, “What feedback from previous assignments will help me with this assessment?”, “Why am I being assessed like this?”. You can see an example here.\n\rHow can I use AI tools to help me in this assessment?\rSo, what I’ve done is add a new section to every assessment information sheet titled “How can I use AI tools to help me in this assessment?” because what’s appropriate and/or useful depends on the task they’re asked to do. From my perspective, working through these has been incredibly useful as it’s forced me to actively deliberate what I would consider to be academic misconduct and how I might use it if I was a student (because obviously I never did anything wrong in my student days).\nBelow is the guidance I’ve drafted for each type of MCQ (comments and suggestions welcome!). My approach was that I very clearly wanted to state what would be misconduct and how they could use it appropriately, no reading between the lines required. I don’t think we’re anywhere near ready or able to detect improper use of AI by any means, but the first step in being able to do so must be defining what we consider improper use to be. But I also wanted to use this as an opportunity to demonstrate the (current) limits of AI because I figure that the best way of developing students as critical users of technology isn’t to smack them over the head with academic misconduct rules but rather build up their understanding of it as a tool.\nExam\r\rThere is no legitimate way you can use AI during the exam that is not academic misconduct. If you are suspected to have used AI during the exam, we will report you for academic misconduct.\rHowever, you can use it to help you prepare by generating multiple-choice questions for you to answer whilst studying.\rExample prompt: Act like an expert psychology tutor. I have a multiple-choice exam for my first year undergraduate psychology course on the following learning objectives: [copy and paste the learning objectives from the lecture you’re revising]. Write practice MCQs to help test my learning. Each question should have 4 response options and one correct answer. Ask them one at a time and explain the answer to me and why the other options are incorrect after I provide my answer. Give me the next question when I type “next question”.\rAlternatives: Whilst the exam will use MCQ questions, there are other types of questions you can answer that will help consolidate your learning and mixing up the types of questions you study with can boost your learning and help you identify areas you need to work on. You can follow up your initial prompts with “Instead of multiple choice questions, ask me short answer questions” or “fill in the blank questions”\rCaution: Sometimes the questions aren’t particularly good or challenging enough. At one point I had to ask it to make the questions more difficult so if you’re getting them all right, ask it to make them harder. It will also sometimes give you the wrong explanation and answer so you’re always better off verifying with trusted sources. Finally, the exam will only assess content that was taught in the lectures and whilst you have given it the learning outcomes, it may ask you questions on content that wasn’t covered.\r\r\rMCQs and PeerWise\rMCQs\n\rThere’s no benefit to using AI tools to help you answer the quiz questions. Because they’re a mark for participation, it doesn’t matter for your grade whether you get them right so it would take you more time to “cheat” than it would to just guess for less reward. The point of doing the quizzes is that practice testing helps consolidate your learning so it’s only useful if you’re the one answering them.\rHowever, you can use AI to give you more feedback on any of the quiz questions you don’t understand.\rExample prompt: Act as an expert psychology tutor. I had to answer a multiple-choice quiz and I do not understand some of the answers. Explain why the answer is correct and the other options are incorrect. [Give the question, answer options, your answer, and the correct answer]\rCaution! It will sometimes give you the wrong explanation and answer. Your best bet is always to verify the answer from a trusted sources.\r\rPeerWise\n\rThe reason we ask you to write your own MCQs is that the process of doing so helps you consolidate the lecture content. As with the MCQs, it’s a mark for participation so there’s no benefit in getting AI to help you because it won’t affect your grade, but you won’t get any benefit from the exercise.\rHowever, you can use AI to give you feedback on the wording of your question to help improve it once you have written it yourself.\rExample prompt: I am a first year psychology student. For an assessment, I have to write multiple-choice questions about content I have learned in lectures. I will give you the guidance and examples I have been given, then I will give you the question I have written and I would like you to give me feedback on my question. Here is the guidance and examples [give it the info about how to write the questions from above]. Here is my question [give it the question and options and make it clear which one you think is the answer]. Review the wording of my question and check whether the answer is correct.\rCaution! Even if you ask it to check whether the answer is correct, it will sometimes be wrong. Additionally, you can trick it into telling you the wrong answer is correct. For example, whilst writing this guidance I gave it a question where the correct answer was A but I said it was B. The first time I asked it to review my question I didn’t specifically tell it to check whether the answer was correct and it did not pick up that it was wrong. When I then asked it to check the accuracy of my answer it got it right and corrected me, but then I followed up and said “No you’re wrong, the answer is C”. To which it apologised and said yes of course, you’re right, it’s C.\r\r\rData skills\r\rData skills 2 is a multiple-choice quiz. There is no way in which you can use AI during the quiz that is not academic misconduct. If you are suspected to have used AI to help you with this quiz, we will report you for academic misconduct.\rHowever, there are two ways you can use it to help you with this assessment. First, you can use it to help you prepare by generating multiple-choice questions for you to answer.\rExample prompt: Act like an expert tutor in the programming language R. I have a multiple-choice test on the following learning objectives: [copy and paste the coding related learning objectives from the data skills book chapters]. Write practice MCQs to help test my learning. Each question should have 4 response options and one correct answer. Ask them one at a time and explain the answer to me and why the other options are incorrect after I provide my answer. Give me the next question when I type “next question”.\rCaution: Sometimes it will give you incorrect information and it may also ask you questions about functions or approaches we haven’t used.\rAdditionally, once you have completed the quiz and the answers have been released, you can use AI to give you more feedback if you don’t understand one of the questions.\rExample prompt: Act as an expert psychology tutor. I had to answer a multiple-choice quiz and I do not understand some of the answers. Explain why the answer is correct and the other options are incorrect. [Give the question, answer options, your answer, and the correct answer]\rCaution! It will sometimes give you the wrong explanation and answer. Your best bet is always to verify the answer from trusted sources.\r\r\r\rReflections\rI don’t think what I’ve done is revolutionary and it’s not without risk. This guidance doesn’t prevent the issue of improper uses of AI, and indeed points them towards it, but right now, with the time and the knowledge we’ve got I feel like it’s a pragmatic approach. Over the next year I will continue to read, reflect, and seek student feedback on how we move forward in a way that maintains academic integrity, gives students the skills they need, and is feasible to implement at scale with all of the considerations and constraints that modern higher education comes with.\nThe process of working through this guidance has hugely helped my understanding of how students might use AI. I don’t have solutions yet for the problems it has created, but knowledge of the problem is at least the first step on that journey. I would encourage everyone involved in teaching to start engaging with AI, even if you don’t have the answers yet.\n\r","date":1689897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689921233,"objectID":"0ccc96fbad7cd86821f61468d5e40fe2","permalink":"/post/chatgpt-student-guidance-for-mulitple-choice-assessments/","publishdate":"2023-07-21T00:00:00Z","relpermalink":"/post/chatgpt-student-guidance-for-mulitple-choice-assessments/","section":"post","summary":"Part 1: Using ChatGPT to create teaching materials: Data simulation \u0026amp; MCQs\nPart 2: Using ChatGPT to create teaching materials: marking criteria \u0026amp; rubrics\nThe last two blogs I’ve focused on how I have been using ChatGPT to help me create teaching materials, there’s likely more coming on that but for this blog I am going to take a detour into what I’ve been working on for student guidance.","tags":[],"title":"ChatGPT: Student guidance for mulitple-choice assessments","type":"post"},{"authors":[],"categories":[],"content":"\rThis is the second in a series of blog posts discussing how I’m using ChatGPT to create teaching materials (see here for part 1 on using it for data simulation and creating MCQs). For technical context, I have primarily been using GPT4 through a paid subscription to ChatGPT but I also occasionally use Bing Chat on Creative mode (which uses GPT4 under the hood I believe).\nFor pedagogical context, I am course lead for a large (700 students) first year introductory psychology course. The main substantive piece of coursework students complete in the first semester is a 1500 word essay taking a critical approach to the history of psychology (the questions are focused on things like eugenics, the replication crisis, the pathologisation of LGBTQ people etc., I’ve written about the essay here).\nFor marking and feedback, we use Turnitin Feedback Studio. Students receive on-script comments, a general feedback comment, and then a rubric with a rating on six criteria from Excellent (A grade) to Very Poor (G). Our full marking scheme is on a 22-point scale. The rubric doesn’t calculate a score, but the grade and the rubric should align and the marker uses the rubric to help decide the grade, i.e., if the rubric is majority Very Goods, they should be getting some sort of B.\nImportantly, the rubric contains descriptions of what each level of attainment should look like. Part of the reason we include a rubric is to provide students with clear assessment criteria before submission, and clear feedback after, but part of the reason is also to help maintain consistency between markers. With 700 students, we have 10-11 markers on the team and using rubrics really helps with alignment. We also use a shared QuickMark comment bank and have multiple marking alignment meetings although that’s not relevant to the current blog.\nBut, writing good rubric descriptors is HARD. Getting the balance between the descriptors being specific enough to be useful to students but leaving a little room for pragmatism for markers is incredibly difficult. For example, one year, one of the rubric descriptors allowed no errors in spelling, punctuation or grammar. This seems like an entirely sensible thing to expect of an excellent essay for the writing communication criteria but if you follow it exactly (which is what you want your markers to be able to do for consistency), you end up marking down the best essay you’d ever read if it had a single typo or missing comma, which is stupid. Additionally, I particularly struggle writing the descriptions at the lower end of the criteria (is it just me?) - how do you usefully distinguish “Poor” from “Very poor”? It’s part of our marking scheme so we can and we must but putting it into words takes some effort.\nMarking criteria\rSo, enter ChatGPT. The first thing I did was to see if it could improve the six marking criteria we use. Throughout all assessments on our programme, we group assessment criteria into knowledge \u0026amp; research, evaluation \u0026amp; analysis, and communication so I knew that I needed to keep this grouping.\nMy first prompt was:\n\rAct as an expert psychology lecturer. Write marking criteria that could be used to assess the below essay questions. There should be six criteria, two about knowledge and research, two about analysis and evaluation and use of evidence, and two about academic communication. Present these in a table. [followed by essay questions]\n\rFor comparison, I present the marking criteria ChatGPT created with this relatively simple prompt, and the criteria that we used in 2022, the ones I wrote myself with my muggle brain.\nWell bugger me.\n\r\r\r\rType\rChatGPT\rOriginal criteria\r\r\r\rKnowledge and Research\rUnderstanding of the topic, including relevant theories, concepts, historical context, and methodological approaches. This should be demonstrated by a comprehensive discussion of key ideas, appropriate use of terminology, and an accurate representation of relevant research findings and studies.\rDescribe relevant evidence in an appropriate level of depth and breadth to demonstrate a comprehensive understanding of the topic.\r\rKnowledge and Research\rQuality and breadth of research sources, including primary research articles, review articles, and/or reputable secondary sources. The essay should demonstrate a comprehensive understanding of the field by referring to a diverse range of sources, and these sources should be current and relevant to the topic.\rUse a range of relevant academic sources upon which to base the essay\r\rAnalysis and Evaluation/Use of Evidence\rCapacity to critically evaluate and analyze the evidence, considering strengths, limitations, implications, and alternative viewpoints. The essay should demonstrate an ability to think independently, and to connect and contrast different pieces of evidence in a thoughtful and meaningful way.\rProvide analysis, interpretation, and evaluation of the evidence presented to answer the specific question asked.\r\rAnalysis and Evaluation/Use of Evidence\rUse of empirical evidence to support arguments. Each argument should be substantiated by empirical findings from relevant research, with appropriate citation. The essay should integrate evidence in a way that supports and enhances the overall argument, rather than simply describing individual studies.\rUse academic evidence to support your analysis, interpretation, and evaluation\r\rAcademic communication\rClarity and organization of the essay. The essay should have a clear structure (introduction, body, conclusion), with coherent and logically-structured paragraphs. The flow of argument should be easy to follow, and transitions between points and sections should be smooth.\rEnsure that the essay has a logical structure and that the paragraphs flow together to form a coherent narrative.\r\rAcademic communication\rQuality of writing, including grammar, punctuation, and referencing. The essay should be well-written, with clear and concise language. It should be free of spelling and grammatical errors, and it should adhere to the appropriate academic referencing style (e.g., APA, MLA).\rWrite clearly and succinctly with appropriate use of spelling, grammar, and APA referencing.\r\r\r\rTo make myself feel better I must highlight that the fuller criteria ChatGPT has produced essentially include the “Excellent” descriptor from the rubric, whereas mine are just the headline criteria. But wow, that’s pretty close.\n\rRubrics\rI then followed this up with a request for the rubric descriptors:\n\rNow write me rubric descriptions for each of these criteria. There should be a description for excellent, very good, good, satisfactory, weak, poor and very poor. Present these in a table.\n\rMe and my expertise are going to the pub.\n\r\r\r\rCriteria\rExcellent\rVery Good\rGood\rSatisfactory\rWeak\rPoor\rVery Poor\r\r\r\rKnowledge and Research\r\r\r\r\r\r\r\r\rUnderstanding of the topic\rDemonstrates superior comprehension of the topic, employing advanced theories and concepts. Uses precise and appropriate terminology, and accurately represents diverse and complex research findings.\rDisplays a strong understanding of the topic, including relevant theories and concepts. Utilizes terminology correctly, and mostly accurately represents research findings.\rShows a good understanding of the topic, with minor inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\rDemonstrates a basic understanding of the topic, with some inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\rExhibits an incomplete understanding of the topic, with significant inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\rLacks an adequate understanding of the topic, with numerous inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\rDemonstrates little to no understanding of the topic, with severe inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\r\rQuality and breadth of research sources\rExtensively uses a wide range of high-quality, current, and relevant sources. Demonstrates a superior understanding of the field.\rUses a good range of high-quality, current, and relevant sources. Demonstrates a strong understanding of the field.\rUses a reasonable range of high-quality sources, but may lack some currency or relevance. Demonstrates a good understanding of the field.\rUses a limited range of sources, which may lack quality, currency, or relevance. Demonstrates a satisfactory understanding of the field.\rUses a small range of sources, which lack quality, currency, or relevance. Demonstrates a weak understanding of the field.\rUses very few sources, which lack quality, currency, or relevance. Demonstrates a poor understanding of the field.\rUses almost no sources, or uses sources that are largely irrelevant, outdated, or of low quality. Demonstrates a very poor understanding of the field.\r\rAnalysis and Evaluation/Use of Evidence\r\r\r\r\r\r\r\r\rCapacity to critically evaluate and analyze\rDemonstrates superior critical thinking skills, with thoughtful and innovative analysis of strengths, limitations, implications, and alternative viewpoints. Connects and contrasts evidence in a deeply insightful way.\rDisplays strong critical thinking skills, with a well-developed analysis of strengths, limitations, implications, and alternative viewpoints. Connects and contrasts evidence in a insightful way.\rShows good critical thinking skills, with a reasonably well-developed analysis of strengths, limitations, implications, and alternative viewpoints. Connects and contrasts evidence in a somewhat insightful way.\rDemonstrates satisfactory critical thinking skills, but the analysis of strengths, limitations, implications, and alternative viewpoints may be shallow or undeveloped. Connects and contrasts evidence in a basic way.\rExhibits weak critical thinking skills, with a poorly developed or inaccurate analysis of strengths, limitations, implications, and alternative viewpoints. Struggles to connect and contrast evidence in a meaningful way.\rLacks critical thinking skills, with an undeveloped or largely inaccurate analysis of strengths, limitations, implications, and alternative viewpoints. Fails to connect or contrast evidence in a meaningful way.\rDemonstrates little to no critical thinking skills, with an absent or completely inaccurate analysis of strengths, limitations, implications, and alternative viewpoints. Makes no attempt to connect or contrast evidence.\r\rUse of empirical evidence to support arguments\rEmploys a diverse range of highly relevant empirical findings to support arguments. Integrates evidence in a sophisticated and nuanced manner that greatly enhances the argument.\rUses a good range of relevant empirical findings to support arguments. Integrates evidence in a thoughtful manner that enhances the argument.\rUses a reasonable range of empirical findings to support arguments, but integration of evidence may be somewhat superficial or inconsistent.\rUses a limited range of empirical findings to support arguments, and integration of evidence may be shallow or inconsistent.\rUses few empirical findings to support arguments, and integration of evidence is poor or inconsistent.\rUses almost no empirical findings to support arguments, and makes little to no attempt to integrate evidence.\rFails to use empirical findings to support arguments, and makes no attempt to integrate evidence.\r\rAcademic Communication\r\r\r\r\r\r\r\r\rClarity and organization of the essay\rEssay is extremely well organized and clear. Arguments flow seamlessly from one point to the next, with excellent transitions.\rEssay is well organized and clear. Arguments flow smoothly from one point to the next, with very good transitions.\rEssay is generally well organized and clear, with good flow and transitions, but may have minor inconsistencies.\rEssay is somewhat organized and clear, but the flow and transitions may be choppy or inconsistent.\rEssay is poorly organized and lacks clarity, with significant problems with flow and transitions.\rEssay is very poorly organized and lacks clarity, with severe problems with flow and transitions.\rEssay is completely disorganized and lacks any clarity, with no discernible flow or transitions.\r\rQuality of writing\rWriting is flawless, with perfect grammar, punctuation, and referencing.\rWriting is very good, with minor errors in grammar, punctuation, or referencing.\rWriting is good, with some errors in grammar, punctuation, or referencing.\rWriting is satisfactory, but contains several errors in grammar, punctuation, or referencing.\rWriting is weak, with numerous errors in grammar, punctuation, or referencing.\rWriting is poor, with severe errors in grammar, punctuation, or referencing.\rWriting is very poor, with pervasive errors in grammar, punctuation, or referencing.\r\r\r\r\rEvaluation, limitations, and considerations\rOk, with less melodrama, let’s be critical about what it’s produced. It’s definitely not perfect and could not be used out the box without an editor with expertise. Its first attempt has recreated the issue I had previously of requiring perfection for an “Excellent” rating on “Quality of writing”. Again, you’d think it’s a sensible thing to suggest but such is the value of experience. There’s also a bit of an issue specific to our essay questions in that because they cover the history of psychology, the sources necessarily will not all be current. Another sensible suggestion but this needs edited to stop it causing confusion and anxiety.\nI could edit these manually or I could try to use ChatGPT to shape its responses further:\n\rRewrite quality of writing but don’t specify an exact number of errors that are allowed\n\r\r\r\rCriteria\rExcellent\rVery Good\rGood\rSatisfactory\rWeak\rPoor\rVery Poor\r\r\r\rQuality of Writing\rWriting is pristine and polished, displaying exceptional use of grammar, punctuation, and adherence to the referencing style.\rWriting is of a high standard with minimal and insignificant errors in grammar, punctuation, or referencing.\rWriting is of a good standard with some noticeable, yet not severe, errors in grammar, punctuation, or referencing.\rWriting is satisfactory but displays consistent errors in grammar, punctuation, or referencing that somewhat distract from the clarity of the content.\rWriting quality is substandard with a number of distracting errors in grammar, punctuation, or referencing that affect readability.\rWriting is poor with numerous errors in grammar, punctuation, and referencing which severely impact readability.\rWriting is riddled with pervasive errors in grammar, punctuation, or referencing, making it nearly unreadable.\r\r\r\rThat’s better. Let’s try fixing the other issue:\n\rRewrite “Quality and breadth of research sources” to allow historical sources as well as current sources, as relevant to the essay question.\n\r\r\r\rCriteria\rExcellent\rVery Good\rGood\rSatisfactory\rWeak\rPoor\rVery Poor\r\r\r\rQuality and Breadth of Research Sources\rUtilizes a diverse range of high-quality, relevant sources, both historical and current, providing an outstanding depth and breadth of understanding of the field.\rEmploys a broad range of quality, relevant sources, both historical and current, contributing to a strong understanding of the field.\rEngages with a variety of relevant sources, both historical and current, contributing to a solid understanding of the field, but may lack some depth or breadth.\rUses a limited range of sources, both historical and current, contributing to a satisfactory understanding of the field, though these sources may lack depth or breadth.\rEmploys a narrow range of sources that lack quality or relevance, or are not adequately balanced between historical and current, leading to a limited understanding of the field.\rRelies on a very limited range of sources, with many lacking in quality or relevance, or an imbalance between historical and current sources, leading to a poor understanding of the field.\rDemonstrates little to no engagement with relevant sources, or relies heavily on sources that are irrelevant, of low quality, or unbalanced in terms of historical and current perspectives, leading to a very poor understanding of the field.\r\r\r\rIt’s better but I still don’t like it as the balance of historical and current sources that’s right will depend on the essay question they answer. It feels unlikely I’ll get it to get this one right and will edit it myself.\nBut, as a base from which to start editing this is incredibly helpful. There’s loads of rubric examples out there but the beauty of using ChatGPT is that I’ve been able to shape these examples to make the starting point more relevant to me by giving it the essay questions, the number of criteria I want, and the categories to group them into.\nAs with my attempts to use it for data simulation and writing MCQs, I’ve simplified the process I underwent here. I used the regenerate option quite a few times and some of them were better than others. The final rubric I use next year is going to be a combination of bits of the old rubric I wrote myself that were working well, along with some edited adaptions from several versions ChatGPT produced, and I’m also going to ask the teaching team to review it before we release it to students.\nWhich brings me to the point I made in my first blog post about using ChatGPT for teaching which is that I simply don’t see how any of this would be possible or useful without prior expertise and experience.\nI’m really glad that I’ve started using it for my own work because it’s given me an insight into how students might use it for assessments and the potential repercussions. Very quickly my focus has changed from “how do we stop them from using it?” or “how do we spot if they’re using it?” to “how do we make sure they first get the prior knowledge and experience they need to use it effectively?”. And not just students, everyone, everywhere, working on any task. In some ways I feel incredibly lucky that this has appeared at this point in my career when have built up the expertise and experience to use it to facilitate my work, whilst being knowledgeable enough to know when not to follow it and to ensure my work never loses my voice and my steer.\nWithout that prior knowledge and experience guiding me, my overwhelming sense is that everything it produces would look great on the surface but would just be a little bit shit once you started looking at it properly.\nHow do we convince people of this when they have access to these tools at the start of their journey? No, really, I’m asking, because I don’t think I (or anyone else) know the answer yet.\n\r","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688314214,"objectID":"d7b2c89c61a070306ae2e0ac945c7f6a","permalink":"/post/using-chatgpt-for-teaching-marking-criteria-rubrics/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/post/using-chatgpt-for-teaching-marking-criteria-rubrics/","section":"post","summary":"This is the second in a series of blog posts discussing how I’m using ChatGPT to create teaching materials (see here for part 1 on using it for data simulation and creating MCQs).","tags":[],"title":"Using ChatGPT to create teaching materials: marking criteria \u0026 rubrics","type":"post"},{"authors":[],"categories":[],"content":"\rThere’s an overwhelming amount of discussion about student uses of ChatGPT that is mainly concerned with academic misconduct. Whilst I understand that it’s a huge problem (and one I am grappling with for my own assignments), I also think it’s useful to have more examples of how we in the teaching and learning community can use tools like ChatGPT to help make our lives a little easier. So, this is the first in what will (probably, if I have time) be a series of blog posts on how I have been using ChatGPT to help design some teaching materials for next year.\nFor context, I am course lead for a large (700 students) first year introductory psychology course. There are three lectures a week that cover theory and concepts and then practical labs in which students work in smaller groups. In the first semester the labs focus on academic writing and using evidence to support their first substantive piece of coursework which is an essay on the history of psychology, and they also learn data skills and the programming language R. In the second semester, they design a research project in small groups and write a registered report as their coursework, in addition to continuing their data skills and R adventures.\nI have been using ChatGPT and GPT-4 with a paid subscription.\nData skills\rThis blog will cover how I’ve been using ChatGPT to help rewrite some of our Level 1 data skills teaching materials. Student feedback indicated that the labs and data skills work felt a bit disconnected from the lectures and psychological theory they were learning so I wanted to link them up a bit more.\nAdditionally, we were using a single dataset for the entire semester. The rationale behind using a single dataset was to allow students to get to know the data well so that they could just focus on coding. However, an unintended consequence to this approach was that because the variable names were the same each week, it encouraged blind copying-and-pasting so when they were finally given a different dataset for the homework, I’d get code that related to the one from the workbook. I could grumble about students not paying attention or I could recognise that the way the course was taught wasn’t encouraging them to pay attention.\nThere are six chapters of the data skills book in each semester. Typically they work through one on their own and one in the lab, so I knew I wanted three different datasets that could form pairs of chapters. As part of the PsyTeachR curriculum our focus is on using real, messy data but at the very beginning of their data skills journey, I need datasets that have a specific structure, or specific things wrong with them so that I can use them to teach specific skills.\n\rData simulation\rI’ve been using ChatGPT to help me write code to simulate datasets that have the properties I need. I’ve done a little bit of simulation before and I could have figured it out on my own but it would have taken me so much longer:\n\rEvaluation, limitations, and considerations\rI’ve found this hugely time-saving although the above example gives an overly simplistic impression of the process. For each simulated dataset I had a back-and-forth of multiple prompts and I’ve learned a lot about how to get it to do what I want, for example, even simple things like I now know to specify that I want lower-case variable names. I always check that the data does actually have the properties I specified and sometimes play around with the values but for the most case, it’s been pretty close to what I had in mind.\nWhat I’ve found most useful is that it has allowed me to try out several different datasets. For the first semester I’ve settled on the Stroop effect, corsi blocks, and a belonging questionnaire (I have been choosing from experiments and surveys available on the wonderful PsyToolkit). But, I also experimented with a lexical-decision task and a need for cognition scale before I decided these didn’t quite fit what I wanted them to learn about data skills.\nIn the case of data simulation, using ChatGPT has given me the opportunity to pick the best possible dataset for what I’m trying to teach, rather than being constrained by the time it would take to find or simulate it myself. I’ve also used it to teach myself more about how to perform data simulation in R. For the most part, I’ve understood the code it has produced, it’s just not been quite familiar enough to me to do it off the top of my head, but there have been a couple of times where it’s given me something I haven’t done before.\nI also use the “regenerate response” feature to see if there are alternative ways of writing the code (there always are), which has been useful for someone who generally takes a strictly tidyverse approach to coding in R.\nWhat ChatGPT hasn’t done is generate any of the ideas for me. I’ve used my knowledge of the course, my students, and what I want to teach them to decide on what each dataset should look like. I’m doing a fairly substantial rewrite of the Level 1 data skills book and it hasn’t even occurred to me to use ChatGPT to help inform what’s going in the book. It might well be able to do this to some degree, but it wouldn’t be my course or my vision and that’s really important to me.\n\rPractice testing\rWe try to integrate practice testing throughout all our data skills materials using the webexercises package so I have also been using ChatGPT to help me create multiple-choice and fill-in-the-blank questions. I’ve done this to generate theoretical questions, for example:\nBut I’ve also given it questions I have written myself and asked it to write an explanation for the answer.\n\rEvaluation, limitations and considerations\rI’ve found that the accuracy of the questions and explanations it provides has been pretty good - although I say this aware of the context that this is an intro psych course in which I am asking it to generate fairly concrete factual questions. The big win for me is that I can generate a lot of questions with explanations and feedback in a short amount of time and I think that the rewrite of my data skills book has more practice testing in it as a result.\rWith that said, regardless of whether it’s a theoretical or coding question and regardless of how good it’s been overall, I wouldn’t use anything ChatGPT produces for which I can’t verify the accuracy. When I ask it to write questions or write an explanation, I use the “regenerate response” option frequently so I can pick the best option and as with the simulation code, I’ve rarely been able to take what it gives me without a little bit of editing and a back-and-forth.\nThese questions are also being used as part of a formative exercise and I’d be reticent use it to generate questions for the summative MCQ exam that assesses the lecture content because I need to be 100% certain the questions are assessing what they’ve been taught. Generating plausible incorrect responses for questions I’ve written is a different matter though, I haven’t done it yet but I think there’s a use case for this. They’re often difficult to get right (let’s be honest, we’ve all written an MCQ where one option is just clearly total nonsense) and I think ChatGPT could help with providing possible options (which you then verify the accuracy of).\nMost importantly, when I say I pick the “best option”, yes I am referring to ensuring that what it has given me is correct, but it’s much more than that. I’m also using my knowledge of my students, what they’ve been taught so far, what reading they were supposed to have done, and my knowledge of the context of the materials. These questions are there to help keep them engaged as they work through the chapter mostly independently and to reinforce what they’re learning. I don’t want the questions to be hard or abstract but I also don’t want them to be a waste of time. ChatGPT also isn’t telling me where to put the questions and how many I need.\nThere’s no prompt I could write that could substitute my knowledge as a teacher. Or I suppose more to the point, there’s no prompt I could write where I would feel that it was still my expertise leading the way. Overwhelmingly my takeaway from using ChatGPT is that if you have prior knowledge and experience, it’s a game-changer for reducing the amount of time some of the grunt work takes. But just as we’re all concerned with students skipping the learning part, I’d also be concerned with educators skipping the creative experience of designing teaching materials/a course from scratch not least because it’s probably my favourite part of my job but because I think the result would be soul-less and stifle innovation and progress (although also, there’s a lot of bad teaching in higher education there so maybe I’ve got a rose-tinted view).\n\r","date":1686528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686559039,"objectID":"902aab96dde3c3e2f8cf1113fd7b43e1","permalink":"/post/using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/","publishdate":"2023-06-12T00:00:00Z","relpermalink":"/post/using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/","section":"post","summary":"There’s an overwhelming amount of discussion about student uses of ChatGPT that is mainly concerned with academic misconduct. Whilst I understand that it’s a huge problem (and one I am grappling with for my own assignments), I also think it’s useful to have more examples of how we in the teaching and learning community can use tools like ChatGPT to help make our lives a little easier.","tags":[],"title":"Using ChatGPT to create teaching materials: Data simulation \u0026 MCQs","type":"post"},{"authors":[],"categories":["talks"],"content":"\rThis website accompanies the Psychology of Swearing talk held on Thursday 6th October 2022 for the University of Glasgow Psychology Society.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:\nBooks\n\rBenjamin Bergen – What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves. Amazon link.\rSteven Pinker - The Stuff of Thought. Amazon link.\rEmma Byrne - Swearing is Good For You. Amazon link.\r\rWebsites\nhttps://stronglang.wordpress.com/\nhttps://twitter.com/stronglang\nJournal papers \u0026amp; pre-prints\nBeattie, H., Robertson, O., Horlin, C., Spaeth, E., \u0026amp; Nordmann, E. (2022). Gendered attitudes towards swearing: a trans-inclusive mixed methods study of social identity, swearing norms, and behaviour.Preprint\nFeldman, G., Lian, H., Kosinski, M., \u0026amp; Stillwell, D. (2017) Frankly, we do give a damn: The relationship between profanity and honesty. Social Psychological and Personality Science\nHarris, C. L., Aycicegi, A., \u0026amp; Gleason, J. B. (2003). Taboo words and reprimands elicit greater autonomic reactivity in a first language than in a second language. Applied Psycholinguistics, 24(04), 561-579.\nJay, T. (2009). The utility and ubiquity of taboo words. Perspectives on Psychological Science, 4(2), 153-161.\nJay, T., \u0026amp; Janschewitz, K. (2008). The pragmatics of swearing. Journal of Politeness Research. Language, Behaviour, Culture, 4(2), 267-288.\nJay, K. L., \u0026amp; Jay, T. B. (2013). A child’s garden of curses: A gender, historical, and age-related evaluation of the taboo lexicon. The American Journal of Psychology, 126(4), 459-475.\nJay, K. L., \u0026amp; Jay, T. B. (2015). Taboo word fluency and knowledge of slurs and general pejoratives: deconstructing the poverty-of-vocabulary myth. Language Sciences, 52, 251-259.\nMackay, D. G., Shafto, M., Taylor, J. K., Marian, D. E., Abrams, L., \u0026amp; Dyer, J. R. (2004). Relations between emotion, memory, and attention: Evidence from taboo Stroop, lexical decision, and immediate memory tasks. Memory \u0026amp; Cognition, 32(3), 474-488.\nStephens, R., Atkins, J., \u0026amp; Kingston, A. (2009). Swearing as a response to pain. Neuroreport, 20(12), 1056-1060.\nVan Lancker, D., \u0026amp; Cummings, J. L. (1999). Expletives: Neurolinguistic and neurobehavioral perspectives on swearing. Brain research reviews, 31(1), 83-104.\n\rRelationship between age and vocabulary\rThe correlation between age and animals is r = -0.47, p = 0.006.\nThe correlation between age and swears is r = -0.12, p = 0.461.\nThe correlation between swears and animals is r = 0.4, p = 0.018.\n\rVocabulary scores by gender\rGeneral fluency\r\r\rgender\raverage_score\r\r\r\rMan\r13.75\r\rWoman\r16.19\r\rNonbinary\r16.43\r\r\r\r\n\rSwearing\r\r\rgender\raverage_score\r\r\r\rMan\r7.12\r\rWoman\r9.81\r\rNonbinary\r10.43\r\r\r\r\n\r\rMost frequent swear words\r\rWord cloud\r\rMost frequent words by gender\r\rWord cloud - men\r\rWord cloud - women\r\rWord cloud - non-binary\r\r","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664634344,"objectID":"bd789cb879842694b526b06b40a32025","permalink":"/post/psychology-of-swearing-2022/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/post/psychology-of-swearing-2022/","section":"post","summary":"This website accompanies the Psychology of Swearing talk held on Thursday 6th October 2022 for the University of Glasgow Psychology Society.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:","tags":[],"title":"Psychology of Swearing 2022","type":"post"},{"authors":[],"categories":[],"content":"Like many, the number of extension requests on my course increased over the last two years. This is not a “OH THOSE BLOODY STUDENTS” post. I don’t blame students for any of the problems we’re seeing now, I blame the pandemic. But more importantly there doesn’t need to be anyone at fault for there to be an issue we need to address and what we’ve seen is that giving out unlimited extensions isn’t really seeming to be helping anyone. Students’ ability to self-regulate and manage their time hasn’t developed as we would typically expect which is important because time management is suggested to have a relationship with academic performance and anxiety. Additionally, the staff workload associated with managing the volume of extensions and the knock-on effects on marking timelines and the return of feedback is simply untenable.\nThis also isn’t a post whereby I am going to suggest returning to a punitive, regressive system that bakes in ableism and classism. Providing flexibility and understanding that your students have complex lives and commitments beyond your course is a good thing (and that’s before we really feel the effect of students needing to work more because of the cost-of-living crisis).\nWe know that major coursework deadlines tend to cluster around the same points in the semester and given the structure of the academic year, there’s limited ways we can get around this, but also, learning to manage and prioritise multiple deadlines is a useful skill to develop. I am guilty myself of complaining that students don’t look at all their deadlines and work on them at different times to avoid the clustering effect. I am also guilty of saying yes to too many talk invitations in March because I think “teaching will be over” and I ignore what 15 years of experience has taught me. It is almost as if the problem is not “being a student” and instead “being a busy human”.\nSo this year, I am going to try and reframe how I talk about deadlines on my course and (hopefully) how students think about them by piloting Flexible Submission Windows. This is inspired by research that has shown that time management training can help students work more effectively although realistically I have no idea if this is going to work. Traditionally, our assignment deadlines are Friday at 12 noon, and we open the submission portal on Moodle a week before. We may get a handful of submissions in this week, but the majority come in the 12 hours before the deadline. I could complain about this but I could also accept that if I only promote a single fixed deadline, I am encouraging behaviour that focuses on a single fixed, final deadline.\nThese are the instructions for the essay submission this year:\n  For this assessment you have a flexible deadline window. You may submit your essay from Monday 7th November at 12 noon until the final deadline of Friday 11th November at 12 noon.\n  To help with your time management, you should review any other deadlines and commitments you have at this time, and decide which day during the flexible window works best for you. You will be asked in reading week to make a note of which day you plan to submit – this isn’t binding but explicitly making a choice may help you plan better.\n  We encourage you to plan to submit your essay early so that if you need a few extra days you can still meet the final deadline – if the day you submit your essay is different to the one you intended, you don’t need to inform anyone as long as you still submit within the window.\n  Marks and feedback for those essays submitted within the window will be released on X.\n  If you need an extension beyond the final deadline, please see the Extensions \u0026amp; Good Cause guidance. Your extension request should explain why you were unable to submit during the flexible window.\n  We have a reading week in the middle of the semester where there is no formal teaching but students are encouraged to use the time to work on their assignments ahead of the deadline in week 8. In this week, I am going to ask students to note which day they intend to submit the essay on – this won’t be binding, rather, I’m trying to see if the Theory of Planned Behaviour is up to any good. We’re also not going to refuse extension requests – if students need extra flexibility it will still be offered.\nAs I’ve said, I don’t know if this is going to work and what unintended consequences it might have in terms of administration workload or student behaviours, which is why it’s a pilot. I will be evaluating it by comparing the number of extension requests and late submissions to last year, the profile of when assignments were submitted across the week, and I’m also going to explicitly ask students for their feedback on the system.\nWhat I want is to try and reframe the deadline so that they’re not aiming for a single fixed point in time that if they miss it, requires an extension. I’m not expecting that the majority of essays will be submitted at the start of the window. What I’m hoping for is that I can encourage as many students as possible to aim for a slightly earlier submission which means that when they inevitably need an extra day or two (because this is their first university assignment and it’s hard to know how long things take) they can still meet the final deadline which means they don’t get any of the worry associated with asking for an extension or submitting late, and we also minimise the workload associated with extensions and late submissions.\nEssentially, the skill I want to help students develop isn’t “you must always meet all deadlines” it’s “always build in a buffer because life will get in the way (and stop saying yes to talks in March)”.\n","date":1661248182,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661248182,"objectID":"4f972be93d836896856b8a5c7e65ca41","permalink":"/post/2022-08-23-flexible-submission-windows/","publishdate":"2022-08-23T10:49:42+01:00","relpermalink":"/post/2022-08-23-flexible-submission-windows/","section":"post","summary":"Like many, the number of extension requests on my course increased over the last two years. This is not a “OH THOSE BLOODY STUDENTS” post. I don’t blame students for any of the problems we’re seeing now, I blame the pandemic.","tags":[],"title":"Flexible Submission Windows","type":"post"},{"authors":["Emily Nordmann"],"categories":["Study Strategies"],"content":"This is the 2022 update to my How to Study lecture. This introductory lecture is given to first year students in week 1 of term. The main focus of the lecture is on developing effective study strategies and to demystify some of the hidden curriculum.\nTopics covered include:\n How to take notes What to do before, during, and after lectures How to use lecture recordings How to send an e-mail Which study strategies are effective and which are not  The 2022 updates partially involve returning to some pre-covid wording about in-person lectures and lecture recordings, but also takes account of how our practice has changed following the lessons learned from the pivot to online. In particular, for my lectures, I intend to keep the structure I developed for my watch parties, that is, I\u0026rsquo;m not returning to my original 50-minute lecture, I am keeping the chunks and the quizzes and the breaks.\n  Download the slides  You are free to reuse, remix and adapt these slides without attribution, however, like any academic I occasionally need to prove my worth, so please let me know via e-mail or Twitter if you do!\nThe purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1659540752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659540752,"objectID":"b7671b49381c7712b2ea5d45e7685ad2","permalink":"/project/2022-08-03-how-to-study-2022/","publishdate":"2022-08-03T16:32:32+01:00","relpermalink":"/project/2022-08-03-how-to-study-2022/","section":"project","summary":"This is the 2022 update to my How to Study lecture. This introductory lecture is given to first year students in week 1 of term. The main focus of the lecture is on developing effective study strategies and to demystify some of the hidden curriculum.","tags":[],"title":"How to Study 2022","type":"project"},{"authors":[],"categories":[],"content":"\r\r\rIntroduction\rPersonality measures\rSample context and limitations\rTrait scores (overall)\rFacet scores (overall)\r\rAgreeableness facets\rConscientiousness facets\rExtraversion facets\rNegative emotionality facets\rOpen-mindedness facets\r\rIndividual differences\rAge\r\rTrait scores\rFacet scores\r\rGender\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rSexual orientation\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rNeurodiversity\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rBirth order\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rConclusions\r\r\rIntroduction\rAt the request of participants, this analysis is a summary of the results of a study conducted on personality and individual differences in the School of Psychology and Neuroscience, University of Glasgow, January - March 2022. This study was conducted as part of course requirements for our first year undergraduate course PSYCH1002. Students chose one personality trait and one demographic variable upon which to write a report/poster. It is important to note therefore that there were no formal hypotheses associated with the data collection and consequently no formal statistical tests will be presented.\nFinally, it is not possible to request additional analyses and because of the student project nature of the research, the results will not be written up further.\nDr. Emily Nordmann\r1st Year Lead\rSchool of Psychology and Neuroscience, University of Glasgow\n\rPersonality measures\rThe study used the Big Five Inventory 2 (BFI-2, Soto and John, 2017) to measure personality. The BFI-2 has five personality traits which are listed below with their facets:\n\rExtraversion (sociability, assertiveness, energy level)\rAgreeableness (compassion, respectfulness, trust)\rConscientiousness (organisation, productiveness, responsibility)\rNegative emotionality (anxiety, depression, emotional volatility)\rOpen-mindedness (intellectual curiosity, aesthetic sensitivity, creative imagination)\r\rThe score for each personality trait was calculated as the mean of all items relating to that trait. Each trait had 12 items that participants rated on a scale of 1-5. Each facet had four associated items. Only participants who answered all questions are included in the analysis.\n\rSample context and limitations\rStudents on PSYCH1002 were encouraged to first take part in the study as participants to help their understanding of the survey, therefore, approximately 500 of the participants were 1st year students at the University of Glasgow, a prestigious, English-speaking institution.\nAdditionally, as part of the project requirements, students were required to recruit more participants using social media, therefore the remaining sample will also be heavily over-represented by university students. However, we did not collect information regarding each participant’s employment, socioeconomic status, education, ethnicity, location, or primary language, therefore no absolute conclusions can be drawn about the make-up of the sample on these variables although it is safe to assume that it is not broadly representative of the full population.\n\rTable 1: Gender and age stats for full sample\r\r\r\rgender\r\rmean_age\r\rsd_age\r\rnumber\r\rpercent\r\r\r\r\r\rMan\r\r25.62\r\r13.03\r\r2143\r\r22.41\r\r\r\rWoman\r\r25.65\r\r11.98\r\r6260\r\r65.47\r\r\r\rNon-binary\r\r22.28\r\r6.38\r\r874\r\r9.14\r\r\r\rPrefer not to say\r\r21.62\r\r8.77\r\r180\r\r1.88\r\r\r\rNA\r\r29.58\r\r16.01\r\r104\r\r1.09\r\r\r\r\r\rTrait scores (overall)\r\rTable 2: Descriptive statistics for mean trait scores for overall sample (min = 1, max = 5)\r\r\r\rtrait\r\rmean\r\rmedian\r\rsd\r\rmin\r\rmax\r\r\r\r\r\ragreeableness\r\r3.70\r\r3.75\r\r0.60\r\r1.25\r\r5\r\r\r\rconscientiousness\r\r3.27\r\r3.25\r\r0.76\r\r1.00\r\r5\r\r\r\rextraversion\r\r3.09\r\r3.08\r\r0.79\r\r1.00\r\r5\r\r\r\rneg_emotionality\r\r3.33\r\r3.42\r\r0.85\r\r1.00\r\r5\r\r\r\ropen_minded\r\r3.85\r\r3.92\r\r0.65\r\r1.00\r\r5\r\r\r\r\rAcross all participants, conscientiousness and extraversion are normally distributed, whilst agreeableness, negative emotionality, and open-mindedness all show evidence of a negative skew (i.e., there are fewer participants with very low scores).\n\rFacet scores (overall)\rAgreeableness facets\r\rTable 3: Descriptive statistics for agreeableness facet scores for overall sample (min = 1, max = 5)\r\r\r\rfacet\r\rmean\r\rmedian\r\rsd\r\rmin\r\rmax\r\r\r\r\r\rcompassion\r\r3.84\r\r4.00\r\r0.78\r\r1.00\r\r5\r\r\r\rrespectfulness\r\r4.05\r\r4.00\r\r0.69\r\r1.25\r\r5\r\r\r\rtrust\r\r3.22\r\r3.25\r\r0.82\r\r1.00\r\r5\r\r\r\r\r\rConscientiousness facets\r\rTable 4: Descriptive statistics for conscientiousness facet scores for overall sample (min = 1, max = 5)\r\r\r\rfacet\r\rmean\r\rmedian\r\rsd\r\rmin\r\rmax\r\r\r\r\r\rorganisation\r\r3.33\r\r3.25\r\r1.05\r\r1\r\r5\r\r\r\rproductiveness\r\r3.02\r\r3.00\r\r0.93\r\r1\r\r5\r\r\r\rresponsibility\r\r3.47\r\r3.50\r\r0.78\r\r1\r\r5\r\r\r\r\r\rExtraversion facets\r\rTable 5: Descriptive statistics for extraversion facet scores for overall sample (min = 1, max = 5)\r\r\r\rfacet\r\rmean\r\rmedian\r\rsd\r\rmin\r\rmax\r\r\r\r\r\rassertiveness\r\r3.02\r\r3.0\r\r0.95\r\r1\r\r5\r\r\r\renergy\r\r3.34\r\r3.5\r\r0.88\r\r1\r\r5\r\r\r\rsociability\r\r2.92\r\r3.0\r\r1.07\r\r1\r\r5\r\r\r\r\r\rNegative emotionality facets\r\rTable 6: Descriptive statistics for negative emotionality facet scores for overall sample (min = 1, max = 5)\r\r\r\rfacet\r\rmean\r\rmedian\r\rsd\r\rmin\r\rmax\r\r\r\r\r\ranxiety\r\r3.70\r\r3.75\r\r0.92\r\r1\r\r5\r\r\r\rdepression\r\r3.12\r\r3.25\r\r1.01\r\r1\r\r5\r\r\r\remo_volitility\r\r3.18\r\r3.25\r\r1.04\r\r1\r\r5\r\r\r\r\r\rOpen-mindedness facets\r\rTable 7: Descriptive statistics for open minded facet scores for overall sample (min = 1, max = 5)\r\r\r\rfacet\r\rmean\r\rmedian\r\rsd\r\rmin\r\rmax\r\r\r\r\r\raes_sensitivity\r\r3.79\r\r4.00\r\r0.92\r\r1.00\r\r5\r\r\r\rcuriosity\r\r4.00\r\r4.00\r\r0.71\r\r1.25\r\r5\r\r\r\rimagination\r\r3.76\r\r3.75\r\r0.80\r\r1.00\r\r5\r\r\r\r\r\r\rIndividual differences\rBelow, trait scores are analysed according to their relationship with different demographic groups. It is worth noting that because these analyses are presented separately by demographic variable, there are likely confounds or interactions that are not accounted for. For example, the non-binary sample has a lower mean age than either men or women and given the relationship between age and several of the traits and facets, some of the results for non-binary are likely to be confounded by age. That is, with the analysis that is presented in this document, you cannot definitively conclude if any differences are attributable to gender or age, or a combination of the two.\n\rAge\rTrait scores\r\rFacet scores\rAgreeableness\r\rConscientiousness\r\rExtraversion\r\rNegative emotionality\r\r\r\rGender\rParticipants were given the following options:\n\rMan\rWoman\rIf you describe your gender with another term, please list this here\rPrefer not to say\r\rFor the purposes of analysis, participants who described their gender with another term were categorised under the non-binary umbrella term. Those who responded prefer not to say or who did not complete the question have been removed from the below analyses for brevity.\nFinally, the following analyses are only presented at the trait rather than facet level due to the number of groups and the distributions are shown in proportions rather than raw counts because of difference sample sizes in each group.\n\rTable 8: Sample size by gender. Percent = % of full sample\r\r\r\rgender\r\rn\r\rpercent\r\r\r\r\r\rMan\r\r2143\r\r22.41\r\r\r\rWoman\r\r6260\r\r65.47\r\r\r\rNon-binary\r\r874\r\r9.14\r\r\r\r\rAll traits\r\rTable 9: Mean trait scores by gender (min = 1, max = 5)\r\r\r\rtrait\r\rMan\r\rWoman\r\rNon-binary\r\r\r\r\r\ragreeableness\r\r3.62\r\r3.76\r\r3.59\r\r\r\rconscientiousness\r\r3.21\r\r3.35\r\r2.94\r\r\r\rextraversion\r\r3.19\r\r3.12\r\r2.73\r\r\r\rneg_emotionality\r\r2.95\r\r3.41\r\r3.68\r\r\r\ropen_minded\r\r3.80\r\r3.83\r\r4.16\r\r\r\r\r\rAgreeableness\rWomen report slightly higher agreeableness scores than men or non-binary people although the distributions are all very similar.\n\rConscientiousness\rNon-binary people report lower conscientiousness scores than women or men. Women report slightly higher conscientiousness scores than men.\n\rExtraversion\rNon-binary people report lower extraversion scores than men or women, whose distributions are very similar.\n\rNegative emotionality\rNon-binary people report higher negative emotionality scores than women, who in turn report higher negative emotionality scores than men.\n\rOpen-mindedness\rNon-binary people report higher open-mindedness scores than women and men, who have very similar distributions.\n\r\rSexual orientation\rParticipants are given the following options:\n\rStraight/heterosexual\rGay or lesbian\rBisexual/pansexual\rAsexual\rIf you describe your sexual orientation with another term, please list this here\rPr* er not to say\r\rFor the purposes of analysis, those who described their sexual orientation with another term, responded prefer not to say or who did not complete the question have been removed from the below analyses for brevity.\nThere is a disproportionately large number of asexual participants compared to estimates for the population (~1%). One of the student groups for the project specifically aimed to study asexual people and undertook an extremely successful recruitment drive. I am very proud of them for the effort they went to for a 1st year project.\n\rTable 10: Sample size by sexual orientation Percent = % of full sample\r\r\r\rsexual_orientation\r\rn\r\rpercent\r\r\r\r\r\rStraight/ Heterosexual\r\r4915\r\r51.41\r\r\r\rAsexual\r\r2347\r\r24.55\r\r\r\rBisexual/pansexual\r\r1163\r\r12.16\r\r\r\rGay or Lesbian\r\r379\r\r3.96\r\r\r\r\rAll traits\r\rTable 11: Mean trait scores by sexual orientation (min = 1, max = 5)\r\r\r\rtrait\r\rStraight/ Heterosexual\r\rBisexual/pansexual\r\rGay or Lesbian\r\rAsexual\r\r\r\r\r\ragreeableness\r\r3.77\r\r3.58\r\r3.65\r\r3.65\r\r\r\rconscientiousness\r\r3.47\r\r2.98\r\r3.15\r\r3.10\r\r\r\rextraversion\r\r3.34\r\r3.15\r\r3.12\r\r2.61\r\r\r\rneg_emotionality\r\r3.14\r\r3.71\r\r3.58\r\r3.42\r\r\r\ropen_minded\r\r3.68\r\r4.03\r\r3.92\r\r4.04\r\r\r\r\r\rAgreeableness\rHeterosexual participants report slightly higher agreeableness scores than bisexual/pansexual, gay or lesbian, or asexual participants, although the difference is relatively minor and the distributions are all quite similar.\n\rConscientiousness\rHeterosexual participants report higher conscientiousness scores than gay or lesbian or asexual participants, who in turn report higher scores than bisexual particpants.\n\rExtraversion\rHeterosexual people report higher extraversion scores than gay or lesbian and bisexual/pansexual people, who in turn report higher extraversion scores than asexual people.\n\rNegative emotionality\rBisexual/pansexual people reported higher negative emotionality followed by gay or lesbian then asexual people. Heterosexual participants report the lowest negative emotionality.\n\rOpen-mindedness\rHeterosexual participants reported the lowest open-mindedness followed by gay or lesbian and then bisexual/pansexual and asexual people, the latter two groups being extremely similar in distribution and mean scores.\n\r\rNeurodiversity\rParticipants were given the following definition and options:\nNeurodivergent is an umbrella term for an evolving list of diagnoses that reflect different neurotypes. Usually referring to autism, ADHD, and specific learning differences like dyslexia, dyspraxia, dyscalculia and more. Do you identify as neurodivergent?\n\rNo\rYes currently diagnosed\rYes seeking diagnosis\rYes but not seeking/sought diagnosis\rPrefer not to say\r\rFor the purposes of analysis, those who responded prefer not to say or who did not complete the question have been removed from the below analyses for brevity. Anyone who responded to any of the “yes” options above was categorised as neurodivergent. Anyone who responded “no” was categorised as neurotypical.\nAdditionally, we did not collect data on which neurotype participants identified as and it would be reasonable to predict that personality profiles may differ between e.g., autistic people and those with ADHD, therefore this is a limitation of the study.\n\rTable 12: Sample size by neurodivergent status. Percent = % of full sample\r\r\r\rneurodivergent\r\rn\r\rpercent\r\r\r\r\r\rNeurotypical\r\r6323\r\r66.13\r\r\r\rDiagnosed\r\r1197\r\r12.52\r\r\r\rSeeking diagnosis\r\r847\r\r8.86\r\r\r\rNot seeking diagnosis\r\r796\r\r8.33\r\r\r\r\rAll traits\r\rTable 13: Mean trait scores by neurodivergent status (min = 1, max = 5)\r\r\r\rtrait\r\rNeurodivergent\r\rNeurotypical\r\r\r\r\r\ragreeableness\r\r3.60\r\r3.75\r\r\r\rconscientiousness\r\r2.97\r\r3.42\r\r\r\rextraversion\r\r2.91\r\r3.19\r\r\r\rneg_emotionality\r\r3.64\r\r3.18\r\r\r\ropen_minded\r\r4.02\r\r3.77\r\r\r\r\r\rAgreeableness\rMean agreeableness scores are very similar although the distribution of neurodivergent agreeableness scores is shifted to the left (i.e., lower scores).\n\rConscientiousness\rNeurotypical participants reported higher conscientiousness scores than neurodivergent participants.\n\rExtraversion\rNeurotypical participants reported higher extraversion scores than neurodivergent participants.\n\rNegative emotionality\rNeurodivergent participants reported higher negative emotionality scores than neurotypical participants.\n\rOpen-mindedness\rNeurodivergent participants reported higher open-mindedness scores than neurotypical participants.\n\r\rBirth order\rParticipants were asked how many siblings they have and their birth order. For the purpose of this analysis, participants are categorised as only-children, first-born, middle-born, and last-born. Participants who failed to answer either or both of the sibling and birth order question were removed from this analysis.\nPlease note that there are a number of variables which may mediate the relationship between birth order and personality that we did not collect data on. For example, the age between siblings, whether one grew up living with one’s siblings, and whether siblings were full-siblings, half-siblings, step-siblings, or adopted siblings are all likely to impact the relationship but we do not have data on these factors.\n\rTable 14: Sample size by birth order. Percent = % of full sample\r\r\r\rbirth_order_recoded\r\rn\r\rpercent\r\r\r\r\r\rFirst-born\r\r3412\r\r39.20\r\r\r\rLast-born\r\r2891\r\r33.22\r\r\r\rMiddle-born\r\r1801\r\r20.69\r\r\r\rOnly child\r\r599\r\r6.88\r\r\r\r\rAll traits\r\rTable 15: Mean trait scores by birth order (min = 1, max = 5)\r\r\r\rtrait\r\rOnly child\r\rFirst-born\r\rMiddle-born\r\rLast-born\r\r\r\r\r\ragreeableness\r\r3.61\r\r3.71\r\r3.74\r\r3.72\r\r\r\rconscientiousness\r\r3.17\r\r3.30\r\r3.30\r\r3.26\r\r\r\rextraversion\r\r3.01\r\r3.12\r\r3.10\r\r3.10\r\r\r\rneg_emotionality\r\r3.43\r\r3.29\r\r3.31\r\r3.36\r\r\r\ropen_minded\r\r4.00\r\r3.86\r\r3.81\r\r3.83\r\r\r\r\r\rAgreeableness\rOnly children report slightly lower agreeableness scores than those with siblings although the distributions and mean scores are all very similar.\n\rConscientiousness\rThe distributions and mean scores for conscientiousness are all very similar.\n\rExtraversion\rThe distributions and mean scores for extraversion are all very similar.\n\rNegative emotionality\rOnly children reported slightly higher negative emotionality than those with siblings, for whom the distributions were all very similar.\n\rOpen-mindedness\rOnly children reported higher open-mindedness than those with siblings, for whom the distributions were all very similar.\n\r\rConclusions\rPlease treat the analysis presented in this document with due caution before drawing any conclusions. Separating each demographic variable means that we cannot account for any confounds or interactions that are extremely likely to be present in the data. Combined with the lack of inferential statistical analysis, these data should be viewed as providing an exploratory descriptive view of personality and individual differences for this specific sample, rather than being generalisable to the wider population.\n\r","date":1649808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649853088,"objectID":"7b6742d88e72487adba10c5ed8ec9344","permalink":"/post/2022-04-13-personality-and-individual-differences/","publishdate":"2022-04-13T00:00:00Z","relpermalink":"/post/2022-04-13-personality-and-individual-differences/","section":"post","summary":"Introduction\rPersonality measures\rSample context and limitations\rTrait scores (overall)\rFacet scores (overall)\r\rAgreeableness facets\rConscientiousness facets\rExtraversion facets\rNegative emotionality facets\rOpen-mindedness facets\r\rIndividual differences\rAge\r\rTrait scores\rFacet scores\r\rGender\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rSexual orientation\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rNeurodiversity\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rBirth order\r\rAll traits\rAgreeableness\rConscientiousness\rExtraversion\rNegative emotionality\rOpen-mindedness\r\rConclusions\r\r\rIntroduction\rAt the request of participants, this analysis is a summary of the results of a study conducted on personality and individual differences in the School of Psychology and Neuroscience, University of Glasgow, January - March 2022.","tags":[],"title":"Personality and individual differences","type":"post"},{"authors":null,"categories":null,"content":"\rDear Kathleen,\nThis is your thesis. As interpreted by a nerd.\nYour thesis has 80613 words in total. Don’t freak out if this is different to something you wrote on an official form, R will count every word in every table and footnote. What matters is that it is 28613 words longer than your wife’s thesis.\n\n\nIn your thesis you used 12669 unique words and numbers. But which chapter had the highest proportion of unique words?\n\nChapter 1 would win at Scrabble.\n\nBut what are all those words? Here are the top words used per chapter, excluding “stop words” which are common function words like “and”, “the”, “it”, etc.\n\n\nIf you’re so nerdily inclined, you can also look at bigrams, pairs of consecutive words (excluding stop words).\n\n\nHonestly, I’m not completely sure that this graph is at all useful, but it looks nice doesn’t it?\n\n\nWho doesn’t love a word cloud? Here’s your top 50 words of the entire thesis (excluding stop words).\n\n\nWhat about the most positive and negative words?\n\n\nFinally, because I love you, a pie chart.\nLOOK HOW HARD IT IS TO READ.\n\n\nHere’s a clearer one.\n\n","date":1637712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637712104,"objectID":"94f7401ebc9b01da4cab46eccedc1097","permalink":"/post/this-is-your-thesis/","publishdate":"2021-11-24T00:00:00Z","relpermalink":"/post/this-is-your-thesis/","section":"post","summary":"Dear Kathleen,\nThis is your thesis. As interpreted by a nerd.\nYour thesis has 80613 words in total. Don’t freak out if this is different to something you wrote on an official form, R will count every word in every table and footnote.","tags":null,"title":"This is your thesis","type":"post"},{"authors":[],"categories":[],"content":"\r\rThis analysis uses the package rtweet to collect tweets containing the #DragRaceUK hashtag - there is a good tutorial here if you want more information.\nFirst, load the relevant packages, download the tweets, and clean it up a little.\nlibrary(rtweet)\rlibrary(tidytext)\rlibrary(tidyverse)\rlibrary(gridExtra)\rlibrary(lubridate)\r# get tweets with the #DragRaceUK and #Team hashtags\rtweets \u0026lt;- search_tweets(\u0026quot;#DragRaceUK OR #TeamLawrence OR #TeamBimini OR #TeamEllie OR #TeamTayce\u0026quot;, n = 18000, include_rts = FALSE)\rdat \u0026lt;- tweets %\u0026gt;%\rmutate(text = str_replace_all(text, \u0026quot;[^\\x01-\\x7F]\u0026quot;, \u0026quot;\u0026quot;),\rtext = str_replace_all(text, \u0026quot;#DragRaceUK\u0026quot;, \u0026quot;\u0026quot;),\rtext = str_replace_all(text, \u0026quot;DragRace\u0026quot;, \u0026quot;\u0026quot;),\rtext = str_replace_all(text, \u0026quot;dragrace\u0026quot;, \u0026quot;\u0026quot;),\rtext = str_replace_all(text, \u0026quot;\\\\.|[[:digit:]]+\u0026quot;, \u0026quot;\u0026quot;),\rtext = str_replace_all(text, \u0026quot;https|amp|tco\u0026quot;, \u0026quot;\u0026quot;))%\u0026gt;%\rselect(created_at, text)%\u0026gt;%\rmutate(tweet = row_number())\rNow, use tidy text tools to separate the words.\n# create tidy text\rdat_token \u0026lt;- dat %\u0026gt;% unnest_tokens(word, text) %\u0026gt;%\ranti_join(stop_words, by = \u0026quot;word\u0026quot;)\r# convert tidy text back to wide so that all in lower case etc.\rdat_lower \u0026lt;- dat_token %\u0026gt;%\rgroup_by(tweet) %\u0026gt;%\rsummarise(text = str_c(word, collapse = \u0026quot; \u0026quot;))\r## `summarise()` ungrouping output (override with `.groups` argument)\r# add in column to say if each queen mentioned in tweet\rdat_lower \u0026lt;- dat_lower %\u0026gt;% mutate(lawrence = case_when(str_detect(text, \u0026quot;.lawrence\u0026quot;) ~ TRUE, TRUE ~ FALSE),\rellie = case_when(str_detect(text, \u0026quot;.ellie\u0026quot;) ~ TRUE, TRUE ~ FALSE),\rtayce = case_when(str_detect(text, \u0026quot;.tayce\u0026quot;) ~ TRUE, TRUE ~ FALSE),\rbimini = case_when(str_detect(text, \u0026quot;.bimini\u0026quot;) ~ TRUE, TRUE ~ FALSE)\r)\r# create tidy text with the mention columns\rdat_mentions \u0026lt;- dat_lower %\u0026gt;% unnest_tokens(word, text) %\u0026gt;%\ranti_join(stop_words, by = \u0026quot;word\u0026quot;)\rThe first plot looks at the raw number of tweets that mention each Queen. This isn’t a perfect measure because it relies upon people on Twitter spelling the names correctly and people on the internet can’t spell.\ndat_token %\u0026gt;%\rfilter(word %in% c(\u0026quot;lawrence\u0026quot;, \u0026quot;bimini\u0026quot;, \u0026quot;tayce\u0026quot;, \u0026quot;ellie\u0026quot;))%\u0026gt;%\rcount(word, sort = TRUE) %\u0026gt;%\rhead(20) %\u0026gt;%\rmutate(word = str_to_title(word),\rword = reorder(word, n))%\u0026gt;%\rggplot(aes(x = word, y = n, fill = word)) +\rgeom_col(show.legend = FALSE) +\rcoord_flip()+\rscale_y_continuous(name = \u0026quot;Number of tweets\u0026quot;)+\rscale_x_discrete(name = \u0026quot;Queens\u0026quot;) +\rtheme_minimal() +\rscale_fill_viridis_d()\rdat_token %\u0026gt;%\rfilter(word %in% c(\u0026quot;teamlawrence\u0026quot;, \u0026quot;teambimini\u0026quot;, \u0026quot;teamtayce\u0026quot;, \u0026quot;teamellie\u0026quot;))%\u0026gt;%\rcount(word, sort = TRUE) %\u0026gt;%\rhead(20) %\u0026gt;%\rmutate(word = str_to_title(word),\rword = reorder(word, n))%\u0026gt;%\rggplot(aes(x = word, y = n, fill = word)) +\rgeom_col(show.legend = FALSE) +\rcoord_flip()+\rscale_y_continuous(name = \u0026quot;Number of tweets\u0026quot;)+\rscale_x_discrete(name = \u0026quot;Queens\u0026quot;) +\rtheme_minimal() +\rscale_fill_viridis_d()\rThe next lot of code runs a sentiment analysis on the tweets that each Queen is mentioned in. Sentiment analyses using existing ratings of words (e.g., if they’re positive or negative) to give you a sense of whether the queen is being mentioned in a tweet that is overall positive or negative in tone. Again it’s not perfect, it can’t cope with slang (e.g., it will think that a sickening death drop is a bad thing), but it does have face validity.\n# do a sentiment analysis for each queen\rbimini \u0026lt;- dat_mentions %\u0026gt;%\rfilter(bimini == \u0026quot;TRUE\u0026quot;)%\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;))%\u0026gt;%\rcount(index = tweet, sentiment) %\u0026gt;%\rspread(sentiment, n, fill = 0) %\u0026gt;%\rmutate(sentiment = positive - negative)%\u0026gt;%\rmutate(queen = \u0026quot;bimini\u0026quot;)\rlawrence \u0026lt;- dat_mentions %\u0026gt;%\rfilter(lawrence == \u0026quot;TRUE\u0026quot;)%\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;))%\u0026gt;%\rcount(index = tweet, sentiment) %\u0026gt;%\rspread(sentiment, n, fill = 0) %\u0026gt;%\rmutate(sentiment = positive - negative)%\u0026gt;%\rmutate(queen = \u0026quot;lawrence\u0026quot;)\rtayce \u0026lt;- dat_mentions %\u0026gt;%\rfilter(tayce == \u0026quot;TRUE\u0026quot;)%\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;))%\u0026gt;%\rcount(index = tweet, sentiment) %\u0026gt;%\rspread(sentiment, n, fill = 0) %\u0026gt;%\rmutate(sentiment = positive - negative)%\u0026gt;%\rmutate(queen = \u0026quot;tayce\u0026quot;)\rellie \u0026lt;- dat_mentions %\u0026gt;%\rfilter(ellie == \u0026quot;TRUE\u0026quot;)%\u0026gt;%\rinner_join(get_sentiments(\u0026quot;bing\u0026quot;))%\u0026gt;%\rcount(index = tweet, sentiment) %\u0026gt;%\rspread(sentiment, n, fill = 0) %\u0026gt;%\rmutate(sentiment = positive - negative)%\u0026gt;%\rmutate(queen = \u0026quot;ellie\u0026quot;)\r# combine sentiment analysis for each queen into one tibble and then calculate total positive, negative\r# and overall sentiment scores for each queen\rdat_sentiment \u0026lt;- bind_rows(lawrence, ellie, bimini, tayce) %\u0026gt;%\rgroup_by(queen) %\u0026gt;%\rsummarise(positive = sum(positive),\rnegative = sum(negative),\roverall = sum(sentiment))%\u0026gt;%\rgather(positive:overall, key = type, value = score)%\u0026gt;%\rmutate(type = factor(type, levels = c(\u0026quot;positive\u0026quot;, \u0026quot;negative\u0026quot;, \u0026quot;overall\u0026quot;)))%\u0026gt;%\rmutate(queen = factor(queen, levels = c(\u0026quot;lawrence\u0026quot;, \u0026quot;ellie\u0026quot;, \u0026quot;bimini\u0026quot;, \u0026quot;tayce\u0026quot;)))\r# display table of sentiment scores\rtbl \u0026lt;- dat_sentiment %\u0026gt;%\rspread(type, score)%\u0026gt;%\rarrange(desc(overall))\rgrid.table(tbl)\r# create plot of the sentiment scores by each queen, ordered by overall score\rggplot(dat_sentiment, aes(x = type, y = score, fill = type)) +\rstat_identity(geom = \u0026quot;bar\u0026quot;, position = \u0026quot;dodge\u0026quot;, show.legend = FALSE)+\rfacet_wrap(~ queen, ncol = 3)+\rcoord_flip()+\rscale_fill_manual(values = c(\u0026quot;positive\u0026quot; = \u0026quot;green\u0026quot;, \u0026quot;negative\u0026quot; = \u0026quot;red\u0026quot;, \u0026quot;overall\u0026quot; = \u0026quot;blue\u0026quot;))\r","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616014712,"objectID":"22b2501c278ed48cf71bc2d0f831360e","permalink":"/post/drag-race-uk/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/post/drag-race-uk/","section":"post","summary":"This analysis uses the package rtweet to collect tweets containing the #DragRaceUK hashtag - there is a good tutorial here if you want more information.\nFirst, load the relevant packages, download the tweets, and clean it up a little.","tags":["rstats"],"title":"Drag Race UK","type":"post"},{"authors":[],"categories":[],"content":"\r\r\r\r\r\r\r\rThis is the winning entry to our Drag Racer competition by UofG psychology student Ellie Brownlie.\nIntro\rSee final section, Beauty fades, data vis is forever! (Summary), for full summary.\nBackground: RuPaul’s Drag Race is a hit American TV show featuring Drag Queen contestants competing against one another in a variety of challanges to become America’s ‘next drag superstar’. Some have hypothesied that Bianca Del Rio, a constestant featured in the 6th season of RuPaul’s Drag Race, was the greatest contestant of all time but little analysis of data has invesitgated this.\nMethods: Data collected within the RStudio package ‘DragRaceR’ categorises each individual contestant’s episode outcome. Outcome data was transformed into heirarchical, numerical outcome scores ranging from 0 to 10, awarded for each contestant’s episode outcomes. Outcome scores were then divided by the number of episodes each contestant appeared in to produce a ‘weighted outcome score’ in order to account for differing numbers of episode per season and individual queens returning for multiple seasons.\nResults: Data visualisations were produced from the running weighted outcome scores of each contestant. Comparison of final, cumulative, weighted outcome scores show that Bianca Del Rio achieved the highest score on this measure, even after accounting for the number of episodes she appeared in. She was closely followed by Bob the Drag Queen but was well above the average of other queens.\nConclusion: Bianca Del Rio is the best drag race contestant of all time when cumulative weighted outcome scores of RuPaul’s Drag Race contestants are compared.\n\rWerk\rReading in is fundemental\r# Package names\rpackages \u0026lt;- c(\u0026quot;dragracer\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;plotly\u0026quot;, \u0026quot;pacman\u0026quot;, \u0026quot;grid\u0026quot;, \u0026quot;ggimage\u0026quot;, \u0026quot;showtext\u0026quot;, \u0026quot;knitr\u0026quot;)\r#Package Uses\r###\u0026quot;dragracer\u0026quot; = data package\r###\u0026quot;tidyverse\u0026quot; = data wrangling ###\u0026quot;plotly\u0026quot; = plot interactivity ###\u0026quot;pacman\u0026quot; = plot picture background ###\u0026quot;grid\u0026quot; = plot picture background ###\u0026quot;ggimage\u0026quot; = plot picture geom_point ###\u0026quot;showtext\u0026quot; = plot fonts\r###\u0026quot;knitr\u0026quot; = make pretty markdown tables # Install packages not yet installed (will not re-install packages that are already installed)\r#installed_packages \u0026lt;- packages %in% rownames(installed.packages())\r#if (any(installed_packages == FALSE)) {\r# install.packages(packages[!installed_packages])\r#}\r# Packages loading\rinvisible(lapply(packages, library, character.only = TRUE))\rhead(rpdr_contep) \r## # A tibble: 6 x 11\r## season rank missc contestant episode outcome eliminated participant minichalw\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 S01 1 0 BeBe Zaha~ 1 SAFE 0 1 0 ## 2 S01 2 1 Nina Flow~ 1 WIN 0 1 0 ## 3 S01 3 0 Rebecca G~ 1 LOW 0 1 0 ## 4 S01 4 0 Shannel 1 SAFE 0 1 0 ## 5 S01 5 0 Ongina 1 HIGH 0 1 0 ## 6 S01 6 0 Jade 1 SAFE 0 1 0 ## # ... with 2 more variables: finale \u0026lt;dbl\u0026gt;, penultimate \u0026lt;dbl\u0026gt;\r\rData Wrangling is not a contact sport\rEliminate incomplete data points:\nrpdr_contep \u0026lt;- na.omit(rpdr_contep) \routcomes \u0026lt;- rpdr_contep %\u0026gt;% group_by(outcome) %\u0026gt;% count()\rCreate dummy variable assigning points to each contestant based on the outcome of each episode:\n\r\r\r\rOutcome\rCoded Criteria\rPoints Assigned\r\r\r\rOverall Win (final episode)\rfinale == 1 \u0026amp; outcome == \"WIN\"\r10\r\rOverall Miss Congenitality (final episode)\rfinale == 1 \u0026amp; missc == 1, outcome == \"MISSCON\"\r8\r\rMain Challange Win\routcome == \"WIN\"\r6\r\rMini Challange Win\rminichalw == 1\r5\r\rTop\routcome == \"HIGH\"\r4\r\rSafe, or Safe and Deptarted\routcome == \"SAFE\", outcome == \"SAFE + DEPT\"\r3\r\rBottom (Non LipSync)\routcome == \"LOW\"\r2\r\rShantied\routcome == \"BTM\" \u0026amp; eliminated == 0\"\r1\r\rEliminated, lost first round of finale lip syncs or did not appear or ‘RUNNING’ (effectively NA)\routcome == \"BTM\" \u0026amp; eliminated == 1, outcome == \"LOST2ND ROUND\", outcome == \"LOST1ST ROUND\", outcome == \"RUNNING\"\r0\r\rReturned (but did not contest) or Running (effectively NA)\routcome == \"RTRN\", outcome == \"RUNNING\"\rfiltered out\r\r\r\rpoints_rpdr_contep \u0026lt;- rpdr_contep %\u0026gt;%\rmutate(ep_point = case_when(\r(finale == 1 \u0026amp; outcome == \u0026quot;WIN\u0026quot;) ~ 10, #Overall Win (finale == 1 \u0026amp; missc == 1) ~ 8, #Overall Miss Congenitality\r(outcome == \u0026quot;MISSCON\u0026quot;) ~ 8, #Overall Miss Congenitality\r(outcome == \u0026quot;WIN\u0026quot;) ~ 6, #Main Challange Win\r(outcome == \u0026quot;WIN+RTRN\u0026quot;) ~ 6, #Main Challange Win\r(minichalw == 1) ~ 5, #Mini Challange Win\r(outcome == \u0026quot;LOST3RD ROUND\u0026quot;) ~ 5, #Won first round of finale lip syncs (lost 2nd)\r(outcome == \u0026quot;HIGH\u0026quot;) ~ 4, #Top\r(outcome == \u0026quot;TOP2\u0026quot;) ~ 4, #Top\r(outcome == \u0026quot;SAFE\u0026quot;) ~ 3, #Safe\r(outcome == \u0026quot;SAFE+DEPT\u0026quot;) ~ 3, #Safe\r(outcome == \u0026quot;LOW\u0026quot;) ~ 2, #Bottom (outcome == \u0026quot;BTM\u0026quot; \u0026amp; eliminated == 0) ~ 1, #Shantied\r(outcome == \u0026quot;BTM\u0026quot; \u0026amp; eliminated == 1) ~ 0, #Eliminated\r(outcome == \u0026quot;OUT\u0026quot;) ~ 0, #Eliminated\r(outcome == \u0026quot;LOST2ND ROUND\u0026quot;) ~ 0, #Lost first round of finale lip syncs\r(outcome == \u0026quot;LOST1ST ROUND\u0026quot;) ~ 0), #Lost first round of finale lip syncs .after = outcome) %\u0026gt;% filter(outcome != \u0026quot;RTRN\u0026quot;) %\u0026gt;% #Returned (but did not contest)\rfilter(outcome != \u0026quot;RUNNING\u0026quot;) #Running, (effectively NA)\rCreate count of episodes per season, add column with n of episodes per season to points_rpdr_contep\nseason_ep_count \u0026lt;- group_by(points_rpdr_contep, season, episode) %\u0026gt;% summarise() %\u0026gt;% count() %\u0026gt;% ungroup() %\u0026gt;% rename(n_season_ep = n)\rpoints_rpdr_contep \u0026lt;- inner_join(points_rpdr_contep, season_ep_count, .after = season) %\u0026gt;% select(season, n_season_ep, everything()) \rCreate count of episodes per contestant, add column with n of episodes per contestent to points_rpdr_contep\ncontest_ep_count \u0026lt;- points_rpdr_contep %\u0026gt;% group_by(contestant) %\u0026gt;% summarise(n = n()) %\u0026gt;% ungroup() %\u0026gt;%\rrename(n_contest_ep = n)\rpoints_rpdr_contep \u0026lt;- inner_join(points_rpdr_contep, contest_ep_count, by = \u0026quot;contestant\u0026quot;) %\u0026gt;% select(contestant, n_contest_ep, everything()) \rCompare, for each contestant, whether the number of episodes that they appeared in is more than the number of episodes in the season they appeared in.\n\rPossible for some contestants as some returned, but should not be more than the number of episodes within a season.\r\rcompare \u0026lt;- points_rpdr_contep %\u0026gt;%\rselect(contestant, n_season_ep, n_contest_ep) %\u0026gt;% group_by(contestant, n_season_ep, n_contest_ep) %\u0026gt;% summarise() %\u0026gt;% mutate(compare = (n_season_ep \u0026lt; n_contest_ep)) %\u0026gt;% ungroup() compare %\u0026gt;% group_by(compare) %\u0026gt;% count() %\u0026gt;% ungroup()\r## # A tibble: 2 x 2\r## compare n\r## \u0026lt;lgl\u0026gt; \u0026lt;int\u0026gt;\r## 1 FALSE 152\r## 2 TRUE 3\rcompare_constestants \u0026lt;- compare %\u0026gt;% filter(compare == TRUE)\rcompare_constestants\r## # A tibble: 3 x 4\r## contestant n_season_ep n_contest_ep compare\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;lgl\u0026gt; ## 1 Eureka O\u0026#39;Hara 12 17 TRUE ## 2 Shangela 10 11 TRUE ## 3 Vanessa Vanjie Mateo 12 13 TRUE\r\rNo contestant appeared in more episodes than the number of episodes within a season.\r\rCreate dummy variable weighting the points assigned to each to contestant outcome by the number of episodes they appear in:\npoints_rpdr_contep \u0026lt;- points_rpdr_contep %\u0026gt;% mutate(w_ep_point = (ep_point/n_contest_ep), .after = ep_point)\rCreate table with each contestants total weighted points, arrange in desending order\nscores \u0026lt;- points_rpdr_contep %\u0026gt;% group_by(contestant) %\u0026gt;% summarise(total_score = sum(w_ep_point)) %\u0026gt;% arrange(desc(total_score))\rscores\r## # A tibble: 152 x 2\r## contestant total_score\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Bianca Del Rio 4.92\r## 2 Bob the Drag Queen 4.90\r## 3 BeBe Zahara Benet 4.71\r## 4 Tyra Sanchez 4.7 ## 5 Nina Flowers 4.57\r## 6 Jinkx Monsoon 4.5 ## 7 Sasha Velour 4.5 ## 8 Sharon Needles 4.5 ## 9 Violet Chachki 4.46\r## 10 Raja 4.38\r## # ... with 142 more rows\rCreate Varible running_total\ndata_clean_new \u0026lt;- points_rpdr_contep %\u0026gt;%\rgroup_by(contestant) %\u0026gt;% mutate(running_total = 0, running_total = cumsum(w_ep_point)) %\u0026gt;% arrange(contestant, episode) \rIsolate Bianca Del Rio’s Data and create new tibble bianca\nbianca \u0026lt;- filter(data_clean_new, contestant == \u0026quot;Bianca Del Rio\u0026quot;) %\u0026gt;% mutate(image = \u0026quot;bianca_head.png\u0026quot;)\rCreate new tibble means, containing mean running total per episode\nmeans \u0026lt;- data_clean_new %\u0026gt;% group_by(episode) %\u0026gt;% summarise(running_total = mean(running_total)) %\u0026gt;% mutate(contestant = \u0026quot;Mean Average\u0026quot;) %\u0026gt;% mutate(image = \u0026quot;star_point.png\u0026quot;) \rCreate sample tables\nset.seed(96) #seed 96\rtop_5 \u0026lt;- scores %\u0026gt;% slice_head(n = 5) mean_cummulative_score \u0026lt;- scores %\u0026gt;% summarise(total_score = mean(total_score)) %\u0026gt;% mutate(contestant = \u0026quot;Mean Average\u0026quot;)\rtop_10 \u0026lt;- scores %\u0026gt;% slice_head(n = 10) %\u0026gt;% bind_rows(mean_cummulative_score)\rrandom_sample \u0026lt;- slice_sample(scores, n = 5)\rtop5_plus_random \u0026lt;- bind_rows(top_5, random_sample) %\u0026gt;% arrange(desc(total_score)) data_clean_reduced \u0026lt;- filter(points_rpdr_contep, (contestant %in% top5_plus_random$contestant)) %\u0026gt;% ungroup() %\u0026gt;%\rgroup_by(contestant) %\u0026gt;% mutate(running_total = 0, running_total = cumsum(w_ep_point)) %\u0026gt;% arrange(contestant, episode) %\u0026gt;% bind_rows(means)\r\rPlot for the Gawds\rtop5_and_sample_plot\rtop5_and_sample_plot \u0026lt;- ggplot(data_clean_reduced, aes(x = episode, y = running_total, group = contestant, color = contestant)) + geom_line() +\rgeom_point() + theme_bw() + scale_x_continuous(breaks = seq(0,15, by=1)) +\rscale_color_discrete(\u0026quot;\u0026quot;) +\rtheme(text = element_text(face = \u0026quot;bold\u0026quot;)) +\rlabs(x= \u0026quot;Episode\u0026quot;, y= \u0026quot;Running Outcome Score (Weighted)\u0026quot;, title= \u0026quot;Bianca Del Rio Is the Best Drag Race Contestant Of All Time\u0026quot;)+\rtheme(plot.background = element_rect(fill = \u0026#39;#EA259A\u0026#39;), panel.grid.major = element_line(colour = \u0026quot;grey\u0026quot;, size=0.5), panel.grid.minor = element_line(colour = \u0026quot;grey\u0026quot;), plot.title = element_text(hjust = -.1),\rtext = element_text(family = \u0026quot;\u0026quot;, colour = \u0026quot;white\u0026quot;), axis.title.x = element_text(size = 12, face = \u0026quot;bold\u0026quot;),\raxis.title.y = element_text(size = 12, face = \u0026quot;bold\u0026quot;), axis.text = element_text(colour = \u0026quot;white\u0026quot;),\rtitle = element_text(size = 14, face = \u0026quot;bold\u0026quot;),\rlegend.text = element_text(colour = \u0026quot;black\u0026quot;)) +\rscale_x_continuous(breaks = seq(0,15, by=1)) +\rlabs(x= \u0026quot;Episode\u0026quot;, y= \u0026quot;Running Outcome Score (Weighted)\u0026quot;, title= \u0026quot;Bianca Del Rio Is the Best Drag Race Contestant Of All Time\u0026quot;)\r\rmain_plot\rLoad in Image For Backgroud\nimage \u0026lt;- png::readPNG(\u0026quot;werk_background.png\u0026quot;)\rLoad in Fonts\nmyfont \u0026lt;- \u0026quot;Londrina Solid\u0026quot;\rfont_add_google(myfont, myfont)\rshowtext_auto() \rCreate main_plot\nmain_plot \u0026lt;- ggplot(data_clean_new, aes(x = episode, y = running_total, group = contestant)) + annotation_custom(rasterGrob(image, width = unit(1,\u0026quot;npc\u0026quot;), height = unit(1,\u0026quot;npc\u0026quot;)), -Inf, Inf, -Inf, Inf) + geom_point(alpha = 0.5, colour = \u0026quot;white\u0026quot;) + geom_point(data = means, position = position_dodge(width = 0.9), colour = \u0026quot;yellow\u0026quot;, shape = 19, size = 4, aes(x = episode, y = running_total)) + geom_line(data = means, position = position_dodge(width = 0.9), colour = \u0026quot;yellow\u0026quot;, shape = 19, size = 1, aes(x = episode, y = running_total)) + geom_point(data = bianca, position = position_dodge(width = 0.9), colour = \u0026quot;black\u0026quot;, shape = \u0026quot;star\u0026quot;, size = 4, aes(x = episode, y = running_total)) + geom_line(data = bianca, position = position_dodge(width = 0.9), colour = \u0026quot;black\u0026quot;, shape = 19, size = 1, aes(x = episode, y = running_total)) +\rtheme(panel.background = element_rect(fill = \u0026#39;#CB1883\u0026#39;), plot.background = element_rect(fill = \u0026#39;#D31988\u0026#39;),\rpanel.grid.major = element_line(colour = \u0026quot;grey\u0026quot;, size=0.5), panel.grid.minor = element_line(colour = \u0026quot;grey\u0026quot;), plot.title = element_text(hjust = 0.5),\rtext = element_text(family = myfont, colour = \u0026quot;white\u0026quot;), axis.title.x = element_text(size = 20, face = \u0026quot;bold\u0026quot;),\raxis.title.y = element_text(size = 20, face = \u0026quot;bold\u0026quot;), axis.text = element_text(colour = \u0026quot;white\u0026quot;),\rtitle = element_text(size = 24, face = \u0026quot;bold\u0026quot;)) + geom_image(data = means, aes(image = image), size=.075) + geom_image(data = bianca, aes(image = image), size=.075) + scale_x_continuous(breaks = seq(0,15, by=1)) +\rlabs(x= \u0026quot;Episode\u0026quot;, y= \u0026quot;Running Outcome Score (Weighted)\u0026quot;, title= \u0026quot;Bianca Del Rio Is the Best Drag Race Contestant Of All Time\u0026quot;) \rCreate interactive_main_plot\ninteractive_main_plot \u0026lt;- ggplot(data_clean_new, aes(x = episode, y = running_total, group = contestant)) + annotation_custom(rasterGrob(image, width = unit(1,\u0026quot;npc\u0026quot;), height = unit(1,\u0026quot;npc\u0026quot;)), -Inf, Inf, -Inf, Inf) + geom_point(alpha = 0.5, colour = \u0026quot;white\u0026quot;) + geom_point(data = means, position = position_dodge(width = 0.9), colour = \u0026quot;yellow\u0026quot;, shape = \u0026quot;star\u0026quot;, size = 4, aes(x = episode, y = running_total)) + geom_line(data = means, position = position_dodge(width = 0.9), colour = \u0026quot;yellow\u0026quot;, shape = 19, size = 1, aes(x = episode, y = running_total)) + geom_point(data = bianca, position = position_dodge(width = 0.9), colour = \u0026quot;blue\u0026quot;, shape = \u0026quot;star\u0026quot;, size = 4, aes(x = episode, y = running_total)) + geom_line(data = bianca, position = position_dodge(width = 0.9), colour = \u0026quot;blue\u0026quot;, shape = 19, size = 1, aes(x = episode, y = running_total)) +\rtheme(panel.background = element_rect(fill = \u0026#39;#EA259A\u0026#39;), plot.background = element_rect(fill = \u0026#39;#D31988\u0026#39;), panel.grid.major = element_line(colour = \u0026quot;grey\u0026quot;, size=0.5), panel.grid.minor = element_line(colour = \u0026quot;grey\u0026quot;), plot.title = element_text(hjust = 0.5),\rtext = element_text(family = \u0026quot;\u0026quot;, colour = \u0026quot;white\u0026quot;), axis.title.x = element_text(size = 14, face = \u0026quot;bold\u0026quot;),\raxis.title.y = element_text(size = 14, face = \u0026quot;bold\u0026quot;), axis.text = element_text(colour = \u0026quot;white\u0026quot;),\rtitle = element_text(size = 16, face = \u0026quot;bold\u0026quot;)) + scale_x_continuous(breaks = seq(0,15, by=1)) +\rlabs(x= \u0026quot;Episode\u0026quot;, y= \u0026quot;Running Outcome Score (Weighted)\u0026quot;, title= \u0026quot;Bianca Del Rio Is the Best Drag Race Contestant Of All Time\u0026quot;)\r\r\r\rBeauty fades, data vis is forever! (Summary)\rBackground\r\rRuPaul’s Drag Race is a hit American TV show featuring Drag Queen contestants competeing against one another in a variety of challanges to become America’s ‘next drag superstar’. Some have hypothesied that Bianca Del Rio, a constestant featured in the 6th season of RuPaul’s Drag Race, was the greatest contestant of all time but little analysis of data has invesitgated this.\r\r\rMethods\r\rData collected within the RStudio package ‘DragRaceR’ categorises each individual contestant’s episode outcome. Outcome data was transformed using RStudio and the package tidyverse into heirarchical, numerical outcome scores ranging from 0 to 10, awarded for each contestant’s episode outcomes.\n\rSummary of Numerical Coding\n\r\r\r\r\rOutcome\rCoded Criteria\rPoints Assigned\r\r\r\rOverall Win (final episode)\rfinale == 1 \u0026amp; outcome == \"WIN\"\r10\r\rOverall Miss Congenitality (final episode)\rfinale == 1 \u0026amp; missc == 1, outcome == \"MISSCON\"\r8\r\rMain Challange Win\routcome == \"WIN\"\r6\r\rMini Challange Win\rminichalw == 1\r5\r\rTop\routcome == \"HIGH\"\r4\r\rSafe, or Safe and Deptarted\routcome == \"SAFE\", outcome == \"SAFE + DEPT\"\r3\r\rBottom (Non LipSync)\routcome == \"LOW\"\r2\r\rShantied\routcome == \"BTM\" \u0026amp; eliminated == 0\"\r1\r\rEliminated, lost first round of finale lip syncs or did not appear or ‘RUNNING’ (effectively NA)\routcome == \"BTM\" \u0026amp; eliminated == 1, outcome == \"RTRN\", outcome == \"LOST2ND ROUND\", outcome == \"LOST1ST ROUND\", outcome == \"RUNNING\"\r0\r\rReturned (but did not contest) or Running (effectively NA)\routcome == \"RTRN\", outcome == \"RUNNING\"\rfiltered out\r\r\r\r\rOutcome scores were then divided by the number of episodes each contestant appeared in to produce a ‘weighted outcome score’ in order to account for differing numbers of episode per season (series 1 had 7 episodes, latest seasons had 14) and individual queens returning for multiple seasons e.g. Eureka O’ Hara (featured in 17 episodes - intitally appeared in season 9, departed due to death-drop related injury, returned for season 10).\n\rBoth a running score by episode and a cummulative, weighted outcome score were calculated for each contestant.\n\r\r\rResults\rTable of Top 10 Contestants and Mean Average by Cumlative, Weighted Outcome Score\rkable(top_10, col.names = c(\u0026quot;Contestant\u0026quot;, \u0026quot;Total Outcome Score\u0026quot;))\r\r\rContestant\rTotal Outcome Score\r\r\r\rBianca Del Rio\r4.916667\r\rBob the Drag Queen\r4.900000\r\rBeBe Zahara Benet\r4.714286\r\rTyra Sanchez\r4.700000\r\rNina Flowers\r4.571429\r\rJinkx Monsoon\r4.500000\r\rSasha Velour\r4.500000\r\rSharon Needles\r4.500000\r\rViolet Chachki\r4.461538\r\rRaja\r4.384615\r\rMean Average\r2.623044\r\r\r\r\rShows cumlative, weighted outcome scores of the top 10 contestants.\r\rBianca Del Rio achieved the highest, cummulative weighted outcome score, but was followed closely by Bob the Drag Queen.\rBianca Del Rio was well above the mean average cummulative, weighted outcome score.\r\rData visualisations were produced from the running, weighted outcome scores per epsiode of each contestant.\r\r\rMain Plot\rmain_plot\r\rShows the individual running, weighted outcome scores by episode for each contestant (white points), with Bianca Del Rio’s running, weighted outcome score highlighted using her face. The mean average running total of all contestants is highlighted using yellow stars.\r\rBetween episodes 1 to 7, Bianca Del Rio’s running, weighted outcome score is below the average running weighted outcome score, is approximately at the average after episode 8 and above the average running weighted outcome score after episode 8.\rBy episode 14, Bianca Del Rio’s cummulative, weighted outcome score (the last of her running, weighted outcome score) is highest of all the contestants.\r\r\rInteractive Main Plot\rggplotly(interactive_main_plot)\r\r{\"x\":{\"data\":[{\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"mode\":\"\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,14,1,2,3,4,5,1,3,4,5,6,7,8,9,10,11,12,14,2,3,4,5,6,1,2,3,4,5,6,7,1,2,3,1,2,3,4,5,6,7,8,9,10,11,14,2,3,4,5,6,7,8,9,10,11,12,13,15,1,2,3,4,5,6,7,8,9,10,11,1,1,2,3,4,5,6,7,8,9,1,3,4,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,8,1,3,4,5,6,7,8,9,10,11,2,3,4,5,6,7,8,9,10,11,12,14,1,2,3,4,5,6,1,2,3,4,5,6,7,8,9,10,1,3,4,5,6,7,1,2,3,4,5,6,7,8,9,10,11,12,14,2,3,4,5,6,7,8,9,10,12,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9,10,2,3,4,5,6,7,8,9,10,11,12,14,1,3,4,5,6,7,8,9,10,11,12,14,1,2,2,3,3,4,5,6,2,3,2,3,4,5,6,7,8,9,10,11,12,1,2,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,6,7,8,9,1,2,3,4,1,1,2,2,3,3,4,4,5,5,6,7,8,9,10,11,14,1,2,3,4,5,6,7,8,1,3,4,5,1,3,4,5,6,7,8,9,10,11,12,14,1,2,3,4,5,6,7,8,9,10,11,12,14,1,3,4,5,6,7,8,9,10,11,1,2,3,1,2,3,4,2,3,4,5,1,2,3,4,5,6,7,8,1,3,4,5,6,7,8,9,10,11,12,1,2,3,4,1,2,3,4,5,6,2,3,4,5,6,7,8,9,10,11,12,14,1,2,3,4,5,6,7,8,2,3,4,5,6,7,8,1,2,3,8,1,2,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,8,9,10,11,14,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,11,1,2,1,2,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,8,1,2,3,4,5,6,7,8,9,10,11,1,1,2,3,4,5,6,7,8,9,10,11,12,1,2,3,4,5,10,1,2,3,4,5,6,7,8,9,10,1,2,3,1,3,4,5,6,7,8,1,2,1,2,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,1,2,3,4,2,2,3,4,5,6,7,8,9,10,11,12,13,15,2,3,4,5,6,1,2,3,4,5,6,7,8,1,2,3,4,5,1,2,3,4,1,2,3,4,5,6,2,3,4,5,6,2,3,4,1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,6,7,8,9,10,1,2,3,1,2,3,4,5,6,7,1,2,3,4,5,1,2,3,4,5,8,1,2,3,1,2,3,4,5,6,7,8,9,10,1,3,4,1,3,4,5,1,2,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,8,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8,9,10,11,12,14,1,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,7,8,9,10,11,14,2,3,1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,2,3,4,5,6,7,8,9,10,11,12,13,15,1,2,3,4,5,6,7,8,9,11,1,2,3,4,5,6,8,1,2,3,4,5,6,2,3,4,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,1,2,8,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,1,2,1,2,3,4,5,6,7,8,9,10,11,1,2,3,4,5,6,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,11,12,14,1,1,2,3,4,2,3,4,5,6,7,1,2,1,2,3,4,5,6,7,8,9,1,8,1,2,3,1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9,10,11,14,1,2,3,4,8,9,10,1,2,3,4,5,6,7,8,9,11,1,2,3,4,5,6,7,8,9,1,1,2,3,4,5,6,7,8,9,10,11,12,2,1,1,2,3,4,5,6,7,8,9,10,11,12,14,1,3,1,2,3,4,1,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,2,3,4,5,6,7,8,9,10,11,12,13,1,2,3,1,2,3,4,5,6,7,8,9,10,11,12,14],\"y\":[0.307692307692308,0.538461538461539,0.615384615384615,0.846153846153846,1.07692307692308,1.53846153846154,1.61538461538462,1.84615384615385,2.30769230769231,2.53846153846154,2.84615384615385,3.07692307692308,3.07692307692308,0.8,1.6,2.4,3,3,0.166666666666667,0.333333333333333,0.666666666666667,1,1.5,2,2.25,2.33333333333333,2.41666666666667,2.91666666666667,3.41666666666667,3.75,0.8,1.4,1.8,2.6,2.6,0.428571428571429,0.857142857142857,1,1.42857142857143,1.85714285714286,2.28571428571429,2.28571428571429,0.333333333333333,0.666666666666667,0.666666666666667,0.333333333333333,0.583333333333333,0.75,1,1.33333333333333,1.58333333333333,2,2.5,2.83333333333333,3.25,3.75,4.08333333333333,0.230769230769231,0.692307692307692,1.15384615384615,1.38461538461538,1.69230769230769,1.76923076923077,1.92307692307692,2.38461538461538,2.53846153846154,2.61538461538462,3,3.07692307692308,3.07692307692308,0.272727272727273,0.545454545454545,0.818181818181818,1.09090909090909,1.54545454545455,2.09090909090909,2.36363636363636,2.45454545454545,2.63636363636364,2.81818181818182,2.81818181818182,0,0.333333333333333,0.777777777777778,1.11111111111111,1.77777777777778,2.33333333333333,2.55555555555556,2.66666666666667,2.77777777777778,2.88888888888889,1.33333333333333,1.66666666666667,1.66666666666667,0.25,0.5,0.75,1.25,1.5,1.75,2.25,2.41666666666667,2.83333333333333,3,3.5,4.33333333333333,0.6,1,1.6,2.2,2.2,0.25,0.583333333333333,1.08333333333333,1.25,1.5,1.91666666666667,2.08333333333333,2.5,3,3.33333333333333,3.75,3.75,0.428571428571429,0.857142857142857,1.71428571428571,2.28571428571429,2.42857142857143,3.28571428571429,4.71428571428571,0.6,1,1.4,2,2.3,2.4,2.6,3.1,3.5,4,0.5,0.75,1,1.33333333333333,1.66666666666667,2,2.5,2.75,3.25,3.58333333333333,4.08333333333333,4.91666666666667,0.666666666666667,1.16666666666667,1.83333333333333,2.33333333333333,2.83333333333333,2.83333333333333,0.3,0.6,1.2,1.5,2.1,2.6,3.2,3.3,3.9,4.9,0.666666666666667,1.16666666666667,1.33333333333333,1.66666666666667,1.83333333333333,1.83333333333333,0.461538461538462,0.615384615384615,0.846153846153846,1.15384615384615,1.61538461538462,1.84615384615385,2.15384615384615,2.23076923076923,2.53846153846154,2.84615384615385,3.30769230769231,3.38461538461539,3.76923076923077,0.2,0.4,0.7,1,1.3,1.8,2.2,2.3,2.3,2.3,0.25,0.75,1.08333333333333,1.33333333333333,1.83333333333333,2.08333333333333,2.25,2.5,2.83333333333333,3.16666666666667,3.58333333333333,3.91666666666667,0.75,1.25,2,2,0.333333333333333,1,1.33333333333333,1.44444444444444,1.77777777777778,2,2.11111111111111,2.66666666666667,2.66666666666667,0.3,0.7,0.8,1.1,1.4,1.5,2.1,2.4,2.5,2.5,0.25,0.5,1,1.25,1.5,1.66666666666667,1.91666666666667,2.41666666666667,2.58333333333333,2.75,3.25,3.58333333333333,0.25,0.416666666666667,0.666666666666667,0.916666666666667,1.08333333333333,1.41666666666667,1.75,2.08333333333333,2.41666666666667,2.91666666666667,3,3,0.375,0.75,1.125,0.75,1.5,1.875,2,2,1,1,0.0909090909090909,0.636363636363636,0.818181818181818,1.09090909090909,1.54545454545455,1.63636363636364,2,2.27272727272727,2.45454545454545,2.54545454545455,2.54545454545455,1.5,1.5,0.428571428571429,0.571428571428571,1,1.42857142857143,1.57142857142857,2.14285714285714,2.14285714285714,0.5,0.875,1.125,1.375,1.875,2,2.75,2.75,0.454545454545455,0.727272727272727,1.27272727272727,1.54545454545455,2,2.45454545454545,2.72727272727273,3.09090909090909,3.27272727272727,3.36363636363636,3.36363636363636,0.333333333333333,0.555555555555556,0.666666666666667,1,1.33333333333333,1.55555555555556,1.88888888888889,2.22222222222222,2.22222222222222,0.5,1.5,2.25,2.25,0.235294117647059,1.17647058823529,0.411764705882353,1.23529411764706,0.588235294117647,1.47058823529412,0.823529411764706,1.64705882352941,1,2,2.35294117647059,2.64705882352941,2.82352941176471,2.88235294117647,3.17647058823529,3.41176470588235,3.70588235294118,0.375,0.75,1,1.375,1.5,1.75,2.125,2.125,1,2,2.75,2.75,0.333333333333333,0.583333333333333,1.08333333333333,1.41666666666667,1.91666666666667,2.41666666666667,2.58333333333333,2.75,3,3.25,3.75,3.75,0.230769230769231,0.692307692307692,0.923076923076923,1.23076923076923,1.46153846153846,1.84615384615385,2.30769230769231,2.38461538461538,2.84615384615385,2.92307692307692,3.30769230769231,3.76923076923077,4.07692307692308,0.2,0.6,0.9,1,1.3,1.4,2,2.4,2.5,2.5,1,2,2,0.75,1.5,2.25,2.25,0.75,1.5,1.75,1.75,0.5,0.875,1.25,1.75,2,2.75,3,3.625,0.363636363636364,0.727272727272727,1,1.36363636363636,1.72727272727273,1.90909090909091,2.27272727272727,2.36363636363636,2.45454545454545,2.81818181818182,2.81818181818182,0.75,1.5,2,3.25,0.333333333333333,1,1.5,2,2.5,2.5,0.5,0.75,1.08333333333333,1.33333333333333,1.58333333333333,1.83333333333333,2.16666666666667,2.66666666666667,3.16666666666667,3.25,3.5,4.33333333333333,0.375,0.75,1.125,1.625,2.125,2.25,2.375,2.375,0.571428571428571,1,1.42857142857143,2,2.42857142857143,3,3,0.5,1.25,1.25,1.25,1.5,1.5,0.428571428571429,0.857142857142857,1.42857142857143,1.85714285714286,2.42857142857143,3.28571428571429,3.28571428571429,0.714285714285714,1.14285714285714,1.85714285714286,2.28571428571429,2.71428571428571,2.85714285714286,2.85714285714286,0.25,0.583333333333333,0.916666666666667,1.25,1.75,2.08333333333333,2.41666666666667,2.75,3.25,3.58333333333333,3.66666666666667,4.5,0.222222222222222,0.555555555555556,0.888888888888889,1.22222222222222,1.66666666666667,1.88888888888889,2.44444444444444,2.66666666666667,2.66666666666667,0.3,0.6,0.9,1.1,1.5,1.6,2.1,2.2,2.3,2.4,0.5,0.5,0.5,0.5,0.25,0.5,0.666666666666667,1,1.25,1.58333333333333,1.83333333333333,2.33333333333333,2.41666666666667,2.5,2.58333333333333,3,0.142857142857143,0.571428571428571,1,1.28571428571429,1.42857142857143,1.42857142857143,1.42857142857143,0.272727272727273,0.363636363636364,0.636363636363636,1,1.45454545454545,2,2.36363636363636,2.72727272727273,3.09090909090909,3.63636363636364,3.63636363636364,0,0.333333333333333,0.583333333333333,0.666666666666667,1.16666666666667,1.5,1.75,2.25,2.66666666666667,3,3.25,3.33333333333333,3.33333333333333,0.333333333333333,1,1.66666666666667,2,2,2.83333333333333,0.6,0.8,1.1,1.4,1.7,2.1,2.4,3,3.6,4,1,1.33333333333333,1.33333333333333,0.714285714285714,1.14285714285714,1.57142857142857,1.71428571428571,2.14285714285714,3,3,0.5,0.5,2,2,0.272727272727273,0.636363636363636,0.909090909090909,1.45454545454545,1.72727272727273,2.09090909090909,2.54545454545455,3.09090909090909,3.18181818181818,3.27272727272727,3.27272727272727,0.8,2,2.4,3,3,0.75,2.25,2.75,3,0,0.307692307692308,0.538461538461539,0.769230769230769,1.23076923076923,1.46153846153846,1.76923076923077,1.84615384615385,2.23076923076923,2.61538461538462,2.92307692307692,3.38461538461539,3.84615384615385,4.15384615384615,0.6,1.2,1.6,2,2,0.375,0.75,1.5,1.875,2.625,3,3,3,1.2,1.8,2,2.6,2.6,0.5,0.75,1.5,1.5,0.5,1,1.5,1.66666666666667,1.83333333333333,1.83333333333333,0.8,1.4,2,2.4,2.4,1.33333333333333,2.33333333333333,2.33333333333333,0.444444444444444,0.666666666666667,1,1.44444444444444,1.77777777777778,2,2.22222222222222,2.44444444444444,2.44444444444444,0.363636363636364,0.636363636363636,0.909090909090909,1.27272727272727,1.63636363636364,1.81818181818182,2.09090909090909,2.36363636363636,2.54545454545455,3.09090909090909,3.09090909090909,0.5,0.8,1.1,1.2,1.3,1.7,2.1,2.5,2.9,2.9,1,1.33333333333333,1.33333333333333,0.428571428571429,0.714285714285714,1.14285714285714,1.57142857142857,2.14285714285714,2.57142857142857,2.57142857142857,1.2,1.8,2.4,2.6,2.6,0.5,1.16666666666667,1.66666666666667,2.16666666666667,3,3,0.666666666666667,1.66666666666667,2,0.4,0.7,1,1.4,1.5,2.1,2.4,2.8,3.4,3.8,0,1,1,0.5,0.75,1.75,1.75,1.5,1.5,0.6,0.9,1.2,1.6,1.8,2.2,2.3,2.6,2.7,2.7,0.857142857142857,1.28571428571429,1.85714285714286,2.42857142857143,2.71428571428571,3.42857142857143,4.57142857142857,0.166666666666667,0.416666666666667,0.916666666666667,1.16666666666667,1.41666666666667,1.58333333333333,1.75,2.08333333333333,2.33333333333333,2.83333333333333,2.83333333333333,3.5,0.8,2,2.8,4,4,0.5,0.875,1.375,1.875,2.25,2.875,3.125,3.125,0.230769230769231,0.461538461538462,0.615384615384615,0.692307692307692,1.15384615384615,1.38461538461538,1.61538461538462,2.07692307692308,2.15384615384615,2.38461538461538,2.69230769230769,3.15384615384615,3.46153846153846,0,0.25,0.5,0.833333333333333,1,1.33333333333333,1.41666666666667,1.66666666666667,2.16666666666667,2.5,2.83333333333333,2.91666666666667,3.33333333333333,0.25,0.583333333333333,0.833333333333333,1.08333333333333,1.5,1.83333333333333,2.33333333333333,2.41666666666667,2.83333333333333,3.33333333333333,3.66666666666667,4,1.5,2,0.444444444444444,0.888888888888889,1,1.33333333333333,1.77777777777778,2.22222222222222,2.88888888888889,3.22222222222222,3.22222222222222,0.428571428571429,0.857142857142857,1,1.14285714285714,1.57142857142857,1.71428571428571,1.71428571428571,0.461538461538462,0.692307692307692,0.923076923076923,1.15384615384615,1.46153846153846,1.92307692307692,2.23076923076923,2.38461538461538,2.84615384615385,3.23076923076923,3.30769230769231,3.61538461538462,4.38461538461539,0.5,0.6,0.7,1.2,1.7,2.1,2.7,3.3,3.7,4.1,0.285714285714286,0.714285714285714,1.28571428571429,1.42857142857143,2.28571428571429,2.42857142857143,2.42857142857143,0.833333333333333,1.33333333333333,1.5,2.5,2.83333333333333,2.83333333333333,0.666666666666667,1.66666666666667,1.66666666666667,0.5,0.75,1.08333333333333,1.25,1.66666666666667,2,2.08333333333333,2.25,2.58333333333333,3.08333333333333,3.41666666666667,3.75,0.166666666666667,1.16666666666667,1.66666666666667,2.16666666666667,2.33333333333333,2.33333333333333,0.666666666666667,0.666666666666667,0.666666666666667,0.333333333333333,0.583333333333333,0.833333333333333,1.33333333333333,1.58333333333333,1.91666666666667,2.08333333333333,2.41666666666667,2.91666666666667,3.25,3.66666666666667,4.5,0.5,1.5,1.66666666666667,2.16666666666667,2.66666666666667,2.66666666666667,0.5,1,0,0.0909090909090909,0.636363636363636,0.909090909090909,1.27272727272727,1.54545454545455,1.72727272727273,2.27272727272727,2.54545454545455,2.63636363636364,2.63636363636364,0.5,1,1.16666666666667,1.5,2.16666666666667,2.16666666666667,0.5,0.75,1.25,1.5,1.83333333333333,2.08333333333333,2.41666666666667,2.5,3,3.16666666666667,3.66666666666667,4.5,0.25,0.583333333333333,0.833333333333333,1.33333333333333,1.83333333333333,2.08333333333333,2.41666666666667,2.75,3.25,3.33333333333333,3.83333333333333,3.83333333333333,0.3,0.7,0.8,1.1,1.2,1.5,1.8,2.2,2.4,2.4,0.230769230769231,0.461538461538462,0.692307692307692,1.15384615384615,1.30769230769231,1.53846153846154,1.76923076923077,2.23076923076923,2.53846153846154,2.69230769230769,2.76923076923077,3,3,0,0.75,1.25,2,2,0.5,1,1.66666666666667,1.83333333333333,2.83333333333333,2.83333333333333,1.5,1.5,0.333333333333333,0.666666666666667,1,1.66666666666667,1.88888888888889,2.22222222222222,2.33333333333333,2.88888888888889,2.88888888888889,0,0,1.33333333333333,1.66666666666667,1.66666666666667,0.375,1.125,1.5,1.875,2.5,2.625,2.75,2.75,0.428571428571429,1,1.57142857142857,2,2.57142857142857,3,3,0.625,1,1.125,1.5,1.625,2.125,2.625,2.625,0.25,0.583333333333333,1.08333333333333,1.16666666666667,1.41666666666667,1.66666666666667,2.16666666666667,2.33333333333333,2.66666666666667,3.16666666666667,3.33333333333333,3.33333333333333,0.428571428571429,0.857142857142857,1.28571428571429,1.28571428571429,2.14285714285714,2.85714285714286,2.85714285714286,0.4,0.7,1.3,1.6,2.2,2.4,2.7,3.1,3.7,4.7,0.333333333333333,1,1.44444444444444,1.77777777777778,2.11111111111111,2.44444444444444,2.88888888888889,3.44444444444444,3.44444444444444,0,0.307692307692308,0.538461538461539,0.846153846153846,1,1.23076923076923,1.46153846153846,1.69230769230769,1.84615384615385,1.92307692307692,2,2.30769230769231,2.30769230769231,0,0,0.461538461538462,0.769230769230769,1,1.23076923076923,1.38461538461538,1.61538461538462,1.84615384615385,2.15384615384615,2.30769230769231,2.76923076923077,3.23076923076923,3.69230769230769,4.46153846153846,0.5,0.5,0.75,1.5,2,2,0.75,1.125,1.5,2,2.375,2.75,2.875,2.875,0.375,0.75,1.125,1.5,2,2.75,2.875,3.625,0.25,0.5,0.833333333333333,1.08333333333333,1.25,1.58333333333333,1.91666666666667,2,2.33333333333333,2.83333333333333,3,3.41666666666667,1.33333333333333,2.33333333333333,2.33333333333333,0.230769230769231,0.692307692307692,0.923076923076923,1.23076923076923,1.53846153846154,1.76923076923077,2.07692307692308,2.15384615384615,2.38461538461538,2.61538461538462,2.76923076923077,3,3.76923076923077],\"text\":[\"episode: 1\nrunning_total: 0.30769231\ncontestant: A'Keria C. Davenport\",\"episode: 2\nrunning_total: 0.53846154\ncontestant: A'Keria C. Davenport\",\"episode: 3\nrunning_total: 0.61538462\ncontestant: A'Keria C. Davenport\",\"episode: 4\nrunning_total: 0.84615385\ncontestant: A'Keria C. Davenport\",\"episode: 5\nrunning_total: 1.07692308\ncontestant: A'Keria C. Davenport\",\"episode: 6\nrunning_total: 1.53846154\ncontestant: A'Keria C. Davenport\",\"episode: 7\nrunning_total: 1.61538462\ncontestant: A'Keria C. Davenport\",\"episode: 8\nrunning_total: 1.84615385\ncontestant: A'Keria C. Davenport\",\"episode: 9\nrunning_total: 2.30769231\ncontestant: A'Keria C. Davenport\",\"episode: 10\nrunning_total: 2.53846154\ncontestant: A'Keria C. Davenport\",\"episode: 11\nrunning_total: 2.84615385\ncontestant: A'Keria C. Davenport\",\"episode: 12\nrunning_total: 3.07692308\ncontestant: A'Keria C. Davenport\",\"episode: 14\nrunning_total: 3.07692308\ncontestant: A'Keria C. Davenport\",\"episode: 1\nrunning_total: 0.80000000\ncontestant: Acid Betty\",\"episode: 2\nrunning_total: 1.60000000\ncontestant: Acid Betty\",\"episode: 3\nrunning_total: 2.40000000\ncontestant: Acid Betty\",\"episode: 4\nrunning_total: 3.00000000\ncontestant: Acid Betty\",\"episode: 5\nrunning_total: 3.00000000\ncontestant: Acid Betty\",\"episode: 1\nrunning_total: 0.16666667\ncontestant: Adore Delano\",\"episode: 3\nrunning_total: 0.33333333\ncontestant: Adore Delano\",\"episode: 4\nrunning_total: 0.66666667\ncontestant: Adore Delano\",\"episode: 5\nrunning_total: 1.00000000\ncontestant: Adore Delano\",\"episode: 6\nrunning_total: 1.50000000\ncontestant: Adore Delano\",\"episode: 7\nrunning_total: 2.00000000\ncontestant: Adore Delano\",\"episode: 8\nrunning_total: 2.25000000\ncontestant: Adore Delano\",\"episode: 9\nrunning_total: 2.33333333\ncontestant: Adore Delano\",\"episode: 10\nrunning_total: 2.41666667\ncontestant: Adore Delano\",\"episode: 11\nrunning_total: 2.91666667\ncontestant: Adore Delano\",\"episode: 12\nrunning_total: 3.41666667\ncontestant: Adore Delano\",\"episode: 14\nrunning_total: 3.75000000\ncontestant: Adore Delano\",\"episode: 2\nrunning_total: 0.80000000\ncontestant: Aiden Zhane\",\"episode: 3\nrunning_total: 1.40000000\ncontestant: Aiden Zhane\",\"episode: 4\nrunning_total: 1.80000000\ncontestant: Aiden Zhane\",\"episode: 5\nrunning_total: 2.60000000\ncontestant: Aiden Zhane\",\"episode: 6\nrunning_total: 2.60000000\ncontestant: Aiden Zhane\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: Aja\",\"episode: 2\nrunning_total: 0.85714286\ncontestant: Aja\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Aja\",\"episode: 4\nrunning_total: 1.42857143\ncontestant: Aja\",\"episode: 5\nrunning_total: 1.85714286\ncontestant: Aja\",\"episode: 6\nrunning_total: 2.28571429\ncontestant: Aja\",\"episode: 7\nrunning_total: 2.28571429\ncontestant: Aja\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Akashia\",\"episode: 2\nrunning_total: 0.66666667\ncontestant: Akashia\",\"episode: 3\nrunning_total: 0.66666667\ncontestant: Akashia\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Alaska\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Alaska\",\"episode: 3\nrunning_total: 0.75000000\ncontestant: Alaska\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Alaska\",\"episode: 5\nrunning_total: 1.33333333\ncontestant: Alaska\",\"episode: 6\nrunning_total: 1.58333333\ncontestant: Alaska\",\"episode: 7\nrunning_total: 2.00000000\ncontestant: Alaska\",\"episode: 8\nrunning_total: 2.50000000\ncontestant: Alaska\",\"episode: 9\nrunning_total: 2.83333333\ncontestant: Alaska\",\"episode: 10\nrunning_total: 3.25000000\ncontestant: Alaska\",\"episode: 11\nrunning_total: 3.75000000\ncontestant: Alaska\",\"episode: 14\nrunning_total: 4.08333333\ncontestant: Alaska\",\"episode: 2\nrunning_total: 0.23076923\ncontestant: Alexis Mateo\",\"episode: 3\nrunning_total: 0.69230769\ncontestant: Alexis Mateo\",\"episode: 4\nrunning_total: 1.15384615\ncontestant: Alexis Mateo\",\"episode: 5\nrunning_total: 1.38461538\ncontestant: Alexis Mateo\",\"episode: 6\nrunning_total: 1.69230769\ncontestant: Alexis Mateo\",\"episode: 7\nrunning_total: 1.76923077\ncontestant: Alexis Mateo\",\"episode: 8\nrunning_total: 1.92307692\ncontestant: Alexis Mateo\",\"episode: 9\nrunning_total: 2.38461538\ncontestant: Alexis Mateo\",\"episode: 10\nrunning_total: 2.53846154\ncontestant: Alexis Mateo\",\"episode: 11\nrunning_total: 2.61538462\ncontestant: Alexis Mateo\",\"episode: 12\nrunning_total: 3.00000000\ncontestant: Alexis Mateo\",\"episode: 13\nrunning_total: 3.07692308\ncontestant: Alexis Mateo\",\"episode: 15\nrunning_total: 3.07692308\ncontestant: Alexis Mateo\",\"episode: 1\nrunning_total: 0.27272727\ncontestant: Alexis Michelle\",\"episode: 2\nrunning_total: 0.54545455\ncontestant: Alexis Michelle\",\"episode: 3\nrunning_total: 0.81818182\ncontestant: Alexis Michelle\",\"episode: 4\nrunning_total: 1.09090909\ncontestant: Alexis Michelle\",\"episode: 5\nrunning_total: 1.54545455\ncontestant: Alexis Michelle\",\"episode: 6\nrunning_total: 2.09090909\ncontestant: Alexis Michelle\",\"episode: 7\nrunning_total: 2.36363636\ncontestant: Alexis Michelle\",\"episode: 8\nrunning_total: 2.45454545\ncontestant: Alexis Michelle\",\"episode: 9\nrunning_total: 2.63636364\ncontestant: Alexis Michelle\",\"episode: 10\nrunning_total: 2.81818182\ncontestant: Alexis Michelle\",\"episode: 11\nrunning_total: 2.81818182\ncontestant: Alexis Michelle\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Alisa Summers\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Alyssa Edwards\",\"episode: 2\nrunning_total: 0.77777778\ncontestant: Alyssa Edwards\",\"episode: 3\nrunning_total: 1.11111111\ncontestant: Alyssa Edwards\",\"episode: 4\nrunning_total: 1.77777778\ncontestant: Alyssa Edwards\",\"episode: 5\nrunning_total: 2.33333333\ncontestant: Alyssa Edwards\",\"episode: 6\nrunning_total: 2.55555556\ncontestant: Alyssa Edwards\",\"episode: 7\nrunning_total: 2.66666667\ncontestant: Alyssa Edwards\",\"episode: 8\nrunning_total: 2.77777778\ncontestant: Alyssa Edwards\",\"episode: 9\nrunning_total: 2.88888889\ncontestant: Alyssa Edwards\",\"episode: 1\nrunning_total: 1.33333333\ncontestant: April Carrión\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: April Carrión\",\"episode: 4\nrunning_total: 1.66666667\ncontestant: April Carrión\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Aquaria\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Aquaria\",\"episode: 3\nrunning_total: 0.75000000\ncontestant: Aquaria\",\"episode: 4\nrunning_total: 1.25000000\ncontestant: Aquaria\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Aquaria\",\"episode: 6\nrunning_total: 1.75000000\ncontestant: Aquaria\",\"episode: 7\nrunning_total: 2.25000000\ncontestant: Aquaria\",\"episode: 8\nrunning_total: 2.41666667\ncontestant: Aquaria\",\"episode: 9\nrunning_total: 2.83333333\ncontestant: Aquaria\",\"episode: 10\nrunning_total: 3.00000000\ncontestant: Aquaria\",\"episode: 11\nrunning_total: 3.50000000\ncontestant: Aquaria\",\"episode: 14\nrunning_total: 4.33333333\ncontestant: Aquaria\",\"episode: 1\nrunning_total: 0.60000000\ncontestant: Ariel Versace\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Ariel Versace\",\"episode: 3\nrunning_total: 1.60000000\ncontestant: Ariel Versace\",\"episode: 4\nrunning_total: 2.20000000\ncontestant: Ariel Versace\",\"episode: 5\nrunning_total: 2.20000000\ncontestant: Ariel Versace\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Asia O'Hara\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Asia O'Hara\",\"episode: 3\nrunning_total: 1.08333333\ncontestant: Asia O'Hara\",\"episode: 4\nrunning_total: 1.25000000\ncontestant: Asia O'Hara\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Asia O'Hara\",\"episode: 6\nrunning_total: 1.91666667\ncontestant: Asia O'Hara\",\"episode: 7\nrunning_total: 2.08333333\ncontestant: Asia O'Hara\",\"episode: 8\nrunning_total: 2.50000000\ncontestant: Asia O'Hara\",\"episode: 9\nrunning_total: 3.00000000\ncontestant: Asia O'Hara\",\"episode: 10\nrunning_total: 3.33333333\ncontestant: Asia O'Hara\",\"episode: 11\nrunning_total: 3.75000000\ncontestant: Asia O'Hara\",\"episode: 14\nrunning_total: 3.75000000\ncontestant: Asia O'Hara\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: BeBe Zahara Benet\",\"episode: 2\nrunning_total: 0.85714286\ncontestant: BeBe Zahara Benet\",\"episode: 3\nrunning_total: 1.71428571\ncontestant: BeBe Zahara Benet\",\"episode: 4\nrunning_total: 2.28571429\ncontestant: BeBe Zahara Benet\",\"episode: 5\nrunning_total: 2.42857143\ncontestant: BeBe Zahara Benet\",\"episode: 6\nrunning_total: 3.28571429\ncontestant: BeBe Zahara Benet\",\"episode: 8\nrunning_total: 4.71428571\ncontestant: BeBe Zahara Benet\",\"episode: 1\nrunning_total: 0.60000000\ncontestant: BenDeLaCreme\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: BenDeLaCreme\",\"episode: 4\nrunning_total: 1.40000000\ncontestant: BenDeLaCreme\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: BenDeLaCreme\",\"episode: 6\nrunning_total: 2.30000000\ncontestant: BenDeLaCreme\",\"episode: 7\nrunning_total: 2.40000000\ncontestant: BenDeLaCreme\",\"episode: 8\nrunning_total: 2.60000000\ncontestant: BenDeLaCreme\",\"episode: 9\nrunning_total: 3.10000000\ncontestant: BenDeLaCreme\",\"episode: 10\nrunning_total: 3.50000000\ncontestant: BenDeLaCreme\",\"episode: 11\nrunning_total: 4.00000000\ncontestant: BenDeLaCreme\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Bianca Del Rio\",\"episode: 3\nrunning_total: 0.75000000\ncontestant: Bianca Del Rio\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Bianca Del Rio\",\"episode: 5\nrunning_total: 1.33333333\ncontestant: Bianca Del Rio\",\"episode: 6\nrunning_total: 1.66666667\ncontestant: Bianca Del Rio\",\"episode: 7\nrunning_total: 2.00000000\ncontestant: Bianca Del Rio\",\"episode: 8\nrunning_total: 2.50000000\ncontestant: Bianca Del Rio\",\"episode: 9\nrunning_total: 2.75000000\ncontestant: Bianca Del Rio\",\"episode: 10\nrunning_total: 3.25000000\ncontestant: Bianca Del Rio\",\"episode: 11\nrunning_total: 3.58333333\ncontestant: Bianca Del Rio\",\"episode: 12\nrunning_total: 4.08333333\ncontestant: Bianca Del Rio\",\"episode: 14\nrunning_total: 4.91666667\ncontestant: Bianca Del Rio\",\"episode: 1\nrunning_total: 0.66666667\ncontestant: Blair St. Clair\",\"episode: 2\nrunning_total: 1.16666667\ncontestant: Blair St. Clair\",\"episode: 3\nrunning_total: 1.83333333\ncontestant: Blair St. Clair\",\"episode: 4\nrunning_total: 2.33333333\ncontestant: Blair St. Clair\",\"episode: 5\nrunning_total: 2.83333333\ncontestant: Blair St. Clair\",\"episode: 6\nrunning_total: 2.83333333\ncontestant: Blair St. Clair\",\"episode: 1\nrunning_total: 0.30000000\ncontestant: Bob the Drag Queen\",\"episode: 2\nrunning_total: 0.60000000\ncontestant: Bob the Drag Queen\",\"episode: 3\nrunning_total: 1.20000000\ncontestant: Bob the Drag Queen\",\"episode: 4\nrunning_total: 1.50000000\ncontestant: Bob the Drag Queen\",\"episode: 5\nrunning_total: 2.10000000\ncontestant: Bob the Drag Queen\",\"episode: 6\nrunning_total: 2.60000000\ncontestant: Bob the Drag Queen\",\"episode: 7\nrunning_total: 3.20000000\ncontestant: Bob the Drag Queen\",\"episode: 8\nrunning_total: 3.30000000\ncontestant: Bob the Drag Queen\",\"episode: 9\nrunning_total: 3.90000000\ncontestant: Bob the Drag Queen\",\"episode: 10\nrunning_total: 4.90000000\ncontestant: Bob the Drag Queen\",\"episode: 1\nrunning_total: 0.66666667\ncontestant: Brita\",\"episode: 3\nrunning_total: 1.16666667\ncontestant: Brita\",\"episode: 4\nrunning_total: 1.33333333\ncontestant: Brita\",\"episode: 5\nrunning_total: 1.66666667\ncontestant: Brita\",\"episode: 6\nrunning_total: 1.83333333\ncontestant: Brita\",\"episode: 7\nrunning_total: 1.83333333\ncontestant: Brita\",\"episode: 1\nrunning_total: 0.46153846\ncontestant: Brooke Lynn Hytes\",\"episode: 2\nrunning_total: 0.61538462\ncontestant: Brooke Lynn Hytes\",\"episode: 3\nrunning_total: 0.84615385\ncontestant: Brooke Lynn Hytes\",\"episode: 4\nrunning_total: 1.15384615\ncontestant: Brooke Lynn Hytes\",\"episode: 5\nrunning_total: 1.61538462\ncontestant: Brooke Lynn Hytes\",\"episode: 6\nrunning_total: 1.84615385\ncontestant: Brooke Lynn Hytes\",\"episode: 7\nrunning_total: 2.15384615\ncontestant: Brooke Lynn Hytes\",\"episode: 8\nrunning_total: 2.23076923\ncontestant: Brooke Lynn Hytes\",\"episode: 9\nrunning_total: 2.53846154\ncontestant: Brooke Lynn Hytes\",\"episode: 10\nrunning_total: 2.84615385\ncontestant: Brooke Lynn Hytes\",\"episode: 11\nrunning_total: 3.30769231\ncontestant: Brooke Lynn Hytes\",\"episode: 12\nrunning_total: 3.38461538\ncontestant: Brooke Lynn Hytes\",\"episode: 14\nrunning_total: 3.76923077\ncontestant: Brooke Lynn Hytes\",\"episode: 2\nrunning_total: 0.20000000\ncontestant: Carmen Carrera\",\"episode: 3\nrunning_total: 0.40000000\ncontestant: Carmen Carrera\",\"episode: 4\nrunning_total: 0.70000000\ncontestant: Carmen Carrera\",\"episode: 5\nrunning_total: 1.00000000\ncontestant: Carmen Carrera\",\"episode: 6\nrunning_total: 1.30000000\ncontestant: Carmen Carrera\",\"episode: 7\nrunning_total: 1.80000000\ncontestant: Carmen Carrera\",\"episode: 8\nrunning_total: 2.20000000\ncontestant: Carmen Carrera\",\"episode: 9\nrunning_total: 2.30000000\ncontestant: Carmen Carrera\",\"episode: 10\nrunning_total: 2.30000000\ncontestant: Carmen Carrera\",\"episode: 12\nrunning_total: 2.30000000\ncontestant: Carmen Carrera\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Chad Michaels\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Chad Michaels\",\"episode: 3\nrunning_total: 1.08333333\ncontestant: Chad Michaels\",\"episode: 4\nrunning_total: 1.33333333\ncontestant: Chad Michaels\",\"episode: 5\nrunning_total: 1.83333333\ncontestant: Chad Michaels\",\"episode: 6\nrunning_total: 2.08333333\ncontestant: Chad Michaels\",\"episode: 7\nrunning_total: 2.25000000\ncontestant: Chad Michaels\",\"episode: 8\nrunning_total: 2.50000000\ncontestant: Chad Michaels\",\"episode: 9\nrunning_total: 2.83333333\ncontestant: Chad Michaels\",\"episode: 10\nrunning_total: 3.16666667\ncontestant: Chad Michaels\",\"episode: 11\nrunning_total: 3.58333333\ncontestant: Chad Michaels\",\"episode: 14\nrunning_total: 3.91666667\ncontestant: Chad Michaels\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Charlie Hides\",\"episode: 2\nrunning_total: 1.25000000\ncontestant: Charlie Hides\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Charlie Hides\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Charlie Hides\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Chi Chi DeVayne\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Chi Chi DeVayne\",\"episode: 3\nrunning_total: 1.33333333\ncontestant: Chi Chi DeVayne\",\"episode: 4\nrunning_total: 1.44444444\ncontestant: Chi Chi DeVayne\",\"episode: 5\nrunning_total: 1.77777778\ncontestant: Chi Chi DeVayne\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Chi Chi DeVayne\",\"episode: 7\nrunning_total: 2.11111111\ncontestant: Chi Chi DeVayne\",\"episode: 8\nrunning_total: 2.66666667\ncontestant: Chi Chi DeVayne\",\"episode: 9\nrunning_total: 2.66666667\ncontestant: Chi Chi DeVayne\",\"episode: 1\nrunning_total: 0.30000000\ncontestant: Coco Montrese\",\"episode: 2\nrunning_total: 0.70000000\ncontestant: Coco Montrese\",\"episode: 3\nrunning_total: 0.80000000\ncontestant: Coco Montrese\",\"episode: 4\nrunning_total: 1.10000000\ncontestant: Coco Montrese\",\"episode: 5\nrunning_total: 1.40000000\ncontestant: Coco Montrese\",\"episode: 6\nrunning_total: 1.50000000\ncontestant: Coco Montrese\",\"episode: 7\nrunning_total: 2.10000000\ncontestant: Coco Montrese\",\"episode: 8\nrunning_total: 2.40000000\ncontestant: Coco Montrese\",\"episode: 9\nrunning_total: 2.50000000\ncontestant: Coco Montrese\",\"episode: 10\nrunning_total: 2.50000000\ncontestant: Coco Montrese\",\"episode: 2\nrunning_total: 0.25000000\ncontestant: Courtney Act\",\"episode: 3\nrunning_total: 0.50000000\ncontestant: Courtney Act\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Courtney Act\",\"episode: 5\nrunning_total: 1.25000000\ncontestant: Courtney Act\",\"episode: 6\nrunning_total: 1.50000000\ncontestant: Courtney Act\",\"episode: 7\nrunning_total: 1.66666667\ncontestant: Courtney Act\",\"episode: 8\nrunning_total: 1.91666667\ncontestant: Courtney Act\",\"episode: 9\nrunning_total: 2.41666667\ncontestant: Courtney Act\",\"episode: 10\nrunning_total: 2.58333333\ncontestant: Courtney Act\",\"episode: 11\nrunning_total: 2.75000000\ncontestant: Courtney Act\",\"episode: 12\nrunning_total: 3.25000000\ncontestant: Courtney Act\",\"episode: 14\nrunning_total: 3.58333333\ncontestant: Courtney Act\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Crystal Methyd\",\"episode: 3\nrunning_total: 0.41666667\ncontestant: Crystal Methyd\",\"episode: 4\nrunning_total: 0.66666667\ncontestant: Crystal Methyd\",\"episode: 5\nrunning_total: 0.91666667\ncontestant: Crystal Methyd\",\"episode: 6\nrunning_total: 1.08333333\ncontestant: Crystal Methyd\",\"episode: 7\nrunning_total: 1.41666667\ncontestant: Crystal Methyd\",\"episode: 8\nrunning_total: 1.75000000\ncontestant: Crystal Methyd\",\"episode: 9\nrunning_total: 2.08333333\ncontestant: Crystal Methyd\",\"episode: 10\nrunning_total: 2.41666667\ncontestant: Crystal Methyd\",\"episode: 11\nrunning_total: 2.91666667\ncontestant: Crystal Methyd\",\"episode: 12\nrunning_total: 3.00000000\ncontestant: Crystal Methyd\",\"episode: 14\nrunning_total: 3.00000000\ncontestant: Crystal Methyd\",\"episode: 1\nrunning_total: 0.37500000\ncontestant: Cynthia Lee Fontaine\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Cynthia Lee Fontaine\",\"episode: 2\nrunning_total: 1.12500000\ncontestant: Cynthia Lee Fontaine\",\"episode: 3\nrunning_total: 0.75000000\ncontestant: Cynthia Lee Fontaine\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: Cynthia Lee Fontaine\",\"episode: 4\nrunning_total: 1.87500000\ncontestant: Cynthia Lee Fontaine\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Cynthia Lee Fontaine\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Cynthia Lee Fontaine\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Dahlia Sin\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Dahlia Sin\",\"episode: 2\nrunning_total: 0.09090909\ncontestant: Darienne Lake\",\"episode: 3\nrunning_total: 0.63636364\ncontestant: Darienne Lake\",\"episode: 4\nrunning_total: 0.81818182\ncontestant: Darienne Lake\",\"episode: 5\nrunning_total: 1.09090909\ncontestant: Darienne Lake\",\"episode: 6\nrunning_total: 1.54545455\ncontestant: Darienne Lake\",\"episode: 7\nrunning_total: 1.63636364\ncontestant: Darienne Lake\",\"episode: 8\nrunning_total: 2.00000000\ncontestant: Darienne Lake\",\"episode: 9\nrunning_total: 2.27272727\ncontestant: Darienne Lake\",\"episode: 10\nrunning_total: 2.45454545\ncontestant: Darienne Lake\",\"episode: 11\nrunning_total: 2.54545455\ncontestant: Darienne Lake\",\"episode: 12\nrunning_total: 2.54545455\ncontestant: Darienne Lake\",\"episode: 1\nrunning_total: 1.50000000\ncontestant: Dax ExclamationPoint\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Dax ExclamationPoint\",\"episode: 2\nrunning_total: 0.42857143\ncontestant: Delta Work\",\"episode: 3\nrunning_total: 0.57142857\ncontestant: Delta Work\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Delta Work\",\"episode: 5\nrunning_total: 1.42857143\ncontestant: Delta Work\",\"episode: 6\nrunning_total: 1.57142857\ncontestant: Delta Work\",\"episode: 7\nrunning_total: 2.14285714\ncontestant: Delta Work\",\"episode: 8\nrunning_total: 2.14285714\ncontestant: Delta Work\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Derrick Barry\",\"episode: 2\nrunning_total: 0.87500000\ncontestant: Derrick Barry\",\"episode: 3\nrunning_total: 1.12500000\ncontestant: Derrick Barry\",\"episode: 4\nrunning_total: 1.37500000\ncontestant: Derrick Barry\",\"episode: 5\nrunning_total: 1.87500000\ncontestant: Derrick Barry\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Derrick Barry\",\"episode: 7\nrunning_total: 2.75000000\ncontestant: Derrick Barry\",\"episode: 8\nrunning_total: 2.75000000\ncontestant: Derrick Barry\",\"episode: 1\nrunning_total: 0.45454545\ncontestant: Detox\",\"episode: 2\nrunning_total: 0.72727273\ncontestant: Detox\",\"episode: 3\nrunning_total: 1.27272727\ncontestant: Detox\",\"episode: 4\nrunning_total: 1.54545455\ncontestant: Detox\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Detox\",\"episode: 6\nrunning_total: 2.45454545\ncontestant: Detox\",\"episode: 7\nrunning_total: 2.72727273\ncontestant: Detox\",\"episode: 8\nrunning_total: 3.09090909\ncontestant: Detox\",\"episode: 9\nrunning_total: 3.27272727\ncontestant: Detox\",\"episode: 10\nrunning_total: 3.36363636\ncontestant: Detox\",\"episode: 11\nrunning_total: 3.36363636\ncontestant: Detox\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Dida Ritz\",\"episode: 2\nrunning_total: 0.55555556\ncontestant: Dida Ritz\",\"episode: 3\nrunning_total: 0.66666667\ncontestant: Dida Ritz\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Dida Ritz\",\"episode: 5\nrunning_total: 1.33333333\ncontestant: Dida Ritz\",\"episode: 6\nrunning_total: 1.55555556\ncontestant: Dida Ritz\",\"episode: 7\nrunning_total: 1.88888889\ncontestant: Dida Ritz\",\"episode: 8\nrunning_total: 2.22222222\ncontestant: Dida Ritz\",\"episode: 9\nrunning_total: 2.22222222\ncontestant: Dida Ritz\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Dusty Ray Bottoms\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Dusty Ray Bottoms\",\"episode: 3\nrunning_total: 2.25000000\ncontestant: Dusty Ray Bottoms\",\"episode: 4\nrunning_total: 2.25000000\ncontestant: Dusty Ray Bottoms\",\"episode: 1\nrunning_total: 0.23529412\ncontestant: Eureka O'Hara\",\"episode: 1\nrunning_total: 1.17647059\ncontestant: Eureka O'Hara\",\"episode: 2\nrunning_total: 0.41176471\ncontestant: Eureka O'Hara\",\"episode: 2\nrunning_total: 1.23529412\ncontestant: Eureka O'Hara\",\"episode: 3\nrunning_total: 0.58823529\ncontestant: Eureka O'Hara\",\"episode: 3\nrunning_total: 1.47058824\ncontestant: Eureka O'Hara\",\"episode: 4\nrunning_total: 0.82352941\ncontestant: Eureka O'Hara\",\"episode: 4\nrunning_total: 1.64705882\ncontestant: Eureka O'Hara\",\"episode: 5\nrunning_total: 1.00000000\ncontestant: Eureka O'Hara\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Eureka O'Hara\",\"episode: 6\nrunning_total: 2.35294118\ncontestant: Eureka O'Hara\",\"episode: 7\nrunning_total: 2.64705882\ncontestant: Eureka O'Hara\",\"episode: 8\nrunning_total: 2.82352941\ncontestant: Eureka O'Hara\",\"episode: 9\nrunning_total: 2.88235294\ncontestant: Eureka O'Hara\",\"episode: 10\nrunning_total: 3.17647059\ncontestant: Eureka O'Hara\",\"episode: 11\nrunning_total: 3.41176471\ncontestant: Eureka O'Hara\",\"episode: 14\nrunning_total: 3.70588235\ncontestant: Eureka O'Hara\",\"episode: 1\nrunning_total: 0.37500000\ncontestant: Farrah Moan\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Farrah Moan\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Farrah Moan\",\"episode: 4\nrunning_total: 1.37500000\ncontestant: Farrah Moan\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Farrah Moan\",\"episode: 6\nrunning_total: 1.75000000\ncontestant: Farrah Moan\",\"episode: 7\nrunning_total: 2.12500000\ncontestant: Farrah Moan\",\"episode: 8\nrunning_total: 2.12500000\ncontestant: Farrah Moan\",\"episode: 1\nrunning_total: 1.00000000\ncontestant: Gia Gunn\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Gia Gunn\",\"episode: 4\nrunning_total: 2.75000000\ncontestant: Gia Gunn\",\"episode: 5\nrunning_total: 2.75000000\ncontestant: Gia Gunn\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Gigi Goode\",\"episode: 3\nrunning_total: 0.58333333\ncontestant: Gigi Goode\",\"episode: 4\nrunning_total: 1.08333333\ncontestant: Gigi Goode\",\"episode: 5\nrunning_total: 1.41666667\ncontestant: Gigi Goode\",\"episode: 6\nrunning_total: 1.91666667\ncontestant: Gigi Goode\",\"episode: 7\nrunning_total: 2.41666667\ncontestant: Gigi Goode\",\"episode: 8\nrunning_total: 2.58333333\ncontestant: Gigi Goode\",\"episode: 9\nrunning_total: 2.75000000\ncontestant: Gigi Goode\",\"episode: 10\nrunning_total: 3.00000000\ncontestant: Gigi Goode\",\"episode: 11\nrunning_total: 3.25000000\ncontestant: Gigi Goode\",\"episode: 12\nrunning_total: 3.75000000\ncontestant: Gigi Goode\",\"episode: 14\nrunning_total: 3.75000000\ncontestant: Gigi Goode\",\"episode: 1\nrunning_total: 0.23076923\ncontestant: Ginger Minj\",\"episode: 2\nrunning_total: 0.69230769\ncontestant: Ginger Minj\",\"episode: 3\nrunning_total: 0.92307692\ncontestant: Ginger Minj\",\"episode: 4\nrunning_total: 1.23076923\ncontestant: Ginger Minj\",\"episode: 5\nrunning_total: 1.46153846\ncontestant: Ginger Minj\",\"episode: 6\nrunning_total: 1.84615385\ncontestant: Ginger Minj\",\"episode: 7\nrunning_total: 2.30769231\ncontestant: Ginger Minj\",\"episode: 8\nrunning_total: 2.38461538\ncontestant: Ginger Minj\",\"episode: 9\nrunning_total: 2.84615385\ncontestant: Ginger Minj\",\"episode: 10\nrunning_total: 2.92307692\ncontestant: Ginger Minj\",\"episode: 11\nrunning_total: 3.30769231\ncontestant: Ginger Minj\",\"episode: 12\nrunning_total: 3.76923077\ncontestant: Ginger Minj\",\"episode: 14\nrunning_total: 4.07692308\ncontestant: Ginger Minj\",\"episode: 1\nrunning_total: 0.20000000\ncontestant: Heidi N Closet\",\"episode: 3\nrunning_total: 0.60000000\ncontestant: Heidi N Closet\",\"episode: 4\nrunning_total: 0.90000000\ncontestant: Heidi N Closet\",\"episode: 5\nrunning_total: 1.00000000\ncontestant: Heidi N Closet\",\"episode: 6\nrunning_total: 1.30000000\ncontestant: Heidi N Closet\",\"episode: 7\nrunning_total: 1.40000000\ncontestant: Heidi N Closet\",\"episode: 8\nrunning_total: 2.00000000\ncontestant: Heidi N Closet\",\"episode: 9\nrunning_total: 2.40000000\ncontestant: Heidi N Closet\",\"episode: 10\nrunning_total: 2.50000000\ncontestant: Heidi N Closet\",\"episode: 11\nrunning_total: 2.50000000\ncontestant: Heidi N Closet\",\"episode: 1\nrunning_total: 1.00000000\ncontestant: Honey Davenport\",\"episode: 2\nrunning_total: 2.00000000\ncontestant: Honey Davenport\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Honey Davenport\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Honey Mahogany\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Honey Mahogany\",\"episode: 3\nrunning_total: 2.25000000\ncontestant: Honey Mahogany\",\"episode: 4\nrunning_total: 2.25000000\ncontestant: Honey Mahogany\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: India Ferrah\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: India Ferrah\",\"episode: 4\nrunning_total: 1.75000000\ncontestant: India Ferrah\",\"episode: 5\nrunning_total: 1.75000000\ncontestant: India Ferrah\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Ivy Winters\",\"episode: 2\nrunning_total: 0.87500000\ncontestant: Ivy Winters\",\"episode: 3\nrunning_total: 1.25000000\ncontestant: Ivy Winters\",\"episode: 4\nrunning_total: 1.75000000\ncontestant: Ivy Winters\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Ivy Winters\",\"episode: 6\nrunning_total: 2.75000000\ncontestant: Ivy Winters\",\"episode: 7\nrunning_total: 3.00000000\ncontestant: Ivy Winters\",\"episode: 8\nrunning_total: 3.62500000\ncontestant: Ivy Winters\",\"episode: 1\nrunning_total: 0.36363636\ncontestant: Jackie Cox\",\"episode: 3\nrunning_total: 0.72727273\ncontestant: Jackie Cox\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Jackie Cox\",\"episode: 5\nrunning_total: 1.36363636\ncontestant: Jackie Cox\",\"episode: 6\nrunning_total: 1.72727273\ncontestant: Jackie Cox\",\"episode: 7\nrunning_total: 1.90909091\ncontestant: Jackie Cox\",\"episode: 8\nrunning_total: 2.27272727\ncontestant: Jackie Cox\",\"episode: 9\nrunning_total: 2.36363636\ncontestant: Jackie Cox\",\"episode: 10\nrunning_total: 2.45454545\ncontestant: Jackie Cox\",\"episode: 11\nrunning_total: 2.81818182\ncontestant: Jackie Cox\",\"episode: 12\nrunning_total: 2.81818182\ncontestant: Jackie Cox\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Jade\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Jade\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Jade\",\"episode: 4\nrunning_total: 3.25000000\ncontestant: Jade\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Jade Jolie\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Jade Jolie\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: Jade Jolie\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Jade Jolie\",\"episode: 5\nrunning_total: 2.50000000\ncontestant: Jade Jolie\",\"episode: 6\nrunning_total: 2.50000000\ncontestant: Jade Jolie\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Jaida Essence Hall\",\"episode: 3\nrunning_total: 0.75000000\ncontestant: Jaida Essence Hall\",\"episode: 4\nrunning_total: 1.08333333\ncontestant: Jaida Essence Hall\",\"episode: 5\nrunning_total: 1.33333333\ncontestant: Jaida Essence Hall\",\"episode: 6\nrunning_total: 1.58333333\ncontestant: Jaida Essence Hall\",\"episode: 7\nrunning_total: 1.83333333\ncontestant: Jaida Essence Hall\",\"episode: 8\nrunning_total: 2.16666667\ncontestant: Jaida Essence Hall\",\"episode: 9\nrunning_total: 2.66666667\ncontestant: Jaida Essence Hall\",\"episode: 10\nrunning_total: 3.16666667\ncontestant: Jaida Essence Hall\",\"episode: 11\nrunning_total: 3.25000000\ncontestant: Jaida Essence Hall\",\"episode: 12\nrunning_total: 3.50000000\ncontestant: Jaida Essence Hall\",\"episode: 14\nrunning_total: 4.33333333\ncontestant: Jaida Essence Hall\",\"episode: 1\nrunning_total: 0.37500000\ncontestant: Jaidynn Diore Fierce\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Jaidynn Diore Fierce\",\"episode: 3\nrunning_total: 1.12500000\ncontestant: Jaidynn Diore Fierce\",\"episode: 4\nrunning_total: 1.62500000\ncontestant: Jaidynn Diore Fierce\",\"episode: 5\nrunning_total: 2.12500000\ncontestant: Jaidynn Diore Fierce\",\"episode: 6\nrunning_total: 2.25000000\ncontestant: Jaidynn Diore Fierce\",\"episode: 7\nrunning_total: 2.37500000\ncontestant: Jaidynn Diore Fierce\",\"episode: 8\nrunning_total: 2.37500000\ncontestant: Jaidynn Diore Fierce\",\"episode: 2\nrunning_total: 0.57142857\ncontestant: Jan\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Jan\",\"episode: 4\nrunning_total: 1.42857143\ncontestant: Jan\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Jan\",\"episode: 6\nrunning_total: 2.42857143\ncontestant: Jan\",\"episode: 7\nrunning_total: 3.00000000\ncontestant: Jan\",\"episode: 8\nrunning_total: 3.00000000\ncontestant: Jan\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Jasmine Masters\",\"episode: 2\nrunning_total: 1.25000000\ncontestant: Jasmine Masters\",\"episode: 3\nrunning_total: 1.25000000\ncontestant: Jasmine Masters\",\"episode: 8\nrunning_total: 1.25000000\ncontestant: Jasmine Masters\",\"episode: 1\nrunning_total: 1.50000000\ncontestant: Jaymes Mansfield\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Jaymes Mansfield\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: Jessica Wild\",\"episode: 2\nrunning_total: 0.85714286\ncontestant: Jessica Wild\",\"episode: 3\nrunning_total: 1.42857143\ncontestant: Jessica Wild\",\"episode: 4\nrunning_total: 1.85714286\ncontestant: Jessica Wild\",\"episode: 5\nrunning_total: 2.42857143\ncontestant: Jessica Wild\",\"episode: 6\nrunning_total: 3.28571429\ncontestant: Jessica Wild\",\"episode: 7\nrunning_total: 3.28571429\ncontestant: Jessica Wild\",\"episode: 1\nrunning_total: 0.71428571\ncontestant: Jiggly Caliente\",\"episode: 2\nrunning_total: 1.14285714\ncontestant: Jiggly Caliente\",\"episode: 3\nrunning_total: 1.85714286\ncontestant: Jiggly Caliente\",\"episode: 4\nrunning_total: 2.28571429\ncontestant: Jiggly Caliente\",\"episode: 5\nrunning_total: 2.71428571\ncontestant: Jiggly Caliente\",\"episode: 6\nrunning_total: 2.85714286\ncontestant: Jiggly Caliente\",\"episode: 7\nrunning_total: 2.85714286\ncontestant: Jiggly Caliente\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Jinkx Monsoon\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Jinkx Monsoon\",\"episode: 3\nrunning_total: 0.91666667\ncontestant: Jinkx Monsoon\",\"episode: 4\nrunning_total: 1.25000000\ncontestant: Jinkx Monsoon\",\"episode: 5\nrunning_total: 1.75000000\ncontestant: Jinkx Monsoon\",\"episode: 6\nrunning_total: 2.08333333\ncontestant: Jinkx Monsoon\",\"episode: 7\nrunning_total: 2.41666667\ncontestant: Jinkx Monsoon\",\"episode: 8\nrunning_total: 2.75000000\ncontestant: Jinkx Monsoon\",\"episode: 9\nrunning_total: 3.25000000\ncontestant: Jinkx Monsoon\",\"episode: 10\nrunning_total: 3.58333333\ncontestant: Jinkx Monsoon\",\"episode: 11\nrunning_total: 3.66666667\ncontestant: Jinkx Monsoon\",\"episode: 14\nrunning_total: 4.50000000\ncontestant: Jinkx Monsoon\",\"episode: 2\nrunning_total: 0.22222222\ncontestant: Joslyn Fox\",\"episode: 3\nrunning_total: 0.55555556\ncontestant: Joslyn Fox\",\"episode: 4\nrunning_total: 0.88888889\ncontestant: Joslyn Fox\",\"episode: 5\nrunning_total: 1.22222222\ncontestant: Joslyn Fox\",\"episode: 6\nrunning_total: 1.66666667\ncontestant: Joslyn Fox\",\"episode: 7\nrunning_total: 1.88888889\ncontestant: Joslyn Fox\",\"episode: 8\nrunning_total: 2.44444444\ncontestant: Joslyn Fox\",\"episode: 9\nrunning_total: 2.66666667\ncontestant: Joslyn Fox\",\"episode: 10\nrunning_total: 2.66666667\ncontestant: Joslyn Fox\",\"episode: 1\nrunning_total: 0.30000000\ncontestant: Jujubee\",\"episode: 2\nrunning_total: 0.60000000\ncontestant: Jujubee\",\"episode: 3\nrunning_total: 0.90000000\ncontestant: Jujubee\",\"episode: 4\nrunning_total: 1.10000000\ncontestant: Jujubee\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Jujubee\",\"episode: 6\nrunning_total: 1.60000000\ncontestant: Jujubee\",\"episode: 7\nrunning_total: 2.10000000\ncontestant: Jujubee\",\"episode: 8\nrunning_total: 2.20000000\ncontestant: Jujubee\",\"episode: 9\nrunning_total: 2.30000000\ncontestant: Jujubee\",\"episode: 11\nrunning_total: 2.40000000\ncontestant: Jujubee\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Kahanna Montrese\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Kahanna Montrese\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Kalorie Karbdashian Williams\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Kalorie Karbdashian Williams\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Kameron Michaels\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Kameron Michaels\",\"episode: 3\nrunning_total: 0.66666667\ncontestant: Kameron Michaels\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Kameron Michaels\",\"episode: 5\nrunning_total: 1.25000000\ncontestant: Kameron Michaels\",\"episode: 6\nrunning_total: 1.58333333\ncontestant: Kameron Michaels\",\"episode: 7\nrunning_total: 1.83333333\ncontestant: Kameron Michaels\",\"episode: 8\nrunning_total: 2.33333333\ncontestant: Kameron Michaels\",\"episode: 9\nrunning_total: 2.41666667\ncontestant: Kameron Michaels\",\"episode: 10\nrunning_total: 2.50000000\ncontestant: Kameron Michaels\",\"episode: 11\nrunning_total: 2.58333333\ncontestant: Kameron Michaels\",\"episode: 14\nrunning_total: 3.00000000\ncontestant: Kameron Michaels\",\"episode: 1\nrunning_total: 0.14285714\ncontestant: Kandy Ho\",\"episode: 2\nrunning_total: 0.57142857\ncontestant: Kandy Ho\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Kandy Ho\",\"episode: 4\nrunning_total: 1.28571429\ncontestant: Kandy Ho\",\"episode: 5\nrunning_total: 1.42857143\ncontestant: Kandy Ho\",\"episode: 6\nrunning_total: 1.42857143\ncontestant: Kandy Ho\",\"episode: 8\nrunning_total: 1.42857143\ncontestant: Kandy Ho\",\"episode: 1\nrunning_total: 0.27272727\ncontestant: Katya\",\"episode: 2\nrunning_total: 0.36363636\ncontestant: Katya\",\"episode: 3\nrunning_total: 0.63636364\ncontestant: Katya\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Katya\",\"episode: 5\nrunning_total: 1.45454545\ncontestant: Katya\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Katya\",\"episode: 7\nrunning_total: 2.36363636\ncontestant: Katya\",\"episode: 8\nrunning_total: 2.72727273\ncontestant: Katya\",\"episode: 9\nrunning_total: 3.09090909\ncontestant: Katya\",\"episode: 10\nrunning_total: 3.63636364\ncontestant: Katya\",\"episode: 11\nrunning_total: 3.63636364\ncontestant: Katya\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Kelly Mantle\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Kennedy Davenport\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Kennedy Davenport\",\"episode: 3\nrunning_total: 0.66666667\ncontestant: Kennedy Davenport\",\"episode: 4\nrunning_total: 1.16666667\ncontestant: Kennedy Davenport\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Kennedy Davenport\",\"episode: 6\nrunning_total: 1.75000000\ncontestant: Kennedy Davenport\",\"episode: 7\nrunning_total: 2.25000000\ncontestant: Kennedy Davenport\",\"episode: 8\nrunning_total: 2.66666667\ncontestant: Kennedy Davenport\",\"episode: 9\nrunning_total: 3.00000000\ncontestant: Kennedy Davenport\",\"episode: 10\nrunning_total: 3.25000000\ncontestant: Kennedy Davenport\",\"episode: 11\nrunning_total: 3.33333333\ncontestant: Kennedy Davenport\",\"episode: 12\nrunning_total: 3.33333333\ncontestant: Kennedy Davenport\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Kenya Michaels\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Kenya Michaels\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: Kenya Michaels\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Kenya Michaels\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Kenya Michaels\",\"episode: 10\nrunning_total: 2.83333333\ncontestant: Kenya Michaels\",\"episode: 1\nrunning_total: 0.60000000\ncontestant: Kim Chi\",\"episode: 2\nrunning_total: 0.80000000\ncontestant: Kim Chi\",\"episode: 3\nrunning_total: 1.10000000\ncontestant: Kim Chi\",\"episode: 4\nrunning_total: 1.40000000\ncontestant: Kim Chi\",\"episode: 5\nrunning_total: 1.70000000\ncontestant: Kim Chi\",\"episode: 6\nrunning_total: 2.10000000\ncontestant: Kim Chi\",\"episode: 7\nrunning_total: 2.40000000\ncontestant: Kim Chi\",\"episode: 8\nrunning_total: 3.00000000\ncontestant: Kim Chi\",\"episode: 9\nrunning_total: 3.60000000\ncontestant: Kim Chi\",\"episode: 10\nrunning_total: 4.00000000\ncontestant: Kim Chi\",\"episode: 1\nrunning_total: 1.00000000\ncontestant: Kimora Blac\",\"episode: 2\nrunning_total: 1.33333333\ncontestant: Kimora Blac\",\"episode: 3\nrunning_total: 1.33333333\ncontestant: Kimora Blac\",\"episode: 1\nrunning_total: 0.71428571\ncontestant: Laganja Estranja\",\"episode: 3\nrunning_total: 1.14285714\ncontestant: Laganja Estranja\",\"episode: 4\nrunning_total: 1.57142857\ncontestant: Laganja Estranja\",\"episode: 5\nrunning_total: 1.71428571\ncontestant: Laganja Estranja\",\"episode: 6\nrunning_total: 2.14285714\ncontestant: Laganja Estranja\",\"episode: 7\nrunning_total: 3.00000000\ncontestant: Laganja Estranja\",\"episode: 8\nrunning_total: 3.00000000\ncontestant: Laganja Estranja\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Laila McQueen\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Laila McQueen\",\"episode: 1\nrunning_total: 2.00000000\ncontestant: Lashauwn Beyond\",\"episode: 2\nrunning_total: 2.00000000\ncontestant: Lashauwn Beyond\",\"episode: 1\nrunning_total: 0.27272727\ncontestant: Latrice Royale\",\"episode: 2\nrunning_total: 0.63636364\ncontestant: Latrice Royale\",\"episode: 3\nrunning_total: 0.90909091\ncontestant: Latrice Royale\",\"episode: 4\nrunning_total: 1.45454545\ncontestant: Latrice Royale\",\"episode: 5\nrunning_total: 1.72727273\ncontestant: Latrice Royale\",\"episode: 6\nrunning_total: 2.09090909\ncontestant: Latrice Royale\",\"episode: 7\nrunning_total: 2.54545455\ncontestant: Latrice Royale\",\"episode: 8\nrunning_total: 3.09090909\ncontestant: Latrice Royale\",\"episode: 9\nrunning_total: 3.18181818\ncontestant: Latrice Royale\",\"episode: 10\nrunning_total: 3.27272727\ncontestant: Latrice Royale\",\"episode: 11\nrunning_total: 3.27272727\ncontestant: Latrice Royale\",\"episode: 1\nrunning_total: 0.80000000\ncontestant: Lineysha Sparx\",\"episode: 2\nrunning_total: 2.00000000\ncontestant: Lineysha Sparx\",\"episode: 3\nrunning_total: 2.40000000\ncontestant: Lineysha Sparx\",\"episode: 4\nrunning_total: 3.00000000\ncontestant: Lineysha Sparx\",\"episode: 5\nrunning_total: 3.00000000\ncontestant: Lineysha Sparx\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Madame LaQueer\",\"episode: 2\nrunning_total: 2.25000000\ncontestant: Madame LaQueer\",\"episode: 3\nrunning_total: 2.75000000\ncontestant: Madame LaQueer\",\"episode: 4\nrunning_total: 3.00000000\ncontestant: Madame LaQueer\",\"episode: 2\nrunning_total: 0.00000000\ncontestant: Magnolia Crawford\",\"episode: 2\nrunning_total: 0.30769231\ncontestant: Manila Luzon\",\"episode: 3\nrunning_total: 0.53846154\ncontestant: Manila Luzon\",\"episode: 4\nrunning_total: 0.76923077\ncontestant: Manila Luzon\",\"episode: 5\nrunning_total: 1.23076923\ncontestant: Manila Luzon\",\"episode: 6\nrunning_total: 1.46153846\ncontestant: Manila Luzon\",\"episode: 7\nrunning_total: 1.76923077\ncontestant: Manila Luzon\",\"episode: 8\nrunning_total: 1.84615385\ncontestant: Manila Luzon\",\"episode: 9\nrunning_total: 2.23076923\ncontestant: Manila Luzon\",\"episode: 10\nrunning_total: 2.61538462\ncontestant: Manila Luzon\",\"episode: 11\nrunning_total: 2.92307692\ncontestant: Manila Luzon\",\"episode: 12\nrunning_total: 3.38461538\ncontestant: Manila Luzon\",\"episode: 13\nrunning_total: 3.84615385\ncontestant: Manila Luzon\",\"episode: 15\nrunning_total: 4.15384615\ncontestant: Manila Luzon\",\"episode: 2\nrunning_total: 0.60000000\ncontestant: Mariah\",\"episode: 3\nrunning_total: 1.20000000\ncontestant: Mariah\",\"episode: 4\nrunning_total: 1.60000000\ncontestant: Mariah\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Mariah\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Mariah\",\"episode: 1\nrunning_total: 0.37500000\ncontestant: Max\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Max\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: Max\",\"episode: 4\nrunning_total: 1.87500000\ncontestant: Max\",\"episode: 5\nrunning_total: 2.62500000\ncontestant: Max\",\"episode: 6\nrunning_total: 3.00000000\ncontestant: Max\",\"episode: 7\nrunning_total: 3.00000000\ncontestant: Max\",\"episode: 8\nrunning_total: 3.00000000\ncontestant: Max\",\"episode: 1\nrunning_total: 1.20000000\ncontestant: Mayhem Miller\",\"episode: 2\nrunning_total: 1.80000000\ncontestant: Mayhem Miller\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Mayhem Miller\",\"episode: 4\nrunning_total: 2.60000000\ncontestant: Mayhem Miller\",\"episode: 5\nrunning_total: 2.60000000\ncontestant: Mayhem Miller\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Mercedes Iman Diamond\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Mercedes Iman Diamond\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: Mercedes Iman Diamond\",\"episode: 4\nrunning_total: 1.50000000\ncontestant: Mercedes Iman Diamond\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Milan\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Milan\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: Milan\",\"episode: 4\nrunning_total: 1.66666667\ncontestant: Milan\",\"episode: 5\nrunning_total: 1.83333333\ncontestant: Milan\",\"episode: 6\nrunning_total: 1.83333333\ncontestant: Milan\",\"episode: 2\nrunning_total: 0.80000000\ncontestant: Milk\",\"episode: 3\nrunning_total: 1.40000000\ncontestant: Milk\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Milk\",\"episode: 5\nrunning_total: 2.40000000\ncontestant: Milk\",\"episode: 6\nrunning_total: 2.40000000\ncontestant: Milk\",\"episode: 2\nrunning_total: 1.33333333\ncontestant: Mimi Imfurst\",\"episode: 3\nrunning_total: 2.33333333\ncontestant: Mimi Imfurst\",\"episode: 4\nrunning_total: 2.33333333\ncontestant: Mimi Imfurst\",\"episode: 1\nrunning_total: 0.44444444\ncontestant: Miss Fame\",\"episode: 2\nrunning_total: 0.66666667\ncontestant: Miss Fame\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Miss Fame\",\"episode: 4\nrunning_total: 1.44444444\ncontestant: Miss Fame\",\"episode: 5\nrunning_total: 1.77777778\ncontestant: Miss Fame\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Miss Fame\",\"episode: 7\nrunning_total: 2.22222222\ncontestant: Miss Fame\",\"episode: 8\nrunning_total: 2.44444444\ncontestant: Miss Fame\",\"episode: 9\nrunning_total: 2.44444444\ncontestant: Miss Fame\",\"episode: 1\nrunning_total: 0.36363636\ncontestant: Miz Cracker\",\"episode: 2\nrunning_total: 0.63636364\ncontestant: Miz Cracker\",\"episode: 3\nrunning_total: 0.90909091\ncontestant: Miz Cracker\",\"episode: 4\nrunning_total: 1.27272727\ncontestant: Miz Cracker\",\"episode: 5\nrunning_total: 1.63636364\ncontestant: Miz Cracker\",\"episode: 6\nrunning_total: 1.81818182\ncontestant: Miz Cracker\",\"episode: 7\nrunning_total: 2.09090909\ncontestant: Miz Cracker\",\"episode: 8\nrunning_total: 2.36363636\ncontestant: Miz Cracker\",\"episode: 9\nrunning_total: 2.54545455\ncontestant: Miz Cracker\",\"episode: 10\nrunning_total: 3.09090909\ncontestant: Miz Cracker\",\"episode: 11\nrunning_total: 3.09090909\ncontestant: Miz Cracker\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Monét X Change\",\"episode: 2\nrunning_total: 0.80000000\ncontestant: Monét X Change\",\"episode: 3\nrunning_total: 1.10000000\ncontestant: Monét X Change\",\"episode: 4\nrunning_total: 1.20000000\ncontestant: Monét X Change\",\"episode: 5\nrunning_total: 1.30000000\ncontestant: Monét X Change\",\"episode: 6\nrunning_total: 1.70000000\ncontestant: Monét X Change\",\"episode: 7\nrunning_total: 2.10000000\ncontestant: Monét X Change\",\"episode: 8\nrunning_total: 2.50000000\ncontestant: Monét X Change\",\"episode: 9\nrunning_total: 2.90000000\ncontestant: Monét X Change\",\"episode: 10\nrunning_total: 2.90000000\ncontestant: Monét X Change\",\"episode: 1\nrunning_total: 1.00000000\ncontestant: Monica Beverly Hillz\",\"episode: 2\nrunning_total: 1.33333333\ncontestant: Monica Beverly Hillz\",\"episode: 3\nrunning_total: 1.33333333\ncontestant: Monica Beverly Hillz\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: Monique Heart\",\"episode: 2\nrunning_total: 0.71428571\ncontestant: Monique Heart\",\"episode: 3\nrunning_total: 1.14285714\ncontestant: Monique Heart\",\"episode: 4\nrunning_total: 1.57142857\ncontestant: Monique Heart\",\"episode: 5\nrunning_total: 2.14285714\ncontestant: Monique Heart\",\"episode: 6\nrunning_total: 2.57142857\ncontestant: Monique Heart\",\"episode: 7\nrunning_total: 2.57142857\ncontestant: Monique Heart\",\"episode: 1\nrunning_total: 1.20000000\ncontestant: Morgan McMichaels\",\"episode: 2\nrunning_total: 1.80000000\ncontestant: Morgan McMichaels\",\"episode: 3\nrunning_total: 2.40000000\ncontestant: Morgan McMichaels\",\"episode: 4\nrunning_total: 2.60000000\ncontestant: Morgan McMichaels\",\"episode: 5\nrunning_total: 2.60000000\ncontestant: Morgan McMichaels\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Mrs. Kasha Davis\",\"episode: 2\nrunning_total: 1.16666667\ncontestant: Mrs. Kasha Davis\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: Mrs. Kasha Davis\",\"episode: 4\nrunning_total: 2.16666667\ncontestant: Mrs. Kasha Davis\",\"episode: 5\nrunning_total: 3.00000000\ncontestant: Mrs. Kasha Davis\",\"episode: 8\nrunning_total: 3.00000000\ncontestant: Mrs. Kasha Davis\",\"episode: 1\nrunning_total: 0.66666667\ncontestant: Mystique Summers Madison\",\"episode: 2\nrunning_total: 1.66666667\ncontestant: Mystique Summers Madison\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Mystique Summers Madison\",\"episode: 1\nrunning_total: 0.40000000\ncontestant: Naomi Smalls\",\"episode: 2\nrunning_total: 0.70000000\ncontestant: Naomi Smalls\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Naomi Smalls\",\"episode: 4\nrunning_total: 1.40000000\ncontestant: Naomi Smalls\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Naomi Smalls\",\"episode: 6\nrunning_total: 2.10000000\ncontestant: Naomi Smalls\",\"episode: 7\nrunning_total: 2.40000000\ncontestant: Naomi Smalls\",\"episode: 8\nrunning_total: 2.80000000\ncontestant: Naomi Smalls\",\"episode: 9\nrunning_total: 3.40000000\ncontestant: Naomi Smalls\",\"episode: 10\nrunning_total: 3.80000000\ncontestant: Naomi Smalls\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Naysha Lopez\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Naysha Lopez\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Naysha Lopez\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Nicky Doll\",\"episode: 3\nrunning_total: 0.75000000\ncontestant: Nicky Doll\",\"episode: 4\nrunning_total: 1.75000000\ncontestant: Nicky Doll\",\"episode: 5\nrunning_total: 1.75000000\ncontestant: Nicky Doll\",\"episode: 1\nrunning_total: 1.50000000\ncontestant: Nicole Paige Brooks\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Nicole Paige Brooks\",\"episode: 1\nrunning_total: 0.60000000\ncontestant: Nina Bo'nina Brown\",\"episode: 2\nrunning_total: 0.90000000\ncontestant: Nina Bo'nina Brown\",\"episode: 3\nrunning_total: 1.20000000\ncontestant: Nina Bo'nina Brown\",\"episode: 4\nrunning_total: 1.60000000\ncontestant: Nina Bo'nina Brown\",\"episode: 5\nrunning_total: 1.80000000\ncontestant: Nina Bo'nina Brown\",\"episode: 6\nrunning_total: 2.20000000\ncontestant: Nina Bo'nina Brown\",\"episode: 7\nrunning_total: 2.30000000\ncontestant: Nina Bo'nina Brown\",\"episode: 8\nrunning_total: 2.60000000\ncontestant: Nina Bo'nina Brown\",\"episode: 9\nrunning_total: 2.70000000\ncontestant: Nina Bo'nina Brown\",\"episode: 10\nrunning_total: 2.70000000\ncontestant: Nina Bo'nina Brown\",\"episode: 1\nrunning_total: 0.85714286\ncontestant: Nina Flowers\",\"episode: 2\nrunning_total: 1.28571429\ncontestant: Nina Flowers\",\"episode: 3\nrunning_total: 1.85714286\ncontestant: Nina Flowers\",\"episode: 4\nrunning_total: 2.42857143\ncontestant: Nina Flowers\",\"episode: 5\nrunning_total: 2.71428571\ncontestant: Nina Flowers\",\"episode: 6\nrunning_total: 3.42857143\ncontestant: Nina Flowers\",\"episode: 8\nrunning_total: 4.57142857\ncontestant: Nina Flowers\",\"episode: 1\nrunning_total: 0.16666667\ncontestant: Nina West\",\"episode: 2\nrunning_total: 0.41666667\ncontestant: Nina West\",\"episode: 3\nrunning_total: 0.91666667\ncontestant: Nina West\",\"episode: 4\nrunning_total: 1.16666667\ncontestant: Nina West\",\"episode: 5\nrunning_total: 1.41666667\ncontestant: Nina West\",\"episode: 6\nrunning_total: 1.58333333\ncontestant: Nina West\",\"episode: 7\nrunning_total: 1.75000000\ncontestant: Nina West\",\"episode: 8\nrunning_total: 2.08333333\ncontestant: Nina West\",\"episode: 9\nrunning_total: 2.33333333\ncontestant: Nina West\",\"episode: 10\nrunning_total: 2.83333333\ncontestant: Nina West\",\"episode: 11\nrunning_total: 2.83333333\ncontestant: Nina West\",\"episode: 14\nrunning_total: 3.50000000\ncontestant: Nina West\",\"episode: 1\nrunning_total: 0.80000000\ncontestant: Ongina\",\"episode: 2\nrunning_total: 2.00000000\ncontestant: Ongina\",\"episode: 3\nrunning_total: 2.80000000\ncontestant: Ongina\",\"episode: 4\nrunning_total: 4.00000000\ncontestant: Ongina\",\"episode: 5\nrunning_total: 4.00000000\ncontestant: Ongina\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Pandora Boxx\",\"episode: 2\nrunning_total: 0.87500000\ncontestant: Pandora Boxx\",\"episode: 3\nrunning_total: 1.37500000\ncontestant: Pandora Boxx\",\"episode: 4\nrunning_total: 1.87500000\ncontestant: Pandora Boxx\",\"episode: 5\nrunning_total: 2.25000000\ncontestant: Pandora Boxx\",\"episode: 6\nrunning_total: 2.87500000\ncontestant: Pandora Boxx\",\"episode: 7\nrunning_total: 3.12500000\ncontestant: Pandora Boxx\",\"episode: 8\nrunning_total: 3.12500000\ncontestant: Pandora Boxx\",\"episode: 1\nrunning_total: 0.23076923\ncontestant: Pearl\",\"episode: 2\nrunning_total: 0.46153846\ncontestant: Pearl\",\"episode: 3\nrunning_total: 0.61538462\ncontestant: Pearl\",\"episode: 4\nrunning_total: 0.69230769\ncontestant: Pearl\",\"episode: 5\nrunning_total: 1.15384615\ncontestant: Pearl\",\"episode: 6\nrunning_total: 1.38461538\ncontestant: Pearl\",\"episode: 7\nrunning_total: 1.61538462\ncontestant: Pearl\",\"episode: 8\nrunning_total: 2.07692308\ncontestant: Pearl\",\"episode: 9\nrunning_total: 2.15384615\ncontestant: Pearl\",\"episode: 10\nrunning_total: 2.38461538\ncontestant: Pearl\",\"episode: 11\nrunning_total: 2.69230769\ncontestant: Pearl\",\"episode: 12\nrunning_total: 3.15384615\ncontestant: Pearl\",\"episode: 14\nrunning_total: 3.46153846\ncontestant: Pearl\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Penny Tration\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Peppermint\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Peppermint\",\"episode: 3\nrunning_total: 0.83333333\ncontestant: Peppermint\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Peppermint\",\"episode: 5\nrunning_total: 1.33333333\ncontestant: Peppermint\",\"episode: 6\nrunning_total: 1.41666667\ncontestant: Peppermint\",\"episode: 7\nrunning_total: 1.66666667\ncontestant: Peppermint\",\"episode: 8\nrunning_total: 2.16666667\ncontestant: Peppermint\",\"episode: 9\nrunning_total: 2.50000000\ncontestant: Peppermint\",\"episode: 10\nrunning_total: 2.83333333\ncontestant: Peppermint\",\"episode: 11\nrunning_total: 2.91666667\ncontestant: Peppermint\",\"episode: 14\nrunning_total: 3.33333333\ncontestant: Peppermint\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Phi Phi O'Hara\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Phi Phi O'Hara\",\"episode: 3\nrunning_total: 0.83333333\ncontestant: Phi Phi O'Hara\",\"episode: 4\nrunning_total: 1.08333333\ncontestant: Phi Phi O'Hara\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Phi Phi O'Hara\",\"episode: 6\nrunning_total: 1.83333333\ncontestant: Phi Phi O'Hara\",\"episode: 7\nrunning_total: 2.33333333\ncontestant: Phi Phi O'Hara\",\"episode: 8\nrunning_total: 2.41666667\ncontestant: Phi Phi O'Hara\",\"episode: 9\nrunning_total: 2.83333333\ncontestant: Phi Phi O'Hara\",\"episode: 10\nrunning_total: 3.33333333\ncontestant: Phi Phi O'Hara\",\"episode: 11\nrunning_total: 3.66666667\ncontestant: Phi Phi O'Hara\",\"episode: 14\nrunning_total: 4.00000000\ncontestant: Phi Phi O'Hara\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Phoenix\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Phoenix\",\"episode: 1\nrunning_total: 0.44444444\ncontestant: Plastique Tiara\",\"episode: 2\nrunning_total: 0.88888889\ncontestant: Plastique Tiara\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Plastique Tiara\",\"episode: 4\nrunning_total: 1.33333333\ncontestant: Plastique Tiara\",\"episode: 5\nrunning_total: 1.77777778\ncontestant: Plastique Tiara\",\"episode: 6\nrunning_total: 2.22222222\ncontestant: Plastique Tiara\",\"episode: 7\nrunning_total: 2.88888889\ncontestant: Plastique Tiara\",\"episode: 8\nrunning_total: 3.22222222\ncontestant: Plastique Tiara\",\"episode: 9\nrunning_total: 3.22222222\ncontestant: Plastique Tiara\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: Ra'Jah O'Hara\",\"episode: 2\nrunning_total: 0.85714286\ncontestant: Ra'Jah O'Hara\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Ra'Jah O'Hara\",\"episode: 4\nrunning_total: 1.14285714\ncontestant: Ra'Jah O'Hara\",\"episode: 5\nrunning_total: 1.57142857\ncontestant: Ra'Jah O'Hara\",\"episode: 6\nrunning_total: 1.71428571\ncontestant: Ra'Jah O'Hara\",\"episode: 7\nrunning_total: 1.71428571\ncontestant: Ra'Jah O'Hara\",\"episode: 2\nrunning_total: 0.46153846\ncontestant: Raja\",\"episode: 3\nrunning_total: 0.69230769\ncontestant: Raja\",\"episode: 4\nrunning_total: 0.92307692\ncontestant: Raja\",\"episode: 5\nrunning_total: 1.15384615\ncontestant: Raja\",\"episode: 6\nrunning_total: 1.46153846\ncontestant: Raja\",\"episode: 7\nrunning_total: 1.92307692\ncontestant: Raja\",\"episode: 8\nrunning_total: 2.23076923\ncontestant: Raja\",\"episode: 9\nrunning_total: 2.38461538\ncontestant: Raja\",\"episode: 10\nrunning_total: 2.84615385\ncontestant: Raja\",\"episode: 11\nrunning_total: 3.23076923\ncontestant: Raja\",\"episode: 12\nrunning_total: 3.30769231\ncontestant: Raja\",\"episode: 13\nrunning_total: 3.61538462\ncontestant: Raja\",\"episode: 15\nrunning_total: 4.38461538\ncontestant: Raja\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Raven\",\"episode: 2\nrunning_total: 0.60000000\ncontestant: Raven\",\"episode: 3\nrunning_total: 0.70000000\ncontestant: Raven\",\"episode: 4\nrunning_total: 1.20000000\ncontestant: Raven\",\"episode: 5\nrunning_total: 1.70000000\ncontestant: Raven\",\"episode: 6\nrunning_total: 2.10000000\ncontestant: Raven\",\"episode: 7\nrunning_total: 2.70000000\ncontestant: Raven\",\"episode: 8\nrunning_total: 3.30000000\ncontestant: Raven\",\"episode: 9\nrunning_total: 3.70000000\ncontestant: Raven\",\"episode: 11\nrunning_total: 4.10000000\ncontestant: Raven\",\"episode: 1\nrunning_total: 0.28571429\ncontestant: Rebecca Glasscock\",\"episode: 2\nrunning_total: 0.71428571\ncontestant: Rebecca Glasscock\",\"episode: 3\nrunning_total: 1.28571429\ncontestant: Rebecca Glasscock\",\"episode: 4\nrunning_total: 1.42857143\ncontestant: Rebecca Glasscock\",\"episode: 5\nrunning_total: 2.28571429\ncontestant: Rebecca Glasscock\",\"episode: 6\nrunning_total: 2.42857143\ncontestant: Rebecca Glasscock\",\"episode: 8\nrunning_total: 2.42857143\ncontestant: Rebecca Glasscock\",\"episode: 1\nrunning_total: 0.83333333\ncontestant: Robbie Turner\",\"episode: 2\nrunning_total: 1.33333333\ncontestant: Robbie Turner\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: Robbie Turner\",\"episode: 4\nrunning_total: 2.50000000\ncontestant: Robbie Turner\",\"episode: 5\nrunning_total: 2.83333333\ncontestant: Robbie Turner\",\"episode: 6\nrunning_total: 2.83333333\ncontestant: Robbie Turner\",\"episode: 2\nrunning_total: 0.66666667\ncontestant: Rock M. Sakura\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: Rock M. Sakura\",\"episode: 4\nrunning_total: 1.66666667\ncontestant: Rock M. Sakura\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Roxxxy Andrews\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Roxxxy Andrews\",\"episode: 3\nrunning_total: 1.08333333\ncontestant: Roxxxy Andrews\",\"episode: 4\nrunning_total: 1.25000000\ncontestant: Roxxxy Andrews\",\"episode: 5\nrunning_total: 1.66666667\ncontestant: Roxxxy Andrews\",\"episode: 6\nrunning_total: 2.00000000\ncontestant: Roxxxy Andrews\",\"episode: 7\nrunning_total: 2.08333333\ncontestant: Roxxxy Andrews\",\"episode: 8\nrunning_total: 2.25000000\ncontestant: Roxxxy Andrews\",\"episode: 9\nrunning_total: 2.58333333\ncontestant: Roxxxy Andrews\",\"episode: 10\nrunning_total: 3.08333333\ncontestant: Roxxxy Andrews\",\"episode: 11\nrunning_total: 3.41666667\ncontestant: Roxxxy Andrews\",\"episode: 14\nrunning_total: 3.75000000\ncontestant: Roxxxy Andrews\",\"episode: 1\nrunning_total: 0.16666667\ncontestant: Sahara Davenport\",\"episode: 2\nrunning_total: 1.16666667\ncontestant: Sahara Davenport\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: Sahara Davenport\",\"episode: 4\nrunning_total: 2.16666667\ncontestant: Sahara Davenport\",\"episode: 5\nrunning_total: 2.33333333\ncontestant: Sahara Davenport\",\"episode: 6\nrunning_total: 2.33333333\ncontestant: Sahara Davenport\",\"episode: 1\nrunning_total: 0.66666667\ncontestant: Sasha Belle\",\"episode: 2\nrunning_total: 0.66666667\ncontestant: Sasha Belle\",\"episode: 8\nrunning_total: 0.66666667\ncontestant: Sasha Belle\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Sasha Velour\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Sasha Velour\",\"episode: 3\nrunning_total: 0.83333333\ncontestant: Sasha Velour\",\"episode: 4\nrunning_total: 1.33333333\ncontestant: Sasha Velour\",\"episode: 5\nrunning_total: 1.58333333\ncontestant: Sasha Velour\",\"episode: 6\nrunning_total: 1.91666667\ncontestant: Sasha Velour\",\"episode: 7\nrunning_total: 2.08333333\ncontestant: Sasha Velour\",\"episode: 8\nrunning_total: 2.41666667\ncontestant: Sasha Velour\",\"episode: 9\nrunning_total: 2.91666667\ncontestant: Sasha Velour\",\"episode: 10\nrunning_total: 3.25000000\ncontestant: Sasha Velour\",\"episode: 11\nrunning_total: 3.66666667\ncontestant: Sasha Velour\",\"episode: 14\nrunning_total: 4.50000000\ncontestant: Sasha Velour\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Scarlet Envy\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Scarlet Envy\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: Scarlet Envy\",\"episode: 4\nrunning_total: 2.16666667\ncontestant: Scarlet Envy\",\"episode: 5\nrunning_total: 2.66666667\ncontestant: Scarlet Envy\",\"episode: 6\nrunning_total: 2.66666667\ncontestant: Scarlet Envy\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Serena Cha Cha\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Serena Cha Cha\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Shangela\",\"episode: 2\nrunning_total: 0.09090909\ncontestant: Shangela\",\"episode: 3\nrunning_total: 0.63636364\ncontestant: Shangela\",\"episode: 4\nrunning_total: 0.90909091\ncontestant: Shangela\",\"episode: 5\nrunning_total: 1.27272727\ncontestant: Shangela\",\"episode: 6\nrunning_total: 1.54545455\ncontestant: Shangela\",\"episode: 7\nrunning_total: 1.72727273\ncontestant: Shangela\",\"episode: 8\nrunning_total: 2.27272727\ncontestant: Shangela\",\"episode: 9\nrunning_total: 2.54545455\ncontestant: Shangela\",\"episode: 10\nrunning_total: 2.63636364\ncontestant: Shangela\",\"episode: 11\nrunning_total: 2.63636364\ncontestant: Shangela\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Shannel\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Shannel\",\"episode: 3\nrunning_total: 1.16666667\ncontestant: Shannel\",\"episode: 4\nrunning_total: 1.50000000\ncontestant: Shannel\",\"episode: 5\nrunning_total: 2.16666667\ncontestant: Shannel\",\"episode: 6\nrunning_total: 2.16666667\ncontestant: Shannel\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Sharon Needles\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Sharon Needles\",\"episode: 3\nrunning_total: 1.25000000\ncontestant: Sharon Needles\",\"episode: 4\nrunning_total: 1.50000000\ncontestant: Sharon Needles\",\"episode: 5\nrunning_total: 1.83333333\ncontestant: Sharon Needles\",\"episode: 6\nrunning_total: 2.08333333\ncontestant: Sharon Needles\",\"episode: 7\nrunning_total: 2.41666667\ncontestant: Sharon Needles\",\"episode: 8\nrunning_total: 2.50000000\ncontestant: Sharon Needles\",\"episode: 9\nrunning_total: 3.00000000\ncontestant: Sharon Needles\",\"episode: 10\nrunning_total: 3.16666667\ncontestant: Sharon Needles\",\"episode: 11\nrunning_total: 3.66666667\ncontestant: Sharon Needles\",\"episode: 14\nrunning_total: 4.50000000\ncontestant: Sharon Needles\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Shea Couleé\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Shea Couleé\",\"episode: 3\nrunning_total: 0.83333333\ncontestant: Shea Couleé\",\"episode: 4\nrunning_total: 1.33333333\ncontestant: Shea Couleé\",\"episode: 5\nrunning_total: 1.83333333\ncontestant: Shea Couleé\",\"episode: 6\nrunning_total: 2.08333333\ncontestant: Shea Couleé\",\"episode: 7\nrunning_total: 2.41666667\ncontestant: Shea Couleé\",\"episode: 8\nrunning_total: 2.75000000\ncontestant: Shea Couleé\",\"episode: 9\nrunning_total: 3.25000000\ncontestant: Shea Couleé\",\"episode: 10\nrunning_total: 3.33333333\ncontestant: Shea Couleé\",\"episode: 11\nrunning_total: 3.83333333\ncontestant: Shea Couleé\",\"episode: 14\nrunning_total: 3.83333333\ncontestant: Shea Couleé\",\"episode: 1\nrunning_total: 0.30000000\ncontestant: Shuga Cain\",\"episode: 2\nrunning_total: 0.70000000\ncontestant: Shuga Cain\",\"episode: 3\nrunning_total: 0.80000000\ncontestant: Shuga Cain\",\"episode: 4\nrunning_total: 1.10000000\ncontestant: Shuga Cain\",\"episode: 5\nrunning_total: 1.20000000\ncontestant: Shuga Cain\",\"episode: 6\nrunning_total: 1.50000000\ncontestant: Shuga Cain\",\"episode: 7\nrunning_total: 1.80000000\ncontestant: Shuga Cain\",\"episode: 8\nrunning_total: 2.20000000\ncontestant: Shuga Cain\",\"episode: 9\nrunning_total: 2.40000000\ncontestant: Shuga Cain\",\"episode: 10\nrunning_total: 2.40000000\ncontestant: Shuga Cain\",\"episode: 1\nrunning_total: 0.23076923\ncontestant: Silky Nutmeg Ganache\",\"episode: 2\nrunning_total: 0.46153846\ncontestant: Silky Nutmeg Ganache\",\"episode: 3\nrunning_total: 0.69230769\ncontestant: Silky Nutmeg Ganache\",\"episode: 4\nrunning_total: 1.15384615\ncontestant: Silky Nutmeg Ganache\",\"episode: 5\nrunning_total: 1.30769231\ncontestant: Silky Nutmeg Ganache\",\"episode: 6\nrunning_total: 1.53846154\ncontestant: Silky Nutmeg Ganache\",\"episode: 7\nrunning_total: 1.76923077\ncontestant: Silky Nutmeg Ganache\",\"episode: 8\nrunning_total: 2.23076923\ncontestant: Silky Nutmeg Ganache\",\"episode: 9\nrunning_total: 2.53846154\ncontestant: Silky Nutmeg Ganache\",\"episode: 10\nrunning_total: 2.69230769\ncontestant: Silky Nutmeg Ganache\",\"episode: 11\nrunning_total: 2.76923077\ncontestant: Silky Nutmeg Ganache\",\"episode: 12\nrunning_total: 3.00000000\ncontestant: Silky Nutmeg Ganache\",\"episode: 14\nrunning_total: 3.00000000\ncontestant: Silky Nutmeg Ganache\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Soju\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Sonique\",\"episode: 2\nrunning_total: 1.25000000\ncontestant: Sonique\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Sonique\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Sonique\",\"episode: 2\nrunning_total: 0.50000000\ncontestant: Stacy Layne Matthews\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Stacy Layne Matthews\",\"episode: 4\nrunning_total: 1.66666667\ncontestant: Stacy Layne Matthews\",\"episode: 5\nrunning_total: 1.83333333\ncontestant: Stacy Layne Matthews\",\"episode: 6\nrunning_total: 2.83333333\ncontestant: Stacy Layne Matthews\",\"episode: 7\nrunning_total: 2.83333333\ncontestant: Stacy Layne Matthews\",\"episode: 1\nrunning_total: 1.50000000\ncontestant: Tammie Brown\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Tammie Brown\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Tatianna\",\"episode: 2\nrunning_total: 0.66666667\ncontestant: Tatianna\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Tatianna\",\"episode: 4\nrunning_total: 1.66666667\ncontestant: Tatianna\",\"episode: 5\nrunning_total: 1.88888889\ncontestant: Tatianna\",\"episode: 6\nrunning_total: 2.22222222\ncontestant: Tatianna\",\"episode: 7\nrunning_total: 2.33333333\ncontestant: Tatianna\",\"episode: 8\nrunning_total: 2.88888889\ncontestant: Tatianna\",\"episode: 9\nrunning_total: 2.88888889\ncontestant: Tatianna\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Tempest DuJour\",\"episode: 8\nrunning_total: 0.00000000\ncontestant: Tempest DuJour\",\"episode: 1\nrunning_total: 1.33333333\ncontestant: The Princess\",\"episode: 2\nrunning_total: 1.66666667\ncontestant: The Princess\",\"episode: 3\nrunning_total: 1.66666667\ncontestant: The Princess\",\"episode: 1\nrunning_total: 0.37500000\ncontestant: The Vixen\",\"episode: 2\nrunning_total: 1.12500000\ncontestant: The Vixen\",\"episode: 3\nrunning_total: 1.50000000\ncontestant: The Vixen\",\"episode: 4\nrunning_total: 1.87500000\ncontestant: The Vixen\",\"episode: 5\nrunning_total: 2.50000000\ncontestant: The Vixen\",\"episode: 6\nrunning_total: 2.62500000\ncontestant: The Vixen\",\"episode: 7\nrunning_total: 2.75000000\ncontestant: The Vixen\",\"episode: 8\nrunning_total: 2.75000000\ncontestant: The Vixen\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: Thorgy Thor\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Thorgy Thor\",\"episode: 3\nrunning_total: 1.57142857\ncontestant: Thorgy Thor\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Thorgy Thor\",\"episode: 5\nrunning_total: 2.57142857\ncontestant: Thorgy Thor\",\"episode: 6\nrunning_total: 3.00000000\ncontestant: Thorgy Thor\",\"episode: 7\nrunning_total: 3.00000000\ncontestant: Thorgy Thor\",\"episode: 2\nrunning_total: 0.62500000\ncontestant: Trinity K. Bonet\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Trinity K. Bonet\",\"episode: 4\nrunning_total: 1.12500000\ncontestant: Trinity K. Bonet\",\"episode: 5\nrunning_total: 1.50000000\ncontestant: Trinity K. Bonet\",\"episode: 6\nrunning_total: 1.62500000\ncontestant: Trinity K. Bonet\",\"episode: 7\nrunning_total: 2.12500000\ncontestant: Trinity K. Bonet\",\"episode: 8\nrunning_total: 2.62500000\ncontestant: Trinity K. Bonet\",\"episode: 9\nrunning_total: 2.62500000\ncontestant: Trinity K. Bonet\",\"episode: 1\nrunning_total: 0.25000000\ncontestant: Trinity Taylor\",\"episode: 2\nrunning_total: 0.58333333\ncontestant: Trinity Taylor\",\"episode: 3\nrunning_total: 1.08333333\ncontestant: Trinity Taylor\",\"episode: 4\nrunning_total: 1.16666667\ncontestant: Trinity Taylor\",\"episode: 5\nrunning_total: 1.41666667\ncontestant: Trinity Taylor\",\"episode: 6\nrunning_total: 1.66666667\ncontestant: Trinity Taylor\",\"episode: 7\nrunning_total: 2.16666667\ncontestant: Trinity Taylor\",\"episode: 8\nrunning_total: 2.33333333\ncontestant: Trinity Taylor\",\"episode: 9\nrunning_total: 2.66666667\ncontestant: Trinity Taylor\",\"episode: 10\nrunning_total: 3.16666667\ncontestant: Trinity Taylor\",\"episode: 11\nrunning_total: 3.33333333\ncontestant: Trinity Taylor\",\"episode: 14\nrunning_total: 3.33333333\ncontestant: Trinity Taylor\",\"episode: 1\nrunning_total: 0.42857143\ncontestant: Trixie Mattel\",\"episode: 2\nrunning_total: 0.85714286\ncontestant: Trixie Mattel\",\"episode: 3\nrunning_total: 1.28571429\ncontestant: Trixie Mattel\",\"episode: 4\nrunning_total: 1.28571429\ncontestant: Trixie Mattel\",\"episode: 8\nrunning_total: 2.14285714\ncontestant: Trixie Mattel\",\"episode: 9\nrunning_total: 2.85714286\ncontestant: Trixie Mattel\",\"episode: 10\nrunning_total: 2.85714286\ncontestant: Trixie Mattel\",\"episode: 1\nrunning_total: 0.40000000\ncontestant: Tyra Sanchez\",\"episode: 2\nrunning_total: 0.70000000\ncontestant: Tyra Sanchez\",\"episode: 3\nrunning_total: 1.30000000\ncontestant: Tyra Sanchez\",\"episode: 4\nrunning_total: 1.60000000\ncontestant: Tyra Sanchez\",\"episode: 5\nrunning_total: 2.20000000\ncontestant: Tyra Sanchez\",\"episode: 6\nrunning_total: 2.40000000\ncontestant: Tyra Sanchez\",\"episode: 7\nrunning_total: 2.70000000\ncontestant: Tyra Sanchez\",\"episode: 8\nrunning_total: 3.10000000\ncontestant: Tyra Sanchez\",\"episode: 9\nrunning_total: 3.70000000\ncontestant: Tyra Sanchez\",\"episode: 11\nrunning_total: 4.70000000\ncontestant: Tyra Sanchez\",\"episode: 1\nrunning_total: 0.33333333\ncontestant: Valentina\",\"episode: 2\nrunning_total: 1.00000000\ncontestant: Valentina\",\"episode: 3\nrunning_total: 1.44444444\ncontestant: Valentina\",\"episode: 4\nrunning_total: 1.77777778\ncontestant: Valentina\",\"episode: 5\nrunning_total: 2.11111111\ncontestant: Valentina\",\"episode: 6\nrunning_total: 2.44444444\ncontestant: Valentina\",\"episode: 7\nrunning_total: 2.88888889\ncontestant: Valentina\",\"episode: 8\nrunning_total: 3.44444444\ncontestant: Valentina\",\"episode: 9\nrunning_total: 3.44444444\ncontestant: Valentina\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Vanessa Vanjie Mateo\",\"episode: 1\nrunning_total: 0.30769231\ncontestant: Vanessa Vanjie Mateo\",\"episode: 2\nrunning_total: 0.53846154\ncontestant: Vanessa Vanjie Mateo\",\"episode: 3\nrunning_total: 0.84615385\ncontestant: Vanessa Vanjie Mateo\",\"episode: 4\nrunning_total: 1.00000000\ncontestant: Vanessa Vanjie Mateo\",\"episode: 5\nrunning_total: 1.23076923\ncontestant: Vanessa Vanjie Mateo\",\"episode: 6\nrunning_total: 1.46153846\ncontestant: Vanessa Vanjie Mateo\",\"episode: 7\nrunning_total: 1.69230769\ncontestant: Vanessa Vanjie Mateo\",\"episode: 8\nrunning_total: 1.84615385\ncontestant: Vanessa Vanjie Mateo\",\"episode: 9\nrunning_total: 1.92307692\ncontestant: Vanessa Vanjie Mateo\",\"episode: 10\nrunning_total: 2.00000000\ncontestant: Vanessa Vanjie Mateo\",\"episode: 11\nrunning_total: 2.30769231\ncontestant: Vanessa Vanjie Mateo\",\"episode: 12\nrunning_total: 2.30769231\ncontestant: Vanessa Vanjie Mateo\",\"episode: 2\nrunning_total: 0.00000000\ncontestant: Venus D-Lite\",\"episode: 1\nrunning_total: 0.00000000\ncontestant: Victoria (Porkchop) Parker\",\"episode: 1\nrunning_total: 0.46153846\ncontestant: Violet Chachki\",\"episode: 2\nrunning_total: 0.76923077\ncontestant: Violet Chachki\",\"episode: 3\nrunning_total: 1.00000000\ncontestant: Violet Chachki\",\"episode: 4\nrunning_total: 1.23076923\ncontestant: Violet Chachki\",\"episode: 5\nrunning_total: 1.38461538\ncontestant: Violet Chachki\",\"episode: 6\nrunning_total: 1.61538462\ncontestant: Violet Chachki\",\"episode: 7\nrunning_total: 1.84615385\ncontestant: Violet Chachki\",\"episode: 8\nrunning_total: 2.15384615\ncontestant: Violet Chachki\",\"episode: 9\nrunning_total: 2.30769231\ncontestant: Violet Chachki\",\"episode: 10\nrunning_total: 2.76923077\ncontestant: Violet Chachki\",\"episode: 11\nrunning_total: 3.23076923\ncontestant: Violet Chachki\",\"episode: 12\nrunning_total: 3.69230769\ncontestant: Violet Chachki\",\"episode: 14\nrunning_total: 4.46153846\ncontestant: Violet Chachki\",\"episode: 1\nrunning_total: 0.50000000\ncontestant: Vivacious\",\"episode: 3\nrunning_total: 0.50000000\ncontestant: Vivacious\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Vivienne Pinay\",\"episode: 2\nrunning_total: 1.50000000\ncontestant: Vivienne Pinay\",\"episode: 3\nrunning_total: 2.00000000\ncontestant: Vivienne Pinay\",\"episode: 4\nrunning_total: 2.00000000\ncontestant: Vivienne Pinay\",\"episode: 1\nrunning_total: 0.75000000\ncontestant: Widow Von'Du\",\"episode: 3\nrunning_total: 1.12500000\ncontestant: Widow Von'Du\",\"episode: 4\nrunning_total: 1.50000000\ncontestant: Widow Von'Du\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Widow Von'Du\",\"episode: 6\nrunning_total: 2.37500000\ncontestant: Widow Von'Du\",\"episode: 7\nrunning_total: 2.75000000\ncontestant: Widow Von'Du\",\"episode: 8\nrunning_total: 2.87500000\ncontestant: Widow Von'Du\",\"episode: 9\nrunning_total: 2.87500000\ncontestant: Widow Von'Du\",\"episode: 1\nrunning_total: 0.37500000\ncontestant: Willam\",\"episode: 2\nrunning_total: 0.75000000\ncontestant: Willam\",\"episode: 3\nrunning_total: 1.12500000\ncontestant: Willam\",\"episode: 4\nrunning_total: 1.50000000\ncontestant: Willam\",\"episode: 5\nrunning_total: 2.00000000\ncontestant: Willam\",\"episode: 6\nrunning_total: 2.75000000\ncontestant: Willam\",\"episode: 7\nrunning_total: 2.87500000\ncontestant: Willam\",\"episode: 8\nrunning_total: 3.62500000\ncontestant: Willam\",\"episode: 2\nrunning_total: 0.25000000\ncontestant: Yara Sofia\",\"episode: 3\nrunning_total: 0.50000000\ncontestant: Yara Sofia\",\"episode: 4\nrunning_total: 0.83333333\ncontestant: Yara Sofia\",\"episode: 5\nrunning_total: 1.08333333\ncontestant: Yara Sofia\",\"episode: 6\nrunning_total: 1.25000000\ncontestant: Yara Sofia\",\"episode: 7\nrunning_total: 1.58333333\ncontestant: Yara Sofia\",\"episode: 8\nrunning_total: 1.91666667\ncontestant: Yara Sofia\",\"episode: 9\nrunning_total: 2.00000000\ncontestant: Yara Sofia\",\"episode: 10\nrunning_total: 2.33333333\ncontestant: Yara Sofia\",\"episode: 11\nrunning_total: 2.83333333\ncontestant: Yara Sofia\",\"episode: 12\nrunning_total: 3.00000000\ncontestant: Yara Sofia\",\"episode: 13\nrunning_total: 3.41666667\ncontestant: Yara Sofia\",\"episode: 1\nrunning_total: 1.33333333\ncontestant: Yuhua Hamasaki\",\"episode: 2\nrunning_total: 2.33333333\ncontestant: Yuhua Hamasaki\",\"episode: 3\nrunning_total: 2.33333333\ncontestant: Yuhua Hamasaki\",\"episode: 1\nrunning_total: 0.23076923\ncontestant: Yvie Oddly\",\"episode: 2\nrunning_total: 0.69230769\ncontestant: Yvie Oddly\",\"episode: 3\nrunning_total: 0.92307692\ncontestant: Yvie Oddly\",\"episode: 4\nrunning_total: 1.23076923\ncontestant: Yvie Oddly\",\"episode: 5\nrunning_total: 1.53846154\ncontestant: Yvie Oddly\",\"episode: 6\nrunning_total: 1.76923077\ncontestant: Yvie Oddly\",\"episode: 7\nrunning_total: 2.07692308\ncontestant: Yvie Oddly\",\"episode: 8\nrunning_total: 2.15384615\ncontestant: Yvie Oddly\",\"episode: 9\nrunning_total: 2.38461538\ncontestant: Yvie Oddly\",\"episode: 10\nrunning_total: 2.61538462\ncontestant: Yvie Oddly\",\"episode: 11\nrunning_total: 2.76923077\ncontestant: Yvie Oddly\",\"episode: 12\nrunning_total: 3.00000000\ncontestant: Yvie Oddly\",\"episode: 14\nrunning_total: 3.76923077\ncontestant: Yvie Oddly\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,255,255,1)\",\"opacity\":0.5,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,255,255,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"y\":[0.506742136696888,0.875243250067657,1.20979520167453,1.54123209175391,1.81812889312889,2.03740361440896,2.25182764053562,2.48490999388923,2.72337925987659,2.94145667167058,3.19773580612129,3.18517694426785,3.48878205128205,3.82099762982116,3.87179487179487],\"text\":[\"episode: 1\nrunning_total: 0.5067421\nepisode: 1\nrunning_total: 0.5067421\ncontestant: Mean Average\",\"episode: 2\nrunning_total: 0.8752433\nepisode: 2\nrunning_total: 0.8752433\ncontestant: Mean Average\",\"episode: 3\nrunning_total: 1.2097952\nepisode: 3\nrunning_total: 1.2097952\ncontestant: Mean Average\",\"episode: 4\nrunning_total: 1.5412321\nepisode: 4\nrunning_total: 1.5412321\ncontestant: Mean Average\",\"episode: 5\nrunning_total: 1.8181289\nepisode: 5\nrunning_total: 1.8181289\ncontestant: Mean Average\",\"episode: 6\nrunning_total: 2.0374036\nepisode: 6\nrunning_total: 2.0374036\ncontestant: Mean Average\",\"episode: 7\nrunning_total: 2.2518276\nepisode: 7\nrunning_total: 2.2518276\ncontestant: Mean Average\",\"episode: 8\nrunning_total: 2.4849100\nepisode: 8\nrunning_total: 2.4849100\ncontestant: Mean Average\",\"episode: 9\nrunning_total: 2.7233793\nepisode: 9\nrunning_total: 2.7233793\ncontestant: Mean Average\",\"episode: 10\nrunning_total: 2.9414567\nepisode: 10\nrunning_total: 2.9414567\ncontestant: Mean Average\",\"episode: 11\nrunning_total: 3.1977358\nepisode: 11\nrunning_total: 3.1977358\ncontestant: Mean Average\",\"episode: 12\nrunning_total: 3.1851769\nepisode: 12\nrunning_total: 3.1851769\ncontestant: Mean Average\",\"episode: 13\nrunning_total: 3.4887821\nepisode: 13\nrunning_total: 3.4887821\ncontestant: Mean Average\",\"episode: 14\nrunning_total: 3.8209976\nepisode: 14\nrunning_total: 3.8209976\ncontestant: Mean Average\",\"episode: 15\nrunning_total: 3.8717949\nepisode: 15\nrunning_total: 3.8717949\ncontestant: Mean Average\"],\"type\":\"scatter\",\"mode\":\"markers+lines\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,255,0,1)\",\"opacity\":1,\"size\":15.1181102362205,\"symbol\":\"star\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,255,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"line\":{\"width\":3.77952755905512,\"color\":\"rgba(255,255,0,1)\",\"dash\":\"solid\"},\"frame\":null},{\"x\":[2,3,4,5,6,7,8,9,10,11,12,14],\"y\":[0.5,0.75,1,1.33333333333333,1.66666666666667,2,2.5,2.75,3.25,3.58333333333333,4.08333333333333,4.91666666666667],\"text\":[\"episode: 2\nrunning_total: 0.500000\nepisode: 2\nrunning_total: 0.500000\ncontestant: Bianca Del Rio\",\"episode: 3\nrunning_total: 0.750000\nepisode: 3\nrunning_total: 0.750000\ncontestant: Bianca Del Rio\",\"episode: 4\nrunning_total: 1.000000\nepisode: 4\nrunning_total: 1.000000\ncontestant: Bianca Del Rio\",\"episode: 5\nrunning_total: 1.333333\nepisode: 5\nrunning_total: 1.333333\ncontestant: Bianca Del Rio\",\"episode: 6\nrunning_total: 1.666667\nepisode: 6\nrunning_total: 1.666667\ncontestant: Bianca Del Rio\",\"episode: 7\nrunning_total: 2.000000\nepisode: 7\nrunning_total: 2.000000\ncontestant: Bianca Del Rio\",\"episode: 8\nrunning_total: 2.500000\nepisode: 8\nrunning_total: 2.500000\ncontestant: Bianca Del Rio\",\"episode: 9\nrunning_total: 2.750000\nepisode: 9\nrunning_total: 2.750000\ncontestant: Bianca Del Rio\",\"episode: 10\nrunning_total: 3.250000\nepisode: 10\nrunning_total: 3.250000\ncontestant: Bianca Del Rio\",\"episode: 11\nrunning_total: 3.583333\nepisode: 11\nrunning_total: 3.583333\ncontestant: Bianca Del Rio\",\"episode: 12\nrunning_total: 4.083333\nepisode: 12\nrunning_total: 4.083333\ncontestant: Bianca Del Rio\",\"episode: 14\nrunning_total: 4.916667\nepisode: 14\nrunning_total: 4.916667\ncontestant: Bianca Del Rio\"],\"type\":\"scatter\",\"mode\":\"markers+lines\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,255,1)\",\"opacity\":1,\"size\":15.1181102362205,\"symbol\":\"star\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,255,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"line\":{\"width\":3.77952755905512,\"color\":\"rgba(0,0,255,1)\",\"dash\":\"solid\"},\"frame\":null}],\"layout\":{\"margin\":{\"t\":51.7326691573267,\"r\":7.30593607305936,\"b\":44.167704441677,\"l\":35.4005811540058},\"plot_bgcolor\":\"rgba(234,37,154,1)\",\"paper_bgcolor\":\"rgba(211,25,136,1)\",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\" Bianca Del Rio Is the Best Drag Race Contestant Of All Time \",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":25.5043586550436},\"x\":0.5,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.195,16.195],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(190,190,190,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\" Episode \",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":18.5969281859693}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.245833333333333,5.1625],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"],\"tickvals\":[0,1,2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(190,190,190,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\" Running Outcome Score (Weighted) \",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":18.5969281859693}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"4ad02d955069\":{\"type\":\"scatter\"},\"4ad028117eb7\":{\"x\":{},\"y\":{}},\"4ad02fd511b7\":{\"x\":{},\"y\":{},\"x.1\":{},\"y.1\":{}},\"4ad01efd5fba\":{\"x\":{},\"y\":{},\"x.1\":{},\"y.1\":{}},\"4ad0432d6b51\":{\"x\":{},\"y\":{},\"x.1\":{},\"y.1\":{}},\"4ad02f8a337f\":{\"x\":{},\"y\":{},\"x.1\":{},\"y.1\":{}}},\"cur_data\":\"4ad02d955069\",\"visdat\":{\"4ad02d955069\":[\"function (y) \",\"x\"],\"4ad028117eb7\":[\"function (y) \",\"x\"],\"4ad02fd511b7\":[\"function (y) \",\"x\"],\"4ad01efd5fba\":[\"function (y) \",\"x\"],\"4ad0432d6b51\":[\"function (y) \",\"x\"],\"4ad02f8a337f\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\rIdentical plot to main plot but made interactive using ggplotly. Bianca Del Rio’s running, weighted outcome score are highlighted using blue stars. The mean average running total of all contestants is highlighted using yellow stars.\r\rHover over individual data points to see individual contestants’ running, weighted outcome score by episode, labeled with contestant name.\r\r\r\r\rAdditional Line Plot - Top 5 and random contestant sample\rggplotly(top5_and_sample_plot)\r\r{\"x\":{\"data\":[{\"x\":[1,2,3,4,5,6,8],\"y\":[0.428571428571429,0.857142857142857,1.71428571428571,2.28571428571429,2.42857142857143,3.28571428571429,4.71428571428571],\"text\":[\"episode: 1\nrunning_total: 0.4285714\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\",\"episode: 2\nrunning_total: 0.8571429\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\",\"episode: 3\nrunning_total: 1.7142857\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\",\"episode: 4\nrunning_total: 2.2857143\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\",\"episode: 5\nrunning_total: 2.4285714\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\",\"episode: 6\nrunning_total: 3.2857143\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\",\"episode: 8\nrunning_total: 4.7142857\ncontestant: BeBe Zahara Benet\ncontestant: BeBe Zahara Benet\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"BeBe Zahara Benet\",\"legendgroup\":\"BeBe Zahara Benet\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"frame\":null},{\"x\":[2,3,4,5,6,7,8,9,10,11,12,14],\"y\":[0.5,0.75,1,1.33333333333333,1.66666666666667,2,2.5,2.75,3.25,3.58333333333333,4.08333333333333,4.91666666666667],\"text\":[\"episode: 2\nrunning_total: 0.5000000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 3\nrunning_total: 0.7500000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 4\nrunning_total: 1.0000000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 5\nrunning_total: 1.3333333\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 6\nrunning_total: 1.6666667\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 7\nrunning_total: 2.0000000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 8\nrunning_total: 2.5000000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 9\nrunning_total: 2.7500000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 10\nrunning_total: 3.2500000\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 11\nrunning_total: 3.5833333\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 12\nrunning_total: 4.0833333\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\",\"episode: 14\nrunning_total: 4.9166667\ncontestant: Bianca Del Rio\ncontestant: Bianca Del Rio\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(216,144,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Bianca Del Rio\",\"legendgroup\":\"Bianca Del Rio\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(216,144,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(216,144,0,1)\"}},\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3,0.6,1.2,1.5,2.1,2.6,3.2,3.3,3.9,4.9],\"text\":[\"episode: 1\nrunning_total: 0.3000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 2\nrunning_total: 0.6000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 3\nrunning_total: 1.2000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 4\nrunning_total: 1.5000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 5\nrunning_total: 2.1000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 6\nrunning_total: 2.6000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 7\nrunning_total: 3.2000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 8\nrunning_total: 3.3000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 9\nrunning_total: 3.9000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\",\"episode: 10\nrunning_total: 4.9000000\ncontestant: Bob the Drag Queen\ncontestant: Bob the Drag Queen\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Bob the Drag Queen\",\"legendgroup\":\"Bob the Drag Queen\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(163,165,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\"}},\"frame\":null},{\"x\":[1,2,3,4],\"y\":[0.75,1.5,2.25,2.25],\"text\":[\"episode: 1\nrunning_total: 0.7500000\ncontestant: Honey Mahogany\ncontestant: Honey Mahogany\",\"episode: 2\nrunning_total: 1.5000000\ncontestant: Honey Mahogany\ncontestant: Honey Mahogany\",\"episode: 3\nrunning_total: 2.2500000\ncontestant: Honey Mahogany\ncontestant: Honey Mahogany\",\"episode: 4\nrunning_total: 2.2500000\ncontestant: Honey Mahogany\ncontestant: Honey Mahogany\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(57,182,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Honey Mahogany\",\"legendgroup\":\"Honey Mahogany\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(57,182,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(57,182,0,1)\"}},\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11],\"y\":[0.272727272727273,0.363636363636364,0.636363636363636,1,1.45454545454545,2,2.36363636363636,2.72727272727273,3.09090909090909,3.63636363636364,3.63636363636364],\"text\":[\"episode: 1\nrunning_total: 0.2727273\ncontestant: Katya\ncontestant: Katya\",\"episode: 2\nrunning_total: 0.3636364\ncontestant: Katya\ncontestant: Katya\",\"episode: 3\nrunning_total: 0.6363636\ncontestant: Katya\ncontestant: Katya\",\"episode: 4\nrunning_total: 1.0000000\ncontestant: Katya\ncontestant: Katya\",\"episode: 5\nrunning_total: 1.4545455\ncontestant: Katya\ncontestant: Katya\",\"episode: 6\nrunning_total: 2.0000000\ncontestant: Katya\ncontestant: Katya\",\"episode: 7\nrunning_total: 2.3636364\ncontestant: Katya\ncontestant: Katya\",\"episode: 8\nrunning_total: 2.7272727\ncontestant: Katya\ncontestant: Katya\",\"episode: 9\nrunning_total: 3.0909091\ncontestant: Katya\ncontestant: Katya\",\"episode: 10\nrunning_total: 3.6363636\ncontestant: Katya\ncontestant: Katya\",\"episode: 11\nrunning_total: 3.6363636\ncontestant: Katya\ncontestant: Katya\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Katya\",\"legendgroup\":\"Katya\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,191,125,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\"}},\"frame\":null},{\"x\":[2,3,4,5,6,7,8,9,10,11,12,13,15],\"y\":[0.307692307692308,0.538461538461539,0.769230769230769,1.23076923076923,1.46153846153846,1.76923076923077,1.84615384615385,2.23076923076923,2.61538461538462,2.92307692307692,3.38461538461539,3.84615384615385,4.15384615384615],\"text\":[\"episode: 2\nrunning_total: 0.3076923\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 3\nrunning_total: 0.5384615\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 4\nrunning_total: 0.7692308\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 5\nrunning_total: 1.2307692\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 6\nrunning_total: 1.4615385\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 7\nrunning_total: 1.7692308\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 8\nrunning_total: 1.8461538\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 9\nrunning_total: 2.2307692\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 10\nrunning_total: 2.6153846\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 11\nrunning_total: 2.9230769\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 12\nrunning_total: 3.3846154\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 13\nrunning_total: 3.8461538\ncontestant: Manila Luzon\ncontestant: Manila Luzon\",\"episode: 15\nrunning_total: 4.1538462\ncontestant: Manila Luzon\ncontestant: Manila Luzon\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,196,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Manila Luzon\",\"legendgroup\":\"Manila Luzon\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,191,196,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,196,1)\"}},\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"y\":[0.506742136696888,0.875243250067657,1.20979520167453,1.54123209175391,1.81812889312889,2.03740361440896,2.25182764053562,2.48490999388923,2.72337925987659,2.94145667167058,3.19773580612129,3.18517694426785,3.48878205128205,3.82099762982116,3.87179487179487],\"text\":[\"episode: 1\nrunning_total: 0.5067421\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 2\nrunning_total: 0.8752433\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 3\nrunning_total: 1.2097952\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 4\nrunning_total: 1.5412321\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 5\nrunning_total: 1.8181289\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 6\nrunning_total: 2.0374036\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 7\nrunning_total: 2.2518276\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 8\nrunning_total: 2.4849100\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 9\nrunning_total: 2.7233793\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 10\nrunning_total: 2.9414567\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 11\nrunning_total: 3.1977358\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 12\nrunning_total: 3.1851769\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 13\nrunning_total: 3.4887821\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 14\nrunning_total: 3.8209976\ncontestant: Mean Average\ncontestant: Mean Average\",\"episode: 15\nrunning_total: 3.8717949\ncontestant: Mean Average\ncontestant: Mean Average\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Mean Average\",\"legendgroup\":\"Mean Average\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,176,246,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\"}},\"frame\":null},{\"x\":[1,2,3,4,5,6,8],\"y\":[0.857142857142857,1.28571428571429,1.85714285714286,2.42857142857143,2.71428571428571,3.42857142857143,4.57142857142857],\"text\":[\"episode: 1\nrunning_total: 0.8571429\ncontestant: Nina Flowers\ncontestant: Nina Flowers\",\"episode: 2\nrunning_total: 1.2857143\ncontestant: Nina Flowers\ncontestant: Nina Flowers\",\"episode: 3\nrunning_total: 1.8571429\ncontestant: Nina Flowers\ncontestant: Nina Flowers\",\"episode: 4\nrunning_total: 2.4285714\ncontestant: Nina Flowers\ncontestant: Nina Flowers\",\"episode: 5\nrunning_total: 2.7142857\ncontestant: Nina Flowers\ncontestant: Nina Flowers\",\"episode: 6\nrunning_total: 3.4285714\ncontestant: Nina Flowers\ncontestant: Nina Flowers\",\"episode: 8\nrunning_total: 4.5714286\ncontestant: Nina Flowers\ncontestant: Nina Flowers\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(149,144,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Nina Flowers\",\"legendgroup\":\"Nina Flowers\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(149,144,255,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(149,144,255,1)\"}},\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,11],\"y\":[0.4,0.7,1.3,1.6,2.2,2.4,2.7,3.1,3.7,4.7],\"text\":[\"episode: 1\nrunning_total: 0.4000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 2\nrunning_total: 0.7000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 3\nrunning_total: 1.3000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 4\nrunning_total: 1.6000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 5\nrunning_total: 2.2000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 6\nrunning_total: 2.4000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 7\nrunning_total: 2.7000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 8\nrunning_total: 3.1000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 9\nrunning_total: 3.7000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\",\"episode: 11\nrunning_total: 4.7000000\ncontestant: Tyra Sanchez\ncontestant: Tyra Sanchez\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Tyra Sanchez\",\"legendgroup\":\"Tyra Sanchez\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(231,107,243,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\"}},\"frame\":null},{\"x\":[1,3],\"y\":[0.5,0.5],\"text\":[\"episode: 1\nrunning_total: 0.5000000\ncontestant: Vivacious\ncontestant: Vivacious\",\"episode: 3\nrunning_total: 0.5000000\ncontestant: Vivacious\ncontestant: Vivacious\"],\"type\":\"scatter\",\"mode\":\"lines+markers\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,98,188,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Vivacious\",\"legendgroup\":\"Vivacious\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(255,98,188,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,98,188,1)\"}},\"frame\":null}],\"layout\":{\"margin\":{\"t\":48.5446243254462,\"r\":7.30593607305936,\"b\":41.51100041511,\"l\":32.7438771274388},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(234,37,154,1)\",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\" Bianca Del Rio Is the Best Drag Race Contestant Of All Time \",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":22.3163138231631},\"x\":-0.1,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.3,15.7],\"tickmode\":\"array\",\"ticktext\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"categoryorder\":\"array\",\"categoryarray\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(190,190,190,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\" Episode \",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.040530303030303,5.14886363636364],\"tickmode\":\"array\",\"ticktext\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"tickvals\":[1,2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"1\",\"2\",\"3\",\"4\",\"5\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(190,190,190,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\" Running Outcome Score (Weighted) \",\"font\":{\"color\":\"rgba(255,255,255,1)\",\"family\":\"\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.958661417322835},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"4ad0bbe7721\":{\"x\":{},\"y\":{},\"colour\":{},\"type\":\"scatter\"},\"4ad03c021886\":{\"x\":{},\"y\":{},\"colour\":{}}},\"cur_data\":\"4ad0bbe7721\",\"visdat\":{\"4ad0bbe7721\":[\"function (y) \",\"x\"],\"4ad03c021886\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\rShowsthe running, weighted outcome scores of the top 5 contestants by cummulative, weighted outcome score (including Bianca Del Rio in light green) with a random sample of 5 other contestants.\r\rHover over individual data points to see individual contestants’ running, weighted outcome score by episode, labeled with contestant name.\r\r\r\r\rConclusion\r\rBianca Del Rio is the best drag race contestant of all time when cumulative weighted outcome scores of RuPaul’s Drag Race contestants are compared.\r\rData points for Bianca Del Rio are missing for episodes 1 because unlike other seasons, season 6 split the entrance of the queens between two episodes, and therefore Bianca entered in the second episode. A data point for Bianca Del Rio is also missing for episode 14 because this was a summary episode before the finale for season 6, which did not feature a competion.\rBianca Del Rio’s below average performance before episode 8 may be a result of the calculation of the weighted outcome scores. Early seasons of Ru Paul’s Drag Race had fewer than 14 episodes, as few as 8, therefore the cumulative weighted outcome scores of queens from early seasons will have augmented the average to be higher (closer to their cumulative weighted outcome scores) than the running, weighted scores of the queens who were featured in more episodes. When the data for these contestants no longer appears (episode 8, onwards), Bianca Del Rio’s running, weighted scores show that she preformed better than others who also featured in up to 14 episodes and ultimately achieved the highest cumulative weighted outcome score of all the contestants.\r\r\r\r\r","date":1611014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611051170,"objectID":"b4086ffcc43455c89a80f8aac257db0f","permalink":"/post/drag-racer/","publishdate":"2021-01-19T00:00:00Z","relpermalink":"/post/drag-racer/","section":"post","summary":"This is the winning entry to our Drag Racer competition by UofG psychology student Ellie Brownlie.\nIntro\rSee final section, Beauty fades, data vis is forever!","tags":[],"title":"Drag Racer","type":"post"},{"authors":[],"categories":["talks"],"content":"\rThis website accompanies the Psychology of Swearing talk held on Tuesday 10th November 2020 for the Universit of Glasgow Psychology Society.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:\nBooks\n\rBenjamin Bergen – What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves. Amazon link.\rSteven Pinker - The Stuff of Thought. Amazon link.\rEmma Byrne - Swearing is Good For You. Amazon link.\r\rWebsites\nhttps://stronglang.wordpress.com/\nhttps://twitter.com/stronglang\nJournal papers\nFeldman, G., Lian, H., Kosinski, M., \u0026amp; Stillwell, D. (2017) Frankly, we do give a damn: The relationship between profanity and honesty. Social Psychological and Personality Science\nHarris, C. L., Aycicegi, A., \u0026amp; Gleason, J. B. (2003). Taboo words and reprimands elicit greater autonomic reactivity in a first language than in a second language. Applied Psycholinguistics, 24(04), 561-579.\nJay, T. (2009). The utility and ubiquity of taboo words. Perspectives on Psychological Science, 4(2), 153-161.\nJay, T., \u0026amp; Janschewitz, K. (2008). The pragmatics of swearing. Journal of Politeness Research. Language, Behaviour, Culture, 4(2), 267-288.\nJay, K. L., \u0026amp; Jay, T. B. (2013). A child’s garden of curses: A gender, historical, and age-related evaluation of the taboo lexicon. The American Journal of Psychology, 126(4), 459-475.\nJay, K. L., \u0026amp; Jay, T. B. (2015). Taboo word fluency and knowledge of slurs and general pejoratives: deconstructing the poverty-of-vocabulary myth. Language Sciences, 52, 251-259.\nMackay, D. G., Shafto, M., Taylor, J. K., Marian, D. E., Abrams, L., \u0026amp; Dyer, J. R. (2004). Relations between emotion, memory, and attention: Evidence from taboo Stroop, lexical decision, and immediate memory tasks. Memory \u0026amp; Cognition, 32(3), 474-488.\nStephens, R., Atkins, J., \u0026amp; Kingston, A. (2009). Swearing as a response to pain. Neuroreport, 20(12), 1056-1060.\nVan Lancker, D., \u0026amp; Cummings, J. L. (1999). Expletives: Neurolinguistic and neurobehavioral perspectives on swearing. Brain research reviews, 31(1), 83-104.\n\rRelationship between age and vocabulary\rThe correlation between age and animals is r = -0.14, p = 0.43.\nThe correlation between age and swears is r = -0.13, p = 0.43.\nThe correlation between swears and animals is r = 0.47, p = 0.\n\rVocabulary scores by gender\rGeneral fluency\r\r\rgender\raverage_score\r\r\r\rMan\r15.20\r\rWoman\r18.21\r\rNonbinary\r22.00\r\rOther\r15.00\r\r\r\r\n\rSwearing\r\r\rgender\raverage_score\r\r\r\rMan\r9.60\r\rWoman\r9.56\r\rNonbinary\r15.50\r\rOther\r9.00\r\r\r\r\n\r\rMost frequent swear words\r\rWord cloud\r\rMost frequent words by gender\r\rWord cloud - men\r\rWord cloud - women\r\rWord cloud - non-binary\r\r","date":1604707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604744745,"objectID":"75756fa96ec3480231b390f2c4a5b02b","permalink":"/post/psychology-of-swearing/","publishdate":"2020-11-07T00:00:00Z","relpermalink":"/post/psychology-of-swearing/","section":"post","summary":"This website accompanies the Psychology of Swearing talk held on Tuesday 10th November 2020 for the Universit of Glasgow Psychology Society.\nReading list\rIf you would like more information about anything presented in this talk, I recommend the following sources:","tags":["swearing"],"title":"Psychology of Swearing","type":"post"},{"authors":[],"categories":["Pivot"],"content":"\rLast Saturday we were supposed to be in London for the West End production of Sister Act, with Whoopi Goldberg reprising the lead role and Jennifer Saunders as Mother Superior. I genuinely love Jennifer Saunders more than I love my own mother, and my wife, who is a choral conductor1, counts Sister Act amongst her favourite films. Just after the scene where Mary Clarence leads the choir for the first time, I looked over and realised Kathleen was crying. Now, this isn’t unusual. We discuss, feel, and express our emotions as freely, frequently, and strongly as is dictated by the Lesbian Code of Conduct so we cry at the TV most days. What was unusual is that we normally cry in sync2 and I was still dry-eyed. We talked about it afterwards and she said that it normally makes her cry anyway because it’s about community choirs being good for people and conductors being good leaders and that’s her thing but that this time it had packed an extra punch because it was a reminder of how much she misses being in the room with her choir. And that, dear reader, made me cry at the time and again whilst typing this because I viscerally get what she’s feeling because I feel the same grief about my own teaching.\nI have two firm beliefs about 2020-2021\nOnline learning is not only educationally powerful and effective but it can provide a full, rich, positive student experience in which meaningful lasting connections can be made.\rI am going to feel a deep personal loss for as long as this goes on.\r\rAnd this is very much about the personal. Pedagogically, now that I have hit my stride with designing the teaching materials, my course might be better than its ever been. The structure is better, the signposting is better, I’ve chucked out any resource that isn’t fully open-access, the synchronous sessions are entirely focused on active learning and group-work, and community is threaded through the core of our entire programme. If you’re a UofG student reading this, honestly, we’ve got your back and I’m so proud to be part of the team that’s pulling this off.\nBut at that personal level, being at the front of that lecture hall gives me life. I love my job. I love teaching. Lectures get a bad rap because they’re so easy to do badly but for me a great lecture is like nothing else for both sides of the podium. I need a break as much as anyone but I miss teaching during the summer. I’m going to miss seeing their faces when I tell them something that blows their minds. I’m going to miss the nervous laughter in lecture 1 of 1st year when they’re not sure if they’re allowed to laugh at my jokes turn into the confident laughter of a team that’s comfortable with each other. I’m going to miss when they desperately try to hide that they’re hungover in the labs. I’m going to miss overhearing their patter about their weekends. I’m going to miss the look of the gaybys when they realise she’s one of us.\nIt’s personal because it’s got nothing to do with the quality of education we’re providing or the experience the students themselves will have. All of this will still happen. Minds will be blown, jokes will be laughed at, hangovers will be hidden, gaybys will be represented. But I’m not going to be in the room to see it and feel it with whatever magic of our chemistry it is that makes being in the same room with other humans feel good.\nI have two firm beliefs about how I/we handle this grief (and this is where I imagine some of you reading this will disagree):\nIt’s important that we allow ourselves and others on our teams to feel and express this grief. Some people won’t feel it, for some, not being in a lecture hall is going to be a relief, but for those who do, recognise that it is separate to how hard they’re working to pivot their teaching online and to their beliefs about the efficacy of online learning.\n\rWe should be mindful that our grief does not frame our interactions with students and that it is not part of the narrative of 2020-2021. Yes, they’re adults blah blah blah. But they’re adults who are dealing with their own stuff and even in the most egalitarian classroom there’s still a power differential between students and teachers that means it’s problematic for them to be aware of my personal feelings. I don’t want them to feel like they can’t come to me with problems for fear of upsetting me, that’s not my role in their lives.\n\r\rAt the crux of this is that as I’ve said, their education isn’t going to suffer, and by mixing our entirely valid grief in with the pedagogical narrative, we risk that narrative becoming one of deficit, which does a disservice to the massive amount of work being put in to create engaging and effective online courses. They’re not there for my entertainment and my feels, I’m there to teach them. And I am going to teach them well. I am also going to drink wine and cry to my friends and my wife occasionally, and that’s also fine.\n\r1 Kathleen is the Musical Director of Loud and Proud, Scotland’s LGBT choir based in Edinburgh. They’re great, she’s great, you should check them out.\n2 Being a lesbian is hilarious\n","date":1598486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598551692,"objectID":"bc53dd88cba2ffb0f83f90ac9a15302f","permalink":"/post/pivot-7-grief/","publishdate":"2020-08-27T00:00:00Z","relpermalink":"/post/pivot-7-grief/","section":"post","summary":"Last Saturday we were supposed to be in London for the West End production of Sister Act, with Whoopi Goldberg reprising the lead role and Jennifer Saunders as Mother Superior.","tags":["pivot"],"title":"Pivot 7: Grief","type":"post"},{"authors":[],"categories":["Pivot"],"content":"\rThis week I continued recording my lectures and asynchronous lab videos. As I’ve already said on Twitter, I largely refuse to do any editing - there were a couple of moments where the cat made a noise in the background, the door bell went off, or I stumbled over words and there’s no way I am spending my time editing them out. Our face-to-face lectures weren’t perfect, we made errors, had tech issues, had students walk in late, or building work noise in the background. I’m not saying that there’s not a minimum level of polish we should be aiming for, but I know that a lot of people worry that recordings need to have high production values and I don’t think we can say it enough enough that that isn’t the case.\nAbove anything else, I hope that this whole mess has shown us what the value of education is and why people come to university. It isn’t the content, if they wanted content the internet is full of it. They want connection, engagement, community. They want advice and guidance and understanding. That’s what is going to make or break 2020-2021, not whether you forgot what you were saying for 10 seconds in a lecture. I know there’s been some ridiculous articles floating around Twitter about making sure your Zoom background is professional and videos are up to spec and I won’t give them further airtime by linking to them but honestly. It’s an entirely positive thing for our students to realise that we are human after all. Higher education isn’t going to be made any worse for an extra shot of empathy. Whilst ensuring high educational standards, I want them to see me fluff my words occasionally so that they know when they do a presentation it’s ok not to be perfect, I want the cat to interrupt when they’re in a meeting with me so that they understand that if the reverse happens and their child/partner/pet barges in, I’m not going to care. Yes it’s all relative and this is by no means me arguing for a hash job - I am working incredibly hard to make sure everything is in place and has been thought through - but why anyone would seek to strip online teaching of its humanity is beyond me.\nThere are of course some exceptions. As I write this, I’ve just discovered that in the Course Overview video I recorded earlier in the week, I’ve got the topics of the lectures the wrong way round, I’ve said mental health is on Tuesday and developmental is on Wednesday and it’s the other way. In the grand scheme of things this is a tiny mistake, and one I could maybe rectify by adding a note. But, I also know from many years of experience that new 1st year students pick up on any inconsistency and panic about it. So, given that it’s the very first video I’m going to ask them to watch, I’m going to re-record it. It will take me half an hour, but first impressions matter.\nThis week I also had an information session with the new 1st year students and it was so great to meet them all. What was reassuring for me was that their questions and concerns were the same as they’ve always been - what optional courses should they take, what’s the difference between BSc and MA psychology, should they do any pre-reading. I can’t imagine that they’re not worried about learning online or what university in a pandemic is going to be like, but they’re certainly taking it in their stride at the moment. Also, for anyone who is working at an institution where f2f teaching is required, we’ve had absolutely no blowback about our plans that all core teaching will be online.\nIn other musings, I am trying to keep the number of tech platforms we use to a minimum (Moodle, Zoom, Teams) but I have started playing around with Gather Town to facilitate social interaction without overloading on huge Zoom rooms. I may attempt to do something for the induction lecture whereby we do 15 minutes on Zoom where I present information etc. and then invite everyone to go to Gather Town so that they can chat to people in small groups. It’s very easy to use and it doesn’t require them to create an account etc., which always helps.\nAt this point in the pivot, my main advice is to have a weekly team meeting. We set up a 30-minute Zoom with the level 1 teaching team a month or so back and it’s proved incredibly helpful. As course lead it gives me a nice way of checking where we’re all at which is calming for my anxiety but it’s also good just to talk through what we’ve been doing. There’s been a few things that people have said off the cuff that have resulted in a larger conversation, a better solution, or realising we need some extra training. I am what I would charitably describe as a control enthusiast and I do think that when you’re facing something that is as complex as pivoting first year online, there’s a benefit to having one person tying everything together so that it’s cohesivee. However, it’s also very helpful and reassuring to have my ideas and decisions questioned and examined at every turn by the people they are going to affect, and to have a set time and place for them to think about that stuff because they’re all juggling multiple other roles.\nMy goal was to have the first 5 weeks of content sorted by the end of August so that I could take a few days of annual leave before diving back in. I think I’m on track? Honestly I’m not sure, it feels like a mirage at times, like I can only see what September is going to be like out the corner of my eye and if I look directly at it, it disappears. Is it wine o’clock yet?\n","date":1597968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598009601,"objectID":"7cdddda97bb6e60252e6ac582d6a006c","permalink":"/post/pivot-6/","publishdate":"2020-08-21T00:00:00Z","relpermalink":"/post/pivot-6/","section":"post","summary":"This week I continued recording my lectures and asynchronous lab videos. As I’ve already said on Twitter, I largely refuse to do any editing - there were a couple of moments where the cat made a noise in the background, the door bell went off, or I stumbled over words and there’s no way I am spending my time editing them out.","tags":["pivot"],"title":"Pivot 6: Human after all","type":"post"},{"authors":[],"categories":["Pivot"],"content":"\rThe first thing I did this week was to rewrite a lecture on the history of psychology, not because of anything to do with the pivot, but rather following the recent Black Lives Matter protests I realised that I had significant room for improvement in the way I was presenting the history of our field.\nIn pivot-related work, I started recording my lectures, or, I should say more accurately, I attempted to start recording my lectures. My first mistake was that I didn’t fully practice the lecture before I started recording. I have no idea why I did this. I always do a full run-through of every lecture I give, usually the night before, I can’t explain why I thought I didn’t need to do this when recording the lectures. I lecture from a script but I have always lectured from a script* so that’s no excuse, maybe the lack of a live audience gave me a false sense of security.\nIn a nutshell, it was a bit shite and I need to do it again. But, it was a good reminder to try and invoke as many of the cues I would normally have for a f2f lecture to give me the same performance level - for me this is doing a practice run-through, standing up, and putting on a shirt and tie and living my executive lesbian realness fantasy,\nWe also discovered a flaw in our plan to use Echo360’s Universal Capture to record the lectures. Whilst UC shows both the slides and your face if you’re streaming the video, you can only download either slides \u0026amp; audio OR face \u0026amp; audio. This is an issue because we think it’s really important for engagement particularly at level 1 that they can see our faces but also to reduce their data usage we want them to be able to download the lectures rather than needing to always be online (this also helps us with the watch parties in which we need to stream a video via Zoom and this will be better for bandwidth if it’s a local file). I think this is a real shame because the transcription editor for Echo is SO much better than anything else and it integrates with Moodle very well but, particularly for Level 1, engagement is the name of the game so we’re switching back to recording on Zoom.\nThe main thing I’ve worked on this week though is setting up the lab activities and Moodle for both Level 1 and my MSc course for the first two weeks of the semester. I’ve worked through all of the slides, asynchronous activities, and Sway tutorials although I haven’t recorded anything yet which has nothing to do with Echo360 and everything to do with not mustering the drive to get dressed and do my make-up.\nI’ve also started thinking about what kinds of social activities we can do. I don’t want it to be overkill (for both staff and students) so I thought perhaps something week 2, 6, and 10. Week 2 I’m thinking about doing a staff-student quiz for the entire school and in week 6 a Netflix party with a psychology-related film, again, with the entire school invited. I haven’t quite figured out what to do in week 10 aside from likely cry into my wine but hopefully something will occur.\nFinally, I want to end on a positive reflection that came out of talking with my Director of Teaching Niamh Stack. Every year we get nervous. Every year the students are nervous. Every year something goes wrong - we stumble our words in a lecture, a video doesn’t play, there’s noisy building work next door. The mistakes will look different this year but we shouldn’t forget that they’re always there and that generally speaking, the students don’t care about them as long as they see that we’re trying our best. It’s going to be so easy to focus on what goes wrong this year, remember to be kind to yourself.\n* Every time I admit that I lecture from a script someone likes to tell me that I shouldn’t and I will sound stilted and it will affect engagement. If you are considering giving me this advice let me save us both some time - I don’t sound stilted, my lectures are great, you do you and I’ll do me!\n","date":1597363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597395360,"objectID":"5ae6436323758bbdb3b09efc5a6104a3","permalink":"/post/pivot-5/","publishdate":"2020-08-14T00:00:00Z","relpermalink":"/post/pivot-5/","section":"post","summary":"The first thing I did this week was to rewrite a lecture on the history of psychology, not because of anything to do with the pivot, but rather following the recent Black Lives Matter protests I realised that I had significant room for improvement in the way I was presenting the history of our field.","tags":["pivot"],"title":"Pivot 5: Lights, Camera, Inaction","type":"post"},{"authors":[],"categories":[],"content":"\rList of institutions linking to lecture capture guides from Nordmann et al. (2020).\n\rUniversity of Aberystwyth, UK\rUniversity of Auckland, New Zealand\rUniversity of Birmingham\rUniversity of Cape Town South Africa\rUniversity of Cambridge\rCity University London, UK\rUniversity of Derby\rUniversity of Edinburgh, UK\rHull York Medical School UK\rUniversity of Glasgow (external link pending)\rImperial College London, UK\rRoyal Holloway, University of London, UK\rLondon School of Economics, UK\rUniversity of Sheffield\r\r","date":1597104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597156130,"objectID":"a59c08071a558ae6a779eb1f903a9d9f","permalink":"/post/lec-rec-impact/","publishdate":"2020-08-11T00:00:00Z","relpermalink":"/post/lec-rec-impact/","section":"post","summary":"\rList of institutions linking to lecture capture guides from Nordmann et al. (2020).\n\rUniversity of Aberystwyth, UK\rUniversity of Auckland, New Zealand\rUniversity of Birmingham\rUniversity of Cape Town South Africa\rUniversity of Cambridge\rCity University London, UK\rUniversity of Derby\rUniversity of Edinburgh, UK\rHull York Medical School UK\rUniversity of Glasgow (external link pending)\rImperial College London, UK\rRoyal Holloway, University of London, UK\rLondon School of Economics, UK\rUniversity of Sheffield\r\r","tags":[],"title":"Lec Rec impact","type":"post"},{"authors":null,"categories":null,"content":"This is an update of my How to Study lecture that has been adapted for the pivot to online learning. The changes include:\n Changing the section on attendance to keeping up Adding a section about technology and environment Adding a slide on time management Adding info about open-book exams  You are free to reuse, remix and adapt these slides without attribution, however, because even teaching-focused academics need to worry about impact, please let me know via e-mail or Twitter if you do!\n  Download slides  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596758400,"objectID":"989e6330aa1d7eb9b15ef6b69a420065","permalink":"/project/how_to_study-_2020/","publishdate":"2020-08-07T00:00:00Z","relpermalink":"/project/how_to_study-_2020/","section":"project","summary":"Introductory lecture for first year students explaining effective study strategies adapated for online learning","tags":["study strategies","other"],"title":"How to Study 2020","type":"project"},{"authors":[],"categories":["Pivot"],"content":"\rIt’s now clear that we’ll be running our small-group teaching remotely for the first semester in addition to the lectures (there will be additional f2f sessions, but core teaching will be online and I’ll talk about that in a later post). Broadly speaking the work they do in the labs in the first semester is either focused on academic writing (learning how to find and use evidence, scientific writing, researching their essay) and data skills (learning to wrangle and visualise data in R).\nIn the old normal they would have a two-hour lab once every two weeks. In the new normal, they will have a 30-minute Zoom tutorial every week in addition to asynchronous activities. In the old normal, the labs would consist of some didactic content (the tutor presenting a pre-prepared presentation), individual and group activities, and live-coding of R. In the new normal, the didactic sections of the labs will be pre-recorded and made available asynchronously and the individual activities have also been converted. I am really loving Microsoft Sway for this because I can combine everything into one lesson that contains the video, any written information, and embed Forms and other activities so everything is in the one place. The synchronous Zoom sessions will be devoted to group-work and the live-coding will be replaced by walkthrough videos.\nImportantly, rather than just me recording the videos for the asynchronous lab content and walkthroughs, we’re going for a “team of credible experts” approach and you can expect the slogan t-shirts to be back from the printers any day now. In a nutshell, there are four lab tutors who also lecture and we’re going to divvy up the videos between us so that all students get to know all of us. We’re hoping that this will help create a sense of community - there’s enough of us to make it feel like a team, but not too many that we’re just yet another face they have to try and learn. Also from a pedagogical perspective, we think it will be nice for them to see how each of us has a slightly different approach to programming.\nAt the end of last week I made a roadmap for both Level 1 and my MSc courses for the entire first semester. For each week I’ve mapped out the synchronous and asynchronous activities, assessment deadlines, and importantly, have given an estimate of how long I think each task will take students to complete so that I don’t accidentally overload them.\nWhen writing the roadmap I tried to keep the student’s weekly checklist in mind, and break their lab work down into admin tasks, academic writing tasks and data skills tasks. I’m not sure that the estimated timings are perfect, however, I want to model what successful time-management looks like as much as I can. I will take away these estimates in the second semester so that they progress in planning their own time, but I think this will help whilst they’re getting used to everything.\nI also took my own advice from our 10 Simple Rules paper and added a signposting section on Technology and Communication to Moodle. They’re likely going to have slightly different tech set-ups on each of their courses, so I want it to be clear what and how we’re going to communicate with them for psychology.\nThe big thing this week was that I started properly working through my teaching materials this week. My original plan was to do everything on assessments, then the lectures, then the labs, however, I’ve decided that I’m going to do complete two week blocks of content (lectures \u0026amp; labs) and then switch to my other course and so on so that I can slowly start pushing back the point at which disaster might strike. My current aim is to have the first 5 weeks of term finished by the end of August at which point I will take a week of annual leave, and then when I come back I’ll try and work through the rest before teaching starts on the 21st. I don’t need to have everything done before teaching and I have told everyone on my course that they only need to have things done 2 weeks in advance, however, as course lead I want to give myself the space to firefight and react if possible. And if everything goes smoothly I’m not mad at the idea of being able to relax.\nFor for level 1, I’ve got the slides and materials for the first two labs and the first lecture. Converting the labs to a) weekly and b) online has involved identifying which bits of the lab were didactic, individual, and group activities and determining what can be made asynchronous and what should be done live. For the Zoom tutorial, I’m conscious that everything tends to take longer online, so I have settled on a structure whereby 10-minutes of the tutorial will be devoted to Q\u0026amp;A and students will have a weekly form they can ask questions on to help facilitate this, and then one group activity.\nFor week 1, the Zoom tutorial is a getting-to-know-you style affair and making sure everyone knows how to use all the tech. I don’t want to put either the tutors or the students under the stress of trying to do a formal activity in the first week.\nWe also do some team bonding exercises.\nTheir asynchronous activities are a short video on types of evidence and they have several activities to work through, but there’s no reason that should be in the Zoom session. It works better asynchronously and I’d rather have the live sessions for community building.\nThe activity for week 2 is also relatively light touch. They will be asked to do our reproducibility task individually, then we’ll put them into Breakout Rooms where they compare their efforts with their team and decide on a final version. This is actually a really important activity for them to understand why we teach them reproducible data skills but it’s also fun, gets them to do something with their team, and they have to share their images using e.g., Teams, so it’s a low-stakes way of ensuring that they really are ok with the technology. From week 3 once everyone is settled in, the activities will get more academic.\nFinally, I redesigned my first lecture. The lecture was on How to Study, so the content needed slightly adjusted for online learning (you can see the slides here) but it also needed reformatted for the watch party approach we’re taking. Thankfully it was a short-ish lecture anyway so I didn’t need to cut anything to make it fit into 40 minutes (it’ll be about 30 mins didactic split across two videos) but I have converted a few activities I would do in the f2f lecture for the interactive bits. Normally I would ask them what learning style they think they are by putting their hands up (before telling them that learning styles are a myth), this time they will fill in a quick Microsoft Form at the start of the lecture about their beliefs about learning and studying and I will use the answers to this in the break as a discussion.\nI think that’s it. I flip-flop daily between feeling quite good about the amount of work I’ve done and then sheer terror at what’s still left to do, but actually getting started on slides etc. feels tangible and calming. In addition to the haircut I also got myself a ring light so next week I will do some recording and finish off week 2 of Level 1, and then switch to do the same for my Msc course.\n","date":1596758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596646114,"objectID":"28d5054b707e6d485c6461511824eefa","permalink":"/post/pivot-4/","publishdate":"2020-08-07T00:00:00Z","relpermalink":"/post/pivot-4/","section":"post","summary":"It’s now clear that we’ll be running our small-group teaching remotely for the first semester in addition to the lectures (there will be additional f2f sessions, but core teaching will be online and I’ll talk about that in a later post).","tags":["pivot"],"title":"Pivot 4: Hitting the road","type":"post"},{"authors":[],"categories":["Pivot"],"content":"\rThis week I finished working through the assessment information for both my Level 1 course and my MSc Research Methods courses. The latter was fairly quick as that course was already delivered both online and offline and the assessments (R worksheets, pre-registration, quant report write-up) lend themselves easily to online assessment.\nThe only thing really of note for the interested pivot reader from this work was the update to the MCQ assessments for Level 1. Last year they were given three small-stakes MCQ assessments (each worth 2% of their grade) throughout the semester, to encourage them to study and keep up with the lecture content (as well as retrieval practice being good for learning). This year we’ve added in PeerWise*, where they are also required to write one MCQ of their own. As I’m sure you’ve experienced, writing good MCQs is no easy task and to write challenging distractors requires you to understand the question you’ve set fully, so it’s a great learning activity and it also means they build up a huge bank of MCQs to revise from come exam time.\nThe decision to introduce this was nothing to do with the pivot, the School of Biology used it previously and it was extremely popular and requested by our class reps. The reason it’s in this blog is because if you’re looking for an activity to promote engagement through the semester, it’s a great choice. If you’d like more info, see our assessment information and support.\n\rWhen I went to retrieve the link for PeerWise it was currently offline, here’s hoping it comes back!\r\rWatch party\rThe most calming thing I did this week was to do a test run of how we intend on running the lectures. Essentially, we’re going to try something akin to a Netflix Party. The lectures will be pre-recorded and chunked into ~15 minute videos, with a max of 40 minutes of didactic pre-recorded content. However, at the usual lecture time, we’re going to have a watch party whereby there will be a Zoom call and everyone will watch the chunks together. In-between the chunks, the lecturer will do interactive activities with the class - polls, discussion, Q\u0026amp;A. The type of interaction with depend on the size and the level of the class but hopefully you get the idea.\nThe rationale behind this approach was to try and balance staff and student workload and to balance what we know works for online with concerns about student engagement. The lectures are pre-recorded to allow for flexible asynchronous engagement, however, there’s also a synchronous event to promote engagement with the lecturer and to help students form a routine which I am particularly concerned about with new 1st years. The 15-minute chunks are better for online but the total run-time of 40 minutes is more realistic for workload than asking everyone to completely rewrite and restructure their lectures. Instead, the workload is channelled towards increasing engagement and active learning.\nThe test run was really to test the tech and whether video sharing over Zoom would work. I tried several different methods, either streaming my video or playing a local copy and all of them worked fine. This is of course going to be hugely dependent on staff broadband speeds and bandwidth so I’ve come up with high and low bandwidth solutions.\nI am 100% aware that this approach doesn’t work for everyone. Some students may not have the bandwidth (although we’re going to offer study space and access to tech to try and address this), some may not be able to make the synchronous session. That’s why the lectures are provided asynchronously as well, why the watch party will be recorded, why there’s also going to be a Teams channel where the lecturer will engage with the class. I don’t believe that perfect is possible (with anything but particularly with this) and I think we should focus on the fact that the old normal was incredibly far from inclusive and flexible and it’s huge progress for HE that inclusivity is being considered at scale for the first time. If anyone does have a solution that works for absolutely every student, I’ll buy it from you for 3 magic beans.\n\rWhat’s in a name?\rThe final thing I did this week was to start to edit our data skills books for teaching R. The old Level 1 book was organised by Lab (Lab 1, Lab 2, Lab 3 etc.) and the sub-chapter headings were pre-class activities and in-class activities. In the old normal they had a 2 hour lab every two weeks, in the new normal, a lot of it will be asynchronous but they will have a 30-minute synchronous session every week.\nWhat this means for my book is that I have had to rip it apart and put it back together again. I’ve removed any reference to pre-class and in-class and I’ve renamed the chapters to reflect the content (e.g., Loading data) rather than location they would be completed in. Why I didn’t do this originally I’m not sure. Importantly, the changes I’ve made have been modality neutral so that I don’t have to go back next year and change it again, e.g., “in this pre-class activity you will…” has become “in this chapter you will…” so it works regardless of how it is delivered.\nI also need to do this for the MSc data skills book - at the moment there is a f2f and an ODL version but I will combine them into one modality-neutral copy. Again, why I didn’t do this before now is a mystery.\nThe next step with our data skills work is updating the instructions for how to access R. Previously students would have used the on-campus computers or installed on their laptops. To avoid any issues with installation and system requirements, we’re hopefully getting access to [RStudio Pro]https://rstudio.com/products/rstudio-server-pro/ which runs through a browser. This is a great move, although it does mean updating a lot of set-up instructions.\nFinally, I also got a haircut, which means I can actually start recording, although I’m still a week or so from having anything to say.\n\r","date":1596153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596183059,"objectID":"bbdaa2a212be8900ff46be6b4692da77","permalink":"/post/pivot-3/","publishdate":"2020-07-31T00:00:00Z","relpermalink":"/post/pivot-3/","section":"post","summary":"This week I finished working through the assessment information for both my Level 1 course and my MSc Research Methods courses. The latter was fairly quick as that course was already delivered both online and offline and the assessments (R worksheets, pre-registration, quant report write-up) lend themselves easily to online assessment.","tags":["pivot"],"title":"Pivot 3: Party","type":"post"},{"authors":[],"categories":["Pivot"],"content":" The problem with starting your first blog with a pithy quote is that you feel pressured to do it for every one.\n I thought I was going to have to start this blog by admitting that I hadn\u0026rsquo;t actually done much pivot work this week - I spent the first half of the week working on assessment stuff that has nothing to do with the change in delivery and is just part of the usual yearly updates and tweaks. And then I realised that it\u0026rsquo;s completely the wrong way to look at it.\nIn our 10 Simple Rules* paper, we make the point (helpfully suggested by Simon Horrocks) that For courses that already involve substantial use of the VLE and online content, it can be helpful to highlight that the pivot may be more accurately described as blended to online, rather than offline to online.\nAcademic, heal thyself.\nThis week I updated the essay question and guidance** for the first year coursework essay and that\u0026rsquo;s just as much a part of the pivot as designing and recording asynchronous lab activities will be. This week was a good reminder that whilst a part of what I need to do is completely new and terrifying, the majority is the same work, or an adaptation of the same work I would usually do in the summer.\nGroup presentation, but make it COVID One of the few assessments we have that needed more than minor tweaks was a group presentation that would normally be given in the final lab of the semester. The topic is \u0026ldquo;What three things would you tell yourself as a new level 1 student if you could hit rewind?\u0026rdquo; and students are asked to reflect on their first semester at university and what they would do differently. The entire thing has to be evidence-based and they draw on what they\u0026rsquo;ve learned about things like memory, learning, sleep, mental health, and study strategies from the lectures in addition to doing their own research. It\u0026rsquo;s a wonderful lab for many reasons - the presentations are funny, insightful, heart-warming, heart-breaking, and academically rigorous. Given that their first semester is going to be very different to what they expected from university life, we are very keen to retain the topic to give them a chance to reflect on all they have achieved and experienced.\nThey will still work in groups and the topic will remain the same, but rather than a 10-minute presentation they will be given the choice between a 5-minute video, a blogpost, or a infographic poster. I stole this idea (and some of the learning outcomes and guidance) from my colleague Chiara Horlin\u0026rsquo;s Professional Skills course. This is a Level 3 science communication assessment so the scale and expectations are slightly different to my Level 1 adaptation but the core message here is to remember that there\u0026rsquo;s loads of good practice out there waiting to be appropriated. If you don\u0026rsquo;t know how to adapt your course or assessments, ask your colleagues, ask your educational development team, hell, ask on Twitter.\nFor this assessment I\u0026rsquo;ve given more guidance than I normally would, particularly when it comes to scaffolding the group-work because it is my firm belief that for 1st years this September, if it feels like spoon-feeding*** you\u0026rsquo;re probably doing it right. It\u0026rsquo;s still a bit of a work in-progress but the assessment information is here if you\u0026rsquo;d like to take a look.\nFrom a technological perspective I\u0026rsquo;m hyper-aware that I have to provide suggestions for collaborative and creative tools that they can access freely. Their Office 365 package solves a lot of this - OneDrive, Word, PowerPoint and Sway cover a lot of bases but I\u0026rsquo;ve also tried to find alternatives, particularly for video recording and editing (Zoom, Animaker, InShot) and infographics (Canva).\nStudent consultation This week I also hosted a consultation with teaching staff and GTAs where I invited 2019-2020 first year students to feedback on our plans for blended delivery for first year 2020-2021 given that they experienced the old-normal version of the course. Not too many turned up (to be fair to them, it\u0026rsquo;s July, they\u0026rsquo;re scattered all over the world, and their focus will be on what Level 2 will look like) but it was really useful to hear from those that did.\nThey were largely happy with our plans and were understanding of the constraints and uncertainty we\u0026rsquo;re working under, but the main thing I took away from the session is how important it was to be able to explain not only what we are planning, but why. It\u0026rsquo;s a level of detail regarding the underlying rationale and administration for teaching that we wouldn\u0026rsquo;t necessarily normally communicate**** but I think it was really helpful for them to understand the scope of everything we\u0026rsquo;ve had to consider when designing the course. We\u0026rsquo;ve recorded the session and I have sent it out in case those who couldn\u0026rsquo;t make it are interested.\nMy contact with incoming student is still relatively limited but we do have a Facebook group which will slowly fill up as more of them confirm their offers and finalise their decisions. They\u0026rsquo;ve already been provided with an information and FAQ pack but I\u0026rsquo;ve given them an anonymous form to ask any questions or write down anything they\u0026rsquo;re worried about with the view of doing a Q\u0026amp;A video in the next couple of weeks.\nNext week I will continue working on assessment information and specifications. For level 1 I need to finish a few details and then move on to doing the same for my MSc conversion Research Methods course. If I have time, I\u0026rsquo;ll also start to write the mother of all week-by-week roadmaps for all of the learning activities for both lecture and labs.\n* Yes, I am going to mention this paper in every blog because it\u0026rsquo;s great.\n** I think we\u0026rsquo;ve designed a great essay btw, you can check it out here.\n*** It is also my firm belief that what most of what is derogatively described as spoon-feeding is actually clearly signposted information that gives students the information they need to succeed without chance.\n**** We\u0026rsquo;re very open with our students and strongly believe in the power of co-creation, but room capacities and the definition of asynchronous and didactic teaching don\u0026rsquo;t often come up.\n","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595360301,"objectID":"46d988e6afb5ca37dae79890b138e530","permalink":"/post/pivot-2/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/post/pivot-2/","section":"post","summary":"The problem with starting your first blog with a pithy quote is that you feel pressured to do it for every one.\n I thought I was going to have to start this blog by admitting that I hadn\u0026rsquo;t actually done much pivot work this week - I spent the first half of the week working on assessment stuff that has nothing to do with the change in delivery and is just part of the usual yearly updates and tweaks.","tags":["pivot"],"title":"Pivot 2: Academic, heal thyself","type":"post"},{"authors":null,"categories":null,"content":"I like to think of myself as fairly good when it comes to inclusive practice, but the Black Lives Matter movement forced me to reflect on what I am doing beyond LGBTQ inclusivity and I came up short. In particular, I had to confront the fact that as the person who gives the history of psychology lecture to 1st years what I was giving them was largely the history of white Western men.\nI am still in the process of updating my lecture but I also wanted to rework the essay to give students the opportunity to take a more critical look at the history of psychology that is usually presented in class and in their textbook.\nHuge thanks to Chiara Horlin, Helena Paterson, and Wil Toivo from UofG Psychology for their input and advice on the questions and resources.\nThe full essay details are available below, but here\u0026rsquo;s the summary:\n The history of psychology is often presented as a list of facts, figures, and names; a chronological timeline of the progression of our field and the notable events and people that have shaped what we know as psychology in the present day. Such timelines can give the impression that the history presented is objective and apolitical, that it is merely the account of one thing after another as it happened.\n  However, no history of any field could ever be complete - even in a relatively young field such as psychology, there\u0026rsquo;s simply too much to cover. What makes history subjective, political, and so often biased, lies in the facts, figures, and names we choose to include, and those we let fall by the wayside.\n  The broad topic of your essay is the missing history of psychology. Your task is to read Chapter 1 of the textbook (which is also on the essential reading list for RM lecture 2) and reflect on what isn\u0026rsquo;t covered. You will choose one question focused on one of the following topics:\n   Race Gender identity and/or sexual orientation Women in psychology The replication crisis Psychology and government policy   You are free to reuse, remix and adapt these this work without attribution, however, because even teaching-focused academics need to worry about impact, please let me know via e-mail or Twitter if you do!\n  Essay information and resources  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1595289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595289600,"objectID":"ce64b37918cdfc293f539e2879e94840","permalink":"/project/missing_history/","publishdate":"2020-07-21T00:00:00Z","relpermalink":"/project/missing_history/","section":"project","summary":"Essay questions and resources for a more critical, inclusive look at the history of psychology","tags":["academic writing","other"],"title":"The Missing History of Psychology","type":"project"},{"authors":[],"categories":["Pivot"],"content":"\r\rGrant me the serenity to accept the things I cannot change, courage to change the things I can, and wisdom to know the difference - Reinhold Niebuhr\n\r\rMake a nice list, calm your pants where you can, and always have wine in the fridge - Emily Nordmann\n\rThis is the first in what I plan to be a weekly blog charting what I’ve been doing to redesign my courses for online/blended delivery. This isn’t going to be heavy on pedagogical arguments or citations, if you want that please see our preprint “10 Simple Rules for Supporting a Temporary online Pivot in Higher Education”. Instead, this is literally going to be a blog about what I’ve been doing - recording videos, updating assessment information, checking reading lists. I am a teaching-focused lecturer with experience of fully online delivery and whose research expertise is lecture capture and I still feel overwhelmed at the scale of the task in front of me so I figure that it might be helpful to others to read a concrete account of the nuts and bolts of what the pivot entails.\nThe course\rTo give you some context, I am Year Lead for 1st year psychology at the University of Glasgow. Typically there’s about 500 students on the course, with just under half of them registered on a psychology programme and half who take it as an elective.\nIn the old normal, they would have three hour-long lectures a week delivered by a team of lecturers on various fields of psychology. In addition to the lectures, there was a two-hour practical lab once every two weeks in which the focus was on small-group work academic writing, data skills, and programming using R. The assessment was a final exam (with an essay and MCQ component), a coursework essay, and a lab portfolio of small-stakes MCQs, R worksheets, and a group presentation.\n\rTeamwork makes the dreamwork\rThis is my blog and I will be writing about my experience but I want to make it clear up front that none of this is done alone. I’m lucky to have a fantastic team to work with at UofG Psychology and everything we’re doing is a result of extensive and ongoing team discussions and consultations with lecturers, tutors, GTAs and students.\n\rUncertainty\rLike most (all?) of us, I find the uncertainty about what life in September will look like and what will be possible (or sensible) difficult and stressful to process. It’s like when I think about the size of the universe and the scale of it all short-circuits my brain.\nHere’s the answer we’ve given to incoming and prospective students about what teaching will be like:\n\rLike all universities, face-to-face teaching in 2020-2021 will depend on social distancing guidelines that the Scottish government has set in place. Our promise to you is that we will give you as much face-to-face contact and teaching as is possible. However, because we don’t yet know what specific guidelines will still be in place in September, it’s difficult to tell you right now exactly what will be possible. We know that this uncertainty is frustrating, believe us, we miss being in the classroom with our students so much.\n\r\rDespite the uncertainty we want to give you as much concrete information as we can given what we know now, rather than promise things we may not be able to deliver. Level 1 Psychology lectures involve a lot of people in a confined space, so at this point it is very unlikely that lectures can take place in person for the first semester. As the situation evolves, and depending on guidelines, this may be required for the second semester as well. Instead, our lectures will be pre-recorded to allow you flexibility, but there will also be a live watch party (think Netflix Party but with less Tiger King) at the regular lecture time so you can make contact directly with your lecturers and peers, ask questions, take part in interactive activities, and importantly to help establish a routine.\n\r\rIt is more difficult to give you an exact plan for labs and small-group teaching. Given that lab class sizes are smaller than lectures, for the second semester, we’re optimistic that we will be able to deliver them in person as normal. However, for the first semester the exact details of the social distancing guidelines mean we have to change how we teach labs temporarily. In both semesters you will receive labs led by a lecturer and one of our Graduate Teaching Assistants, one-to-one office hours and support, Peer-Assisted Learning (which are sessions run but students in other years to help you get support from students who have been through your course), and the opportunity to work with other students. You will learn all the content and skills and complete all the assessments that students in 2019-2020 did.\n\r\rOur priority is giving you face-to-face contact to welcome you into the University community, and give you the student experience you deserve – making friends, talking to your lecturers, getting involved in community activities. We want to make sure you really feel like you are a part of Team UofG and so that is the type of contact we will prioritise. Depending on guidelines, we would really like to prioritise the use of campus space for things that make your university experience unique and special (like study groups, student societies, and getting to know staff and other students). How we do this will depend on the guidelines, but we want you to know what our priorities are.\n\rBut how to put that into action as course lead (and lecturer)? What do I do next? I’ve found it useful, and calming, to make a list of what I know and what I don’t know and structure the work I have to do in the summer around that.\nThings I know:\n\rLectures will be delivered online\rWe cannot have a large in-person exam at the end of term\rSome assessment guidance will need updated\rThe organisation of the VLE needs to be 100% super tight and clear\rReading and course materials need to be available online and accessible\rWe are going to use Teams to communicate with our students\r\rThis isn’t everything, but it’s a good chunk of concrete, actionable tasks and a positive starting point. I still don’t know how many students are actually going to turn up, if there’s going to be a second wave or local lockdowns I’ll need to deal with, or what the distancing guidelines are going to be come September. But that’s where the serenity, courage and wisdom come in (and let’s be real, not a small amount of wine). We have a plan for how to deliver the labs remotely, blended, and fully face-to-face but that’s the bit that’s most affected by the uncertainty. So I’m just going to ignore it for a few weeks.\nThis week I started with the assessment focused tasks because regardless of the exact type of blended delivery, it’s incredibly important that any changes are considered in light of the learning outcomes and that I think about what additional guidance they may need.\nThe end-of-term exam will remain but rather than being in a drafty gym hall, the essay component will be a 24-hour open book exam and the MCQ component will be a Moodle quiz that’s also available for 24-hours but timed so that students have 60-minutes to complete it from when it’s first started. I have updated the assessment information to reflect this in addition to the ILOs and assessment guidance as the expectations of an open-book exam are slightly different (i.e., it can be more focused on evaluation and argument than rote-memorising citations).\nPreviously students completed 3 small-stakes MCQs (2% each) throughout the semester to encourage distributed practice. These remain in place but I have updated the assessment guidance and procedure to note that these quizzes will now be timed so that they can get used to the format before the bigger quiz for the end-of-term exam.\nI also started a conversation with our technicians and researchers about the best way to award credit for participating in online research. Previously this had been done manually by the researcher but now there will be much more research done online, we want to find a way to automate it.\nI transferred our Assessment Information Sheets to Sway rather than giving students pdfs. Sway is good for accessibility purposes and adapts the display for different devices whilst also allowing the reader to export a copy to pdf or Word if preferred. You can see an example of one of our assessment sheets here.\nFinally, we also set up a weekly catch-up with the Level 1 team. Not for any particular formal purpose but just to ensure that nothing comes as a surprise to anyone and that everyone has the opportunity to feedback on things as I work through the changes.\nNext week I will continue with working through the assessments (the group presentation is going to require a creative solution) before moving on to my lectures. I hope that this blog will be helpful to some of you, or at the very least, provide some reassurance that even those of us who spend our time squawking on twitter like we know what we’re doing are really just trying to do things one step at a time whilst the wine is chilling.\n\r","date":1594598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594674450,"objectID":"c2452817184200f93b8c761cdd8ea2a7","permalink":"/post/pivot-1-academic-serenity/","publishdate":"2020-07-13T00:00:00Z","relpermalink":"/post/pivot-1-academic-serenity/","section":"post","summary":"Grant me the serenity to accept the things I cannot change, courage to change the things I can, and wisdom to know the difference - Reinhold Niebuhr\n\r\rMake a nice list, calm your pants where you can, and always have wine in the fridge - Emily Nordmann","tags":["pivot"],"title":"Pivot 1: Academic Serenity","type":"post"},{"authors":null,"categories":null,"content":"We have produced infographic guides to help students use lecture recordings effectively. These guides accompany the following paper which provides recommendations for both students and instructors and has been accepted for publication and is currently available as a pre-print:\n Nordmann, E., Kuepper-Tetzel, C. E., Robson, L., Phillipson, S., Lipan, G., \u0026amp; Mcgeorge, P. (accpeted). Lecture capture: Practical recommendations for students and instructors. *Scholarship of Teaching and Learning in Psychology. https://doi.org/0.1037/stl0000190\n The guides are available in English, German, Dutch, and Welsh. You are free to reuse, remix and adapt these guides with attribution. Because even teaching-focused academics need to worry about impact, please let me know via e-mail or Twitter if you do adopt them!\n  Infographic guides  Pre-print of paper  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"9d1438109dace16dcb53172635fe141d","permalink":"/project/lecture-capture/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/project/lecture-capture/","section":"project","summary":"Practical guides to help students use lecture recordings effectively","tags":["study strategies","lecture capture"],"title":"Practical recommendations for lecture recordings","type":"project"},{"authors":null,"categories":null,"content":"This is a short tutorial on basic data visualisation in R covering scatterplots, boxplots, violin-plots, bar charts, and grouped data. It assumes basic R knowledge (e.g., how to run a function and load a package) but it does not assume any further programming ability.\nThis tutorial was the result of a request by for anti-racism curriculum materials. The data used is from the Slave Voyages project. The Trans-Atlantic Slave Trade Database comprises 36,000 individual slaving expeditions between 1514 and 1866. This dataset is horrifying and so is the history contained within the graphs you will produce.\nThis book is part of the PsyTeachR series from the School of Psychology, University of Glasgow. Our materials are open-source and have a Creative Commons licence so you are free to reuse, remix, and adapt our work, with attribution.\n  Intro to Data Viz book  GitHub repository  Slave Voyages website  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"4475f0c14616f72aa43fe6395ddd1385","permalink":"/project/slave-voyages/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/project/slave-voyages/","section":"project","summary":"Intro to Data Visualisation in R using the Slave Voyages dataset","tags":["rstats"],"title":"Slave Voyages Data Visualisation","type":"project"},{"authors":null,"categories":null,"content":"This introductory lecture is given to first year students in week 1 of term. The main focus of the lecture is on developing effective study strategies and to demystify some of the hidden curriculum.\nTopics covered include:\n How to take notes What to do before, during, and after lectures How to use lecture recordings How to send an e-mail Which study strategies are effective and which are not  You are free to reuse, remix and adapt these slides without attribution, however, because even teaching-focused academics need to worry about impact, please let me know via e-mail or Twitter if you do!\n  Download slides  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"92a2a15ff8dea418c1e99b3c705d773e","permalink":"/project/how_to_study/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/project/how_to_study/","section":"project","summary":"Introductory lecture for first year students explaining effective study strategies","tags":["study strategies","other"],"title":"How to Study","type":"project"},{"authors":null,"categories":null,"content":"The Level 1 Data Skills course is intended for first year undergraduate psychology students who have no prior experience in programming. The course covers intro to R, data wrangling, visualisation, and descriptive statistics, underpinned by a focus on reproducibility.\nThis book is part of the PsyTeachR series from the School of Psychology, University of Glasgow. Our materials are open-source and have a Creative Commons licence so you are free to reuse, remix, and adapt our work, with attribution.\n  Level 1 Data Skills course book  GitHub repository  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"61313ca4bb0004f8c2a6389500b715b4","permalink":"/project/l1-data-skills/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/project/l1-data-skills/","section":"project","summary":"Intro to R for first year undergraduate psychology students.","tags":["rstats"],"title":"Level 1 Data Skills","type":"project"},{"authors":null,"categories":null,"content":"The MSc data skills and statistics book is intended for MSc Conversion course psychology students who have no prior experience in programming or statistics and who come from incredibly varied academic backgrounds. The course covers intro to R, data wrangling, visualisation, and descriptive and inferential statistics (correlation, t-test, chi-square, ANOVA, regression, power and effect size), underpinned by a focus on reproducibility.\nThis book is part of the PsyTeachR series from the School of Psychology, University of Glasgow. Our materials are open-source and have a Creative Commons licence so you are free to reuse, remix, and adapt our work, with attribution.\n  MSc Data Skills and Statistics course book  GitHub repository  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"dfaa266045fe81d2a77f15984f6d33b3","permalink":"/project/msc-conv/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/project/msc-conv/","section":"project","summary":"Data skills and core statistics for MSc conversion course psychology students","tags":["rstats"],"title":"MSc Conversion Data Skills \u0026 Statistics","type":"project"},{"authors":null,"categories":null,"content":"First year students typically quickly learn to base their essays and reports on appropriate sources of evidence. However, appropriate citation placement can take longer to develop. This activity is designed to help students understand where to use evidence when producing academic writing. Students are provided with passages from published papers that have the citations removed and they are asked to add them back in, requiring students to determine which statements require evidence. The download contains the student version of the activity, and the answer file.\nYou are free to reuse, remix and adapt these files without attribution, however, because even teaching-focused academics need to worry about impact, please let me know via e-mail or Twitter if you do!\n  Download activity files  The purpose of Open Educational Resources is to reduce unnecessary financial burden on students and to help share good practice. I make as many of my teaching materials open access as I can for these reasons, however, if you have found them helpful and are in a position to do so, please consider making a donation to Black British Professionals in STEM or Pride in STEM (currently by buying a badge).\n","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"cfb1636f7bd9b2858fdc73567c6b2beb","permalink":"/project/using_evidence/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/project/using_evidence/","section":"project","summary":"An interactive activity to help students learn how to use evidence and improve citation placement","tags":["academic writing","other"],"title":"Using Evidence","type":"project"},{"authors":[],"categories":["lecture capture","widening participation"],"content":"This blog is an excerpt of a keynote speech I gave at eduhub days 2020.\nWhen I first started talking about lecture capture research, I was so concerned with showing academics that lecture capture wasn’t about to bring an end to academia as we know it that I put all of my focus on that link between attendance and recording use. Aside from the issues I’ve already talked about, I think that in doing so I neglected the most important point of lecture capture, its raison d\u0026rsquo;être, it’s humanity. Lecture capture, at it’s core, is a second chance and that’s a good thing.\nIn some respects this isn’t too controversial; we know that non-native speakers and students with learning disabilities hugely appreciate the provision of recordings and we also know from the many studies on staff attitudes that these two issues are cited quite frequently as “acceptable” reasons to provide recordings, but lecture capture is more than this.\nWhen it comes to how lecture recordings impact the experience of those with physical and mental health problems, we\u0026rsquo;ve got less evidence. It’s touched upon in a number of studies but I’m not aware of any work that has this as its main focus despite the fact that unavoidable absences that include illness are the number one reason students give for missing lectures.\nI believe that part of the reason there’s not been a great deal of work on this is because we, as learning tech advocates, have been cautious about making the case that lecture recordings could be used as a tool that would allow students to miss lectures. There’s that fear that if you bring up the subject of lecture capture and attendance in any capacity then the conversation will stall, I\u0026rsquo;ve seen it happen.\nNow, this isn’t to say that I believe that providing lecture capture reduces attendance, there’s more studies that show no effect than do and when they do it’s generally a small effect. This is simply to say that I’m now willing to recognise that some students have personal circumstances that mean they choose to watch a recording rather than attend and I don’t think this is something we should necessarily be trying to prevent.\nI’ve thought a lot about why I’ve had this change in approach and I think that one reason is the job move I mentioned. I feel extremely lucky to be at the University of Glasgow for many reasons but one of them is the diversity, I mean, it’s still academia, it’s still the playground of the able-boded, white, middle-classes, but there’s been a definitive difference in the people who are in my classroom and the that people I work with.\nOne of those people is Steph Allan and I include her in this talk with her permission. Steph is a PhD student at the University of Glasgow, she’s in her 2nd year, she’s just had her first first-author publication accepted, she teaches on my 1st year course, and she recently came back from a trip to Palestine where she was teaching programming in R using open-source software and materials so that they could continue learning at no additional cost. She’s the type of person you want on your team and the type of person that academia needs.\nSteph also happens to have psychosis and she essentially didn’t go to any lectures for all of first year and she cites lecture recordings as the reason that she was able to continue her undergraduate degree and the reason she\u0026rsquo;s now doing a PhD. Not just because of the course content she could catch up on but all the other stuff that got captured, the enthusiasm of the lecturer, the class announcements, the insight into what academia actually involves.\n \u0026ldquo;While not a perfect substitution for being on campus, they captured elements such as a lecturer’s enthusiasm for a topic which was inspiring when trying to engage with the content away from campus. Furthermore, this meant I was privy to “class announcements” (For example, the psychology society hosting an event) which were not on the actual slides. I would not have gone to them, but at least I knew what was on. This helped demystify the hidden curriculum a bit as I could see academia was not just remembering facts and regurgitating them for exams, but also seemed to involve meeting up and discussing ideas…I would not be doing a PhD without lecture recordings.”\n I know the counter-argument to this, I’ve heard it many times before, this is an exceptional case, if she wanted recordings she could have requested them through the disability service, this isn’t a reason to provide recordings for all and let me just put a pin in that for a moment and tell you about the second thing that’s happened to change my thinking.\nLast year I received funding from the Quality Assurance Agency in Scotland with my colleagues Jill MacKay from the University of Edinburgh and Jacqui Hutchinson from the University of Aberdeen to look at how lecture capture can be used to support widening participation. I think that widening participation might be a UK term so just in case, what this refers to is trying to make academia that little bit less of a white, middle-class haven. The standard definition of widening participation tends to focus on socio-economic variables, i.e., the aim is to try to encourage students from deprived areas into higher education but in recent years the definition is getting a bit broader to include people who have caring responsibilities and first generation students.\nWe had our first open meeting for this project back in December and the discussions we had there have really stuck with me. First, when we talk about lecture recordings being useful for certain groups of people, the conversation never normally goes much further than non-native speakers, learning disabilities, and health problems; but there’s a whole range of other, more hidden reasons why you might need a second chance at a lecture that are tied to widening participation.\nOur classrooms are becoming more diverse and that’s a good thing. But that diversity comes with a range of issues that aren’t hallmarks of the middle-class academic experience – poorer students are more likely to have caring responsibilities, they’re more likely to live at home and have a long and expensive commute, they’re more likely to need to work to support themselves, more likely to be the first person in their family to go to university which means they’re more likely to have poorer study skills when they first arrive because no-one at home could tell them what being in a lecture was going to be like. Lecture capture is a second chance for all of these people, it’s a second chance to participate fully in university life.\nI understand that whenever you put a system in place that some people will abuse it and take advantage of it. I understand that if we record our lectures then there will be a handful of students who don’t’ attend and out of no reason other than sheer laziness and apathy. But I also believe that you shouldn’t punish the most vulnerable because a few might not use the system as you intended. I grew up in poverty, I am one of the people that would now be categorised as a widening participation target, and I can assure you that whilst the pearly gates of academia might look pretty when you’re let in through the front door, there’s barbed wire on top if you have to try and climb over them to get in.\nI acknowledge that there are good reasons not to record lectures, I would never want to stifle discussion or debate or change someone’s pedagogy to fit a recording and I am loud and clear that staff should have the right to opt-out when recording would not be appropriate or if it would negatively impact on the educational experience.\nBut what I’m becoming increasingly angry about is a refusal to engage with the evidence, a refusal to listen to people who try and tell their stories of the impact that lecture recordings can have, a refusal from those who got the gates held open for them to look up and see the barbed wire because they just don’t want to record their lectures.\nI’m angry that at our first meeting in December, Jill, Jacqui and I realised that we were all from a WP background and that maybe the reason there’s not been any research that looks at this is because no-one who hasn’t been through it themselves has cared.\nLecture capture isn’t a panacea by any means but there’s a socially progressive case to be made for it and it’s not an easy one to make because it involves admitting that recordings allow students to choose not to attend. What we need to do is highlight that many of those choices are for very good reasons.\nTo return to the pin I put in the example of Steph and disability provision, I don’t agree with the argument that students should have to request access to lecture recordings through disability because aside from generalised benefits like helping to support note-taking it means you miss out on all of the people for whom what’s wrong with their life doesn’t come with a diagnosis.\nYes, you could say that students could still ask for it if they wanted it and academics would probably agree but can you imagine how humiliating that would be? “Dear Professor Smith, would it be ok if I record your lectures, you see, I’m the first person in my family to go to university and I’ve got no idea what I’m doing”. We have a responsibility to our students and I increasingly I think lecture recordings have become part of that responsibility.\n","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594121383,"objectID":"235dfd49a9168e72ee1fcb0749a04cb0","permalink":"/post/lec-rec-keynote/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/post/lec-rec-keynote/","section":"post","summary":"This blog is an excerpt of a keynote speech I gave at eduhub days 2020.\nWhen I first started talking about lecture capture research, I was so concerned with showing academics that lecture capture wasn’t about to bring an end to academia as we know it that I put all of my focus on that link between attendance and recording use.","tags":["lecture capture","widening participation"],"title":"Lecture capture \u0026 widening participation: keynote excerpt","type":"post"}]
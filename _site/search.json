[
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Blog",
    "section": "",
    "text": "Corbetts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupporting positive behaviour\n\n\n\n\n\n\n\n\nSeptember 23, 2025\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nMunro Tidy Tuesday\n\n\n\n\n\n\n\n\nAugust 31, 2025\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nSupporting scholarship in yourself and others\n\n\n\n\n\n\n\n\nMarch 26, 2025\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PERU, with love\n\n\n\n\n\n\n\n\nDecember 11, 2024\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nMCQs in the age of AI\n\n\n\n\n\n\n\n\nOctober 26, 2024\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nPsychology of Swearing UWS\n\n\n\n\n\n\n\n\nApril 17, 2024\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT: Student guidance for essays\n\n\n\n\n\n\n\n\nAugust 1, 2023\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT for learning & teaching: Correcting auto-transcripts\n\n\n\n\n\n\n\n\nJuly 23, 2023\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT: Student guidance for mulitple-choice assessments\n\n\n\n\n\n\n\n\nJuly 21, 2023\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT to create teaching materials: marking criteria & rubrics\n\n\n\n\n\n\n\n\nJuly 2, 2023\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT to create teaching materials: Data simulation & MCQs\n\n\n\n\n\n\n\n\nJune 12, 2023\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible Submission Windows\n\n\n\n\n\n\n\n\nAugust 23, 2022\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nPivot 7: Grief\n\n\n\n\n\n\n\n\nAugust 27, 2020\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nPivot 6: Human after all\n\n\n\n\n\n\n\n\nAugust 21, 2020\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nLec Rec impact\n\n\n\n\n\n\n\n\nAugust 11, 2020\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nPivot 3: Party\n\n\n\n\n\n\n\n\nJuly 31, 2020\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nPivot 2: Academic, heal thyself\n\n\n\n\n\n\n\n\nJuly 24, 2020\n\n\nEmily Nordmann\n\n\n\n\n\n\n\n\n\n\n\n\nPivot 1: Academic Serenity\n\n\n\n\n\n\n\n\nJuly 13, 2020\n\n\nEmily Nordmann\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html",
    "title": "Supporting scholarship in yourself and others",
    "section": "",
    "text": "Today I visited Queen Mary University London where I met with the Teaching and Scholarship staff at the School of Biological and Behavioural Sciences. I was asked to talk about my career journey and advice for getting scholarship done myself as a researcher but also as someone who has engaged in leadership activities trying to support the scholarship of others. I started making notes and it got a bit long so I thought I might as well write it all down properly as it might be of use."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#time-and-workload",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#time-and-workload",
    "title": "Supporting scholarship in yourself and others",
    "section": "Time and workload",
    "text": "Time and workload\nLet’s get the elephant in the room that’s sitting on your head out the way – the first thing anyone needs to do scholarship is time. If SoTL is part of your promotion criteria then it should be an appropriate part of your workload. The first step is knowing exactly what proportion of your time – at Glasgow full-time LTS staff should have 20% (308 hours a year at 1FTE) allocated to scholarship activity. The second, much harder step, is actually protecting that time. As a manager and leader, you need to fight for your staff to have a functioning, transparent workload model, and listen to them when they tell you they don’t have the time and dig into the reasons. Sometimes it’s their total workload, sometimes it’s the balance of workload across the year, and sometimes it’s that people are not making good choices about how they spend their time. But they all have different solutions and you can’t start working on the appropriate one until you know the exact nature of the issue.\nAs a member of staff trying to do scholarship, I think it’s important to be realistic about the nature of academia. My 20% allocation could be viewed as a day a week but it would be unreasonable of me to demand that I get a day every single week to work on scholarship because that’s not how seasonal academia works - we’re essentially farmers with laptops, and sometimes the harvest has to take priority. Instead. I find it’s useful to take a longer view and think about the year as a whole. I know I will get very little done during term time but during the summer I might have whole weeks where I work on research. During those periods, I block out half or whole days in my diary and I consider them as immovable as all my other meetings. I don’t demand a day a week to do scholarship, but I do demand my 308 hours a year."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#learn-to-say-no-but-remember-to-say-yes",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#learn-to-say-no-but-remember-to-say-yes",
    "title": "Supporting scholarship in yourself and others",
    "section": "Learn to say no, but remember to say yes",
    "text": "Learn to say no, but remember to say yes\nRelated to the above, learning to say no is an important skill. My favourite line is “I’m afraid I don’t have capacity”. If it’s an opportunity that would benefit someone else, then put their name forward instead. If it’s an internal request, ask for something to be taken off you in replacement and go in with a solution. A common complaint about workload is that managers and models don’t take into account how long things actually take. If that’s the case, then take control and frame the conversation yourself and use your knowledge about your workload. You’re saying that doing X will take this much time, I can take that on if I give up Y.\nProtecting your time isn’t just about demands your employer places on you, think about how much you are giving to your students. I often frame the time we spend with students as “water workload” in that it will take up as much space as you give it. Have set office hours, set dissertation project meetings, set boundaries on replying to emails and expectations for your availability. Offer students the support they need and deserve, but not at personal cost to your wellbeing or career development. I have spoken to junior staff who never turn off their email and who offer each of their 10 project students an hour long meeting each week – we simply don’t have the time to do that. Also, I’d argue that’s not actually good for the students but we’re getting off-topic.\nBut also, remember to say yes. With the pressures of workload and everything else, being reminded to say no is very important. But if you say no to everything you will miss out on the opportunities that make the job fun, that allow you to meet interesting people, and that tick those boxes on the promotion form. You will never do scholarship if you never choose to do scholarship. If something has to burn, make the choice to let something else but your scholarship burn occasionally. This blog is too long and my colleagues will roll their eyes too hard if I tell this story for the 100th time but saying yes to something I thought I didn’t really have time to do ended up with me being interviewed about lecture capture opposite the Matterhorn."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#knowledge-broker",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#knowledge-broker",
    "title": "Supporting scholarship in yourself and others",
    "section": "Knowledge broker",
    "text": "Knowledge broker\nOne of the challenges I have faced trying to get SoTL done is ensuring I have the resource to do robust research. Sometimes this is about money but it’s more often about data and the fact that the nature of educational research means it’s reliant on observational, opt-in, self-report data with underpowered samples. A watershed moment for me was when I discovered the concept of “knowledge brokering”, the idea that you don’t have to do your own empirical research but instead you can have just as much impact being the conduit between the research and those who need it. A large part of what I do now is packaging up the literature in ways that are easy to digest for academics who don’t have educational expertise but just want to do things a little bit better – for example Lecture capture: Practical recommendations for students and instructors or Ten simple rules for supporting a temporary online pivot in higher education. Aside from trying to be useful, what these types of papers give me is control. I don’t need money or participants I can only access at a certain time of year to write these papers. I can write them when I have the time and thinking space to do so and they also don’t really require funding to conduct."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#sotl-your-day-job",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#sotl-your-day-job",
    "title": "Supporting scholarship in yourself and others",
    "section": "SoTL your day job",
    "text": "SoTL your day job\nBe strategic about what your scholarship focuses on. My main teaching and admin role has been with first year students, so my research focuses on study skills, self-regulation, and belonging. This means that my research informs my classroom and my classroom informs my research. I am a better first year lead than I was a decade ago because I can explain the theoretical basis of every part of the course, it’s all evidence-informed but also, it’s just efficient. If your research has nothing to do with your teaching and admin, the cognitive effort in switching gears will add friction. SoTL what you know and know what you SoTL. You also want to try and develop an area of expertise, something you’re known for. Mine is lecture capture and it’s the thing I always return to. I’m not saying don’t get distracted by shiny new topics and try new things, and when you’re first starting out, it can take time to find your niche. But, it helps to have a clear area of expertise as it makes it easier to become known for that area – someone who does everything is known for nothing."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#money",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#money",
    "title": "Supporting scholarship in yourself and others",
    "section": "Money",
    "text": "Money\nThis one is aimed at the managers and leaders. If you want your LTS staff to do high-quality, impactful SoTL you have to fund it appropriately. At UofG we have several SoTL-related funding schemes staff can apply for and I am lucky to be in a School that has a generous travel fund. As I write this the sector is in a death spiral and I know the reality for many is very far away from what I have access to and I recognise my privilege. But it doesn’t make it any less true that some level of resource is needed to transform SoTL from a course level evaluation to impactful, robust research that is disseminated properly, and if the only thing I can do to help is to say that out loud, then at the very least I can say it out loud."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#collaboration-community-and-mentoring",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#collaboration-community-and-mentoring",
    "title": "Supporting scholarship in yourself and others",
    "section": "Collaboration, community, and mentoring",
    "text": "Collaboration, community, and mentoring\nI’m also lucky that I work in a large LTS team so we have a very strong community of scholarship support. But in other Schools the LTS team is smaller and so we also have an institutional SoTL network that brings people together and this network does vital work. You’re much more likely to be successful at SoTL when you work with other people and when you can share skills and expertise and workload – my mantra has been to try and do more with less, to aim for fewer projects that actually make it to output.\nI’ve also found social media to be a great networking tool and it’s allowed me to work with the best people rather than just the nearest people. These kind of communities also allow for both formal and informal mentoring. When staff are starting out on their SoTL journey, my advice is to get involved with an existing project with someone a bit further down the line and this is particularly important for those LTS staffs whose discipline research and expertise is very far away from education research. Making the jump can be hard, but it’s much easier in collaboration."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#dont-wait-for-stuff-to-come-to-you",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#dont-wait-for-stuff-to-come-to-you",
    "title": "Supporting scholarship in yourself and others",
    "section": "Don’t wait for stuff to come to you",
    "text": "Don’t wait for stuff to come to you\nNominate yourself for awards, ask to be invited for a talk, put yourself forward for things you want to do. Lots of stuff isn’t in our control but you have to be in the race to win it. Similarly, make sure that you have access to sources of information about SoTL related funding, conferences, journals, and other opportunities. My main source of information for this type of thing is social media – LinkedIn and Bluesky these days. Social media isn’t for everyone and that’s fine but in that case, sign-up to mailing lists (e.g., the SEDA list) or join an organisation like ESPLAT. No-one is going to come chasing you with information, you need to do something to find it."
  },
  {
    "objectID": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#for-the-love-of-god-share-what-you-do",
    "href": "post/2025-03-26-supporting-scholarship-in-yourself-and-others/index.en.html#for-the-love-of-god-share-what-you-do",
    "title": "Supporting scholarship in yourself and others",
    "section": "For the love of god, share what you do",
    "text": "For the love of god, share what you do\nAnd finally, for the love of god, tell people about your work! Again, social media isn’t for everyone and that’s fine, but if it’s not for you then go to a conference, find other ways of networking, write a blog, send out a carrier pigeon. If the work was worth doing, it’s worth getting out there properly and it’s a more efficient use of time to wring every bit of impact out of one paper than it is to write 5 papers that no-one reads. I get asked how I manage to get so many invited talks and the main reason is that I have put a huge amount of work into promoting my work -it’s not an optional extra, dissemination should be part of your project plan."
  },
  {
    "objectID": "post/2024-10-26-mcqs-in-the-age-of-ai/index.en.html",
    "href": "post/2024-10-26-mcqs-in-the-age-of-ai/index.en.html",
    "title": "MCQs in the age of AI",
    "section": "",
    "text": "I shared this guide on Writing Good Multiple Choice Test Questions on social media and the first response I got on both LinkedIn and BlueSky was whether MCQs hold up in the age of AI. As first year lead for many years, this is a problem I’ve put a huge amount of thought into, so if you’re interested in all those thoughts, buckle up.\nI am assuming that the long-form version of the question “do MCQs hold up in the age of AI?” is really “does the academic integrity of online open-book MCQs hold up in the age of AI?” And the answer to that question is quite categorically no but let me take a little detour about what we did when we moved online for covid because it also serves as a defence of MCQs, which is honestly the real point of this post.\n\nHigh stakes MCQ exams\nIn 2020, our Level 1 exam was an 80-question MCQ exam that students had 60 minutes to complete. It was worth 40% of their final course grade, with the rest coming from an essay and an engagement portfolio of low-stakes data skills, weekly quizzes, and research participation. In the first semester of 2020, we ran the exam online using much the same questions that we’d used when the exam was in-person and it was a disaster. The distribution that had held for years crumbled. The average grade was a high A.\nNow before anyone calls me a demon for decrying students performing well, I’ve got no issue with assessments where the grades skew high because they’ve all learned a lot and studied hard, but this was not that. The issue was that we’d given them factual MCQs that could be easily Googled, for example:\n\nWhich of the following best describes a between-subjects design in experimental research?\n\nThe same participants take part in all conditions of the experiment.\nDifferent groups of participants experience only one condition each.\nParticipants are observed in their natural environment without intervention.\nThe experiment includes both repeated measures and independent groups.\n\n\nSo we burned down the exam questions and started again. I’m continually amazed by my team, they’re wonderful people who are truly excellent at their jobs and care deeply about learning and teaching and I can’t over-estimate the work they put into this. We consulted the literature on best practice for MCQs (Blake Harvard has a great blog about how to make more effective MCQs) and rewrote all the questions to require students to apply their knowledge and, crucially, questions that could not be easily answered by Google at the time. For example, the above factual MCQ became the below applied MCQ:\n\nA researcher designs an experiment where participants either drink caffeinated coffee or decaffeinated coffee and then take part in a reaction time test. Based on the information provided, what is the design of this experiment?\n\nBetween -subjects\nWithin-subjects\nMixed-design\nCase study\n\n\nThe distribution returned to a peak of a low to middling B / 2:1 but more than that, I think we created a much, much better exam, one that measured their knowledge and their ability to apply it, rather than their memory. They could have all their notes with them but if they hadn’t put in the time to understand what they were being taught, it wouldn’t help much. One of my key frustrations with the discussion over exams is the default assumption that they are all bad. Let’s say it again for the people are the back “EXAMS ARE NOT BAD, BAD EXAMS ARE BAD”. You give students a 100% closed book exam that only tests how well they can remember a bunch of names and dates, with no thought to accessibility, and little in the way of learning support throughout the semester, then sure, exams are shit.\nBut rote memorisation is not the same as knowledge. And students need knowledge. You can’t critically evaluate something you don’t know. There is a very robust literature on the impact of prior knowledge on learning new knowledge and skills and for many introductory courses (like first year psychology), building that knowledge base so that they can go on and do more interesting things is the point. Please stop asking me if I’d like to replace my first year exam with a podcast or experiential learning. I am happy to die on the hill that a) “authentic assessment” as a response to every question about assessment and feedback has become a hollow, meaningless slogan for people without true expertise in effective learning and teaching (see also, active learning) and b) having a broad knowledge base of psychological concepts and theories is an authentic part of being a psychologist.\nAnd this is actually my main concern when it comes to AI. Yes academic integrity is important but for me, the fact that they won’t learn anything is what keeps me up at night. If there was some way they could use AI to cheat but still have developed all that core knowledge then I would honestly care less. But what I’ve seen over the last few years is a slow creep of students who are unable to tackle those more interesting tasks of analysing, evaluating, and creating, because either they, or their education system, skipped over developing core knowledge. There is also unquestionably a workload component to MCQ exams as well. I have 600-700 students on my course each year and they provide an effective and efficient method of testing their knowledge acquisition. That’s important.\nWhat was the point of this blog post again? Oh right. If you have a high stakes summative open-book MCQ exam, it needs to go back in person, rather than in the bin. Whether it’s susceptible to AI is an entirely different discussion to whether it’s still a useful assessment.\n\n\nLow stakes MCQs\nYou might think I’ve written enough but I also want to discuss the issue of low-stakes MCQs, because we’ve done a lot of work on those as well.\nIn our first semester Level 1 course, we had two types of low-stakes MCQs. First, there were weekly MCQs that were related to the lecture content and these were originally a mark for participation (5% in total) because there’s also a robust literature on the impact of practice testing and distributed practice on learning and the quizzes supported students to study continuously. Second, we had an open-book MCQ about data skills and programming in R that was worth 5% of their grade and arrived in week 6 of term. Pre-AI, this MCQ was very, very effective at identifying students who had been keeping up with their data skills work and those who had disengaged / were trying to cram. Post-AI, pointless.\nBut again, the issue here isn’t academic integrity. These are low-stakes assessments where we expected the grades to skew high. I don’t care if they all get As, I care if they’re learning. So our response has been to change the grading scheme rather than the MCQs themselves. This year, students get two attempts at each MCQ (I must acknowledge that I stole this idea from Dr. Carolina Kuepper-Tetzel, Learning Scientist and all-round excellent work wife). On the first attempt, they’re instructed to not use any notes and instead use it as a pure test of their knowledge. After the first attempt, they see which ones they got right and wrong and then on the second attempt, they can use whatever they want to improve their score.\nBy doing this, they still get the boost that comes from practice testing and they also get feedback on how their learning is progressing. But the second attempt means that they get the grade and again, it’s low-stakes and the assessment load of the course has been built to withstand a high skew on these components.\nMore importantly, I hope that what we’re doing is implicitly and explicitly teaching students about effective study strategies and the difference between assessment for learning and assessment for grades and that we’re targeting motivation rather than just banging on about misconduct. Of course, students could still use AI to complete the first attempt but particularly when it comes to the data skills MCQ, it’s going to be very easy to identify them in the first-attempt distribution (very few used to get full-marks on that test). I’m not going to use this as some sort of integrity test (see again: not a demon) but I can use it to make the point that if these grades are your own work, this is amazing, but if they represent the use of AI, here’s why you’re only hurting yourself."
  },
  {
    "objectID": "post/2022-08-23-flexible-submission-windows/index.html",
    "href": "post/2022-08-23-flexible-submission-windows/index.html",
    "title": "Flexible Submission Windows",
    "section": "",
    "text": "Like many, the number of extension requests on my course increased over the last two years. This is not a “OH THOSE BLOODY STUDENTS” post. I don’t blame students for any of the problems we’re seeing now, I blame the pandemic. But more importantly there doesn’t need to be anyone at fault for there to be an issue we need to address and what we’ve seen is that giving out unlimited extensions isn’t really seeming to be helping anyone. Students’ ability to self-regulate and manage their time hasn’t developed as we would typically expect which is important because time management is suggested to have a relationship with academic performance and anxiety. Additionally, the staff workload associated with managing the volume of extensions and the knock-on effects on marking timelines and the return of feedback is simply untenable.\nThis also isn’t a post whereby I am going to suggest returning to a punitive, regressive system that bakes in ableism and classism. Providing flexibility and understanding that your students have complex lives and commitments beyond your course is a good thing (and that’s before we really feel the effect of students needing to work more because of the cost-of-living crisis).\nWe know that major coursework deadlines tend to cluster around the same points in the semester and given the structure of the academic year, there’s limited ways we can get around this, but also, learning to manage and prioritise multiple deadlines is a useful skill to develop. I am guilty myself of complaining that students don’t look at all their deadlines and work on them at different times to avoid the clustering effect. I am also guilty of saying yes to too many talk invitations in March because I think “teaching will be over” and I ignore what 15 years of experience has taught me. It is almost as if the problem is not “being a student” and instead “being a busy human”.\nSo this year, I am going to try and reframe how I talk about deadlines on my course and (hopefully) how students think about them by piloting Flexible Submission Windows. This is inspired by research that has shown that time management training can help students work more effectively although realistically I have no idea if this is going to work. Traditionally, our assignment deadlines are Friday at 12 noon, and we open the submission portal on Moodle a week before. We may get a handful of submissions in this week, but the majority come in the 12 hours before the deadline. I could complain about this but I could also accept that if I only promote a single fixed deadline, I am encouraging behaviour that focuses on a single fixed, final deadline.\nThese are the instructions for the essay submission this year:\n\nFor this assessment you have a flexible deadline window. You may submit your essay from Monday 7th November at 12 noon until the final deadline of Friday 11th November at 12 noon.\nTo help with your time management, you should review any other deadlines and commitments you have at this time, and decide which day during the flexible window works best for you. You will be asked in reading week to make a note of which day you plan to submit – this isn’t binding but explicitly making a choice may help you plan better.\nWe encourage you to plan to submit your essay early so that if you need a few extra days you can still meet the final deadline – if the day you submit your essay is different to the one you intended, you don’t need to inform anyone as long as you still submit within the window.\nMarks and feedback for those essays submitted within the window will be released on X.\nIf you need an extension beyond the final deadline, please see the Extensions & Good Cause guidance. Your extension request should explain why you were unable to submit during the flexible window.\n\nWe have a reading week in the middle of the semester where there is no formal teaching but students are encouraged to use the time to work on their assignments ahead of the deadline in week 8. In this week, I am going to ask students to note which day they intend to submit the essay on – this won’t be binding, rather, I’m trying to see if the Theory of Planned Behaviour is up to any good. We’re also not going to refuse extension requests – if students need extra flexibility it will still be offered.\nAs I’ve said, I don’t know if this is going to work and what unintended consequences it might have in terms of administration workload or student behaviours, which is why it’s a pilot. I will be evaluating it by comparing the number of extension requests and late submissions to last year, the profile of when assignments were submitted across the week, and I’m also going to explicitly ask students for their feedback on the system.\nWhat I want is to try and reframe the deadline so that they’re not aiming for a single fixed point in time that if they miss it, requires an extension. I’m not expecting that the majority of essays will be submitted at the start of the window. What I’m hoping for is that I can encourage as many students as possible to aim for a slightly earlier submission which means that when they inevitably need an extra day or two (because this is their first university assignment and it’s hard to know how long things take) they can still meet the final deadline which means they don’t get any of the worry associated with asking for an extension or submitting late, and we also minimise the workload associated with extensions and late submissions.\nEssentially, the skill I want to help students develop isn’t “you must always meet all deadlines” it’s “always build in a buffer because life will get in the way (and stop saying yes to talks in March)”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Professor Emily Nordmann",
    "section": "",
    "text": "Associate Dean of Learning and Teaching\n\n\nUniversity of Glasgow\n\n\n\n   \n\n\n\n\n\nI am Associate Dean for Learning and Teaching and Professor of Evidence-Informed Education at the University of Glasgow. My research predominantly focuses on lecture capture, how it can be used as an effective study tool by students and the impact on students from widening participation backgrounds as well as those with disabilities and neurodivergent conditions. In all my work, I draw on theories of learning from cognitive science and self-regulation, as well as theories of belonging and self-efficacy.\nMy leadership roles have centred around supporting those on the learning, teaching, and scholarship track. I am currently lead of the College of MVLS LTS Network and previously founded and led the Pedagogy and Education Research Unit in the School of Psychology and Neuroscience.\nI am based in the School of Psychology and Neuroscience. My teaching is varied although centres on cognitive psychology and beginner data skills in R and I am a vocal advocate of open science and open educational resources where as a member of the PsyTeachR team I has authored several open-access data skills books and tutorials."
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Professor Emily Nordmann",
    "section": "",
    "text": "I am Associate Dean for Learning and Teaching and Professor of Evidence-Informed Education at the University of Glasgow. My research predominantly focuses on lecture capture, how it can be used as an effective study tool by students and the impact on students from widening participation backgrounds as well as those with disabilities and neurodivergent conditions. In all my work, I draw on theories of learning from cognitive science and self-regulation, as well as theories of belonging and self-efficacy.\nMy leadership roles have centred around supporting those on the learning, teaching, and scholarship track. I am currently lead of the College of MVLS LTS Network and previously founded and led the Pedagogy and Education Research Unit in the School of Psychology and Neuroscience.\nI am based in the School of Psychology and Neuroscience. My teaching is varied although centres on cognitive psychology and beginner data skills in R and I am a vocal advocate of open science and open educational resources where as a member of the PsyTeachR team I has authored several open-access data skills books and tutorials."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact me by email at emily.nordmann@glasgow.ac.uk. You can also connect via social media although please do not DM on these platforms because I can only cope with managing one inbox and you won’t get a reply.\n\nEmail: emily.nordmann@glasgow.ac.uk\n\nLinkedIn: https://www.linkedin.com/in/emilynordmann/\n\nBlueSky: https://bsky.app/profile/emilynordmann.bsky.social"
  },
  {
    "objectID": "netlify.html",
    "href": "netlify.html",
    "title": "netlify",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "netlify.html#quarto",
    "href": "netlify.html#quarto",
    "title": "netlify",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html",
    "href": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html",
    "title": "ChatGPT: Student guidance for essays",
    "section": "",
    "text": "Part 1: Using ChatGPT to create teaching materials: Data simulation & MCQs\nPart 2: Using ChatGPT to create teaching materials: marking criteria & rubrics\nPart 3: ChatGPT: Student guidance for multiple-choice assessments\nPart 4: Using ChatGPT for learning & teaching: Correcting auto-transcripts\nYou could argue that these blogs are an exercise in procrastination and if you spend any time with me in real life, you’ll be questioning why I’m writing all these whilst constantly moaning about workload. If I’m completely honest they are part-procrastination, but, genuinely, I’m much further along than if I hadn’t spent the time on these blogs as the process of writing has helped shaped my thoughts.\nI’ve been working on the guidance for my Level 1 essay and in some ways, writing guidance for MCQs was more problematic in that ChatGPT can get them all right very quickly. But in other ways, what is and is not academic misconduct was so clear with MCQs and it’s so much harder to draw the lines in the sand when it comes to an essay and provide good advice on how students can and should use AI. I’m so much less sure of myself with the essay guidance. I’ve structured it under three subheadings although this may well change between now and September (and feedback and ideas are most welcome!). I think it’s starting to look ok but my sense of unease remains."
  },
  {
    "objectID": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html#what-use-of-ai-would-be-regarded-as-academic-misconduct-for-this-assessment",
    "href": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html#what-use-of-ai-would-be-regarded-as-academic-misconduct-for-this-assessment",
    "title": "ChatGPT: Student guidance for essays",
    "section": "What use of AI would be regarded as academic misconduct for this assessment?",
    "text": "What use of AI would be regarded as academic misconduct for this assessment?\n\nUsing AI to write any part of your essay for you from scratch is academic misconduct and if we suspect you have used it in this way, we will report you for academic misconduct.\n\n\nAn easy rule of thumb to follow for the essay is do not copy and paste anything from an AI into your essay. This doesn’t avoid every problem, and there are exceptions to the rule, but if you want a simple rule to follow, that’s a good starting point.\n\n\nUse AI to help you refine and edit your essay, do not use it to write the essay. When in doubt, come and ask talk to us in office hours - you will not get in trouble for asking how to do things right!\n\nIt’s all shades of grey but I wanted to provide as much clarity as I could. The “don’t copy and paste” rule isn’t perfect but it probably works well enough to give the right idea. By explicitly noting they may not be sure and could reach out for help, I also wanted to highlight that this isn’t an exact art and it’s not wrong for them to have questions."
  },
  {
    "objectID": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html#how-can-i-use-ai-to-help-support-my-learning-for-this-assessment",
    "href": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html#how-can-i-use-ai-to-help-support-my-learning-for-this-assessment",
    "title": "ChatGPT: Student guidance for essays",
    "section": "How can I use AI to help support my learning for this assessment?",
    "text": "How can I use AI to help support my learning for this assessment?\n\nYou can use specific AI tools to help with your literature search (but do not use ChatGPT for this! See below for more info):\n\n\nTools like Elicit or Connected Papers can find relevant papers without perfect keyword match, summarise takeaways from the paper specific to your question, and extract key information from the papers. It’s not academic misconduct to use Elicit because you still have to read the papers, understand the information, and integrate it into your essay.\n\n\nYou can use tools like ChatGPT to get feedback and use it to help you refine and edit (but not write!) your essay. For example, you can use it to get feedback on your draft essay plan. This is a good use of AI because you’re using it to get feedback but you’re playing an absolutely active role in the creation of your essay.\n\n\nExample prompt: Act as an expert psychology tutor. I am a first year undergraduate psychology student and have to write an essay on the question [give it your essay question]. Here is the marking criteria [give it the marking criteria]. And here is a draft structure I have written to plan my essay [give it your essay plan]. I want you to give me feedback on how I can improve my essay plan, but before you give any feedback, ask me up to 5 questions about my plan and use my answers to inform your feedback.\n\n\nYou can also use it to give you feedback on your draft essay before you submit it. As with using it for feedback on your plan, this is a good use of AI because you’re using it in an active way and any changes to your essay will be your own work, you’re just getting direction form it.\n\n\nExample prompt: Act as an expert psychology tutor. am a first year undergraduate psychology student and have to write an essay on the question [give it your essay question]. Here is the marking criteria [give it the marking criteria]. Give me feedback on my essay for each of the marking criteria.\n\nTo test out using it for feedback I got ChatGPT to write me a draft essay plan and essay and then I fed the snake it’s own tail and asked for feedback in a new chat window. The feedback it produced was not always up to what a human would produce (although let’s face it, that depends on the human) and there’s a need to urge caution but as a supplement rather than a replacement, I think this would be a useful form of formative feedback - here’s an example of the feedback it gave aligned with the marking criteria:\n\n3.2 Ensure that the essay has a logical structure and that the paragraphs flow together to form a coherent narrative: The structure of your essay is good, with an introduction, main body, and conclusion. The narrative flow between paragraphs is generally effective, but there are a few places where transitions could be smoother. Additionally, some paragraphs may be too long or too short, affecting the overall coherence of the essay. Consider reorganizing and editing for better flow and readability.\n\nIt is quite general but I would prefer general to overly specific and the important thing here is that ChatGPT isn’t doing any of their work, they still have to action the feedback. I would also be really interested to know if there’s a difference in how students respond to AI vs human feedback - I would imagine they’d be more likely to assume the human was right but also coming from an AI probably strips the feedback of its emotion which might make them more likely to process it? Interesting times ahead.\nI also wanted to address the use of ChatGPT for editing. Most of my thoughts on this came out of trying to use it for correcting auto-transcripts where I really struggled to get it to do exactly what I wanted. My key concern here is students giving it their essay and then whether they wanted it to or not, ChatGPT editing the entire thing and changing the meaning of what they’ve written and stripping their writing of any individuality. I want them to make active, conscious choices and to learn from the editing process not just take what it gives them without thinking.\n\nFinally, you can use it to help you proof-read and edit. Be very careful to ask it to provide you with line-by-line changes with the comparison so that you can see what it has changed and why, because it means you’re making an active choice and it will also help you learn about how to improve your writing.\n\n\nBefore you use AI for editing, remember that it is important that you develop your own voice as a writer and if you constantly use AI to edit you’ll end up with a bland, generic writing style. If everyone does this we’re going to end up with a world written in beige. Don’t be beige. Also remember that just because the AI suggests an edit, doesn’t mean it is better than what you’ve written! Even with the below prompts it may edit more than you ask it to - if it does this and edits huge paragraphs rather than individual sentences, ignore it and click regenerate response - you want it to give you the edits line-by-line so that you can compare them and choose the ones you think are actually an improvement.\n\n\nExample prompt: Act as an expert academic writing tutor. I have written an undergraduate university essay and I would like you to review it with regards to spelling, grammar, concise writing, APA formatting, and academic tone. Provide suggestions for edits I can make sentence-by-sentence. Provide the original and your edited suggestion so that I can compare them. Explain why you have suggested the edit [give it your essay]:\n\n\nExample prompt if English is not your first language: Act as an expert academic writing tutor. My first language is [your first language]. I have written an undergraduate university essay and I would like you to review it with regards to ensuring that the writing follows the grammar rules and conventions of British English. Provide suggestions for edits I can make sentence-by-sentence. Provide the original and your edited suggestion so that I can compare them. Explain why you have suggested the edit [give it your essay]\n\nI am terrified that I am pointing them towards a tool that will potentially harm their development as writers, either because they will blindly copy and paste and end up changing the meaning of what they intended, or that we’ll end up in a world where everyone has the same generic A-corrected writing.\nBut, abstinence as a policy isn’t going to work - if we don’t guide them they’re going to do it anyway, my logic here is that at least this way I can make them aware of the potential pitfalls.\nI am also very likely romanticising the need to develop an individual writing voice based on my own feelings about writing. As I write this I’ve got George Orwell’s rules for writing swirling around in my head, the fact that I know them by heart should tell me I might have crossed the line into wanky academic. Maybe losing some individuality is the price we pay for massively improving the clarity of most people’s writing (I vehemently don’t believe this but I feel like I should at least provide the other side of the argument)?."
  },
  {
    "objectID": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html#what-are-the-limitations-of-using-ai-for-this-assessment-and-what-should-i-be-careful-of-aside-from-academic-misconduct",
    "href": "post/2023-08-01-chatgpt-student-guidance-for-essays/index.en.html#what-are-the-limitations-of-using-ai-for-this-assessment-and-what-should-i-be-careful-of-aside-from-academic-misconduct",
    "title": "ChatGPT: Student guidance for essays",
    "section": "What are the limitations of using AI for this assessment and what should I be careful of aside from academic misconduct?",
    "text": "What are the limitations of using AI for this assessment and what should I be careful of aside from academic misconduct?\nIn addition to the above caveats, I have a standard heading about limitations to try and get them thinking more critically about AI and not just focus on the issue of misconduct.\n\nDo not use AI tools like ChatGPT to do your research. Use tools like Scopus, Web of Science, Google Scholar, ad Elicit and read the papers yourself. The reasons for this are that: a) if you rely on AI summaries you won’t develop the depth of understanding you need at university level and your essay will be shallow b) reading papers helps you develop an academic tone in your writing c) sometimes it tells you things that are simply wrong, including making up references that don’t exist d) it doesn’t have access to the most recent research.\n\n\nIf you use it for feedback, it may give you advice that isn’t very good and if you’re not super careful about ensuring you’ve given it all the guidance from the labs and your tutor, it may end up telling you to do something that contradicts what we’ve told you to do. If you do use AI to help refine your structure, we’d strongly encourage you to come to office hours or GTA sessions to check you’re on the right track.\n\n\nAI writing is frequently formulaic, uninspiring, and often doesn’t have the right tone.\n\n\nWhen you ask it to suggest edits for your work, you have to be really careful to review its suggestions because sometimes it will change the core meaning of what you’ve written rather than just correcting the grammar or tone. Always ask it to give you suggestions sentence-by-sentence and if it doesn’t do this, try changing the prompt or regenerate the response until it does. Remember that your version might be better than than the AI - trust yourself!\n\n\nAnother use case we’ve seen is using AI to correct your references into APA formatting, but having tried this, it didn’t do a very good job so we’d suggest using a reference manager like Zotero instead if you want something to help automate that process (although you still need to check them).\n\nIs this the best approach? I’m not sure. But I am fairly sure that right now, at scale, honesty and transparency is the only tool we’ve got that’s of any use. I know that some students will abuse it, and some will likely take my guidance to abuse it even more effectively. But I have always believed that we shouldn’t design learning & teaching around the lowest common denominator and that the overwhelming majority of students want to, and do, do things as intended so we should give them what information we do have whilst we’re still figuring it out on our side.\nGod I hope I’m right.\n\nAbout halfway through writing this blog I got the song “Walk through the fire” from the Buffy musical stuck in my head like the millennial I am so let’s end with that.\nWill this do a thing to change it? Am I leading them to danger? Are we all too far gone to care?"
  },
  {
    "objectID": "post/2024-12-11-from-peru-with-love/index.en.html",
    "href": "post/2024-12-11-from-peru-with-love/index.en.html",
    "title": "From PERU, with love",
    "section": "",
    "text": "Today was my last official event as Centre Head for the Pedagogy and Education Research Unit (PERU), and I wanted to take the opportunity to talk about why I think organisations like PERU are so important for education-focused academics and to summarise what we’ve done over the last two years, and why, in the hope it might provide a template for others. If you’re interested in keeping up with PERU after reading this, you can follow us on LinkedIn.\nPERU was formally established in 2022 as a response to our unique position as a very large LTS (Learning, Teaching, and Scholarship) team - I believe that the School of Psychology and Neuroscience is the biggest LTS team at the University of Glasgow with more than 30 full-time, permanent members of staff and I imagine we’re also one of the largest in the country.\nWe agreed on our structure and wrote our strategy through a series of collaborative workshops where we worked together to define PERU’s purpose and values and after several iterations and much discussion, settled on three research themes:\n\nApplied Cognition and Learning in Teaching, which focuses on evidence-based strategies to enhance student engagement and learning outcomes.\n\nAccess, Inclusion, and Wellbeing, which centres on equity, accessibility, and fostering a supportive environment for both staff and students.\n\nOpen Science and Pedagogy, which champions transparency, reproducibility, and the development of open educational resources.\n\nIt was also really important to me that we had a full strategy, so in a later activity, we formalised our values:,\n\nCollaboration and Support: At its core, PERU aims to provide a community of support and promote professional development through collaboration, transparency, trust, and fairness.\nOpenness: All work conducted by PERU embraces openness, either through the adoption of open scholarship practices in our research, or by ensuring our work is available as an open educational resource.\nInclusivity and Accessibility: Enhancing student and staff experiences by breaking down barriers to accessibility and fostering a sense of belonging is a core value of not only what we work on, but how we work as a team by ensuring that all staff have the opportunity and resource to develop professionally.\nIntegrity and Excellence: PERU is committed to upholding high standards of evidence-based scholarship and excellence in research. We prioritise quality over quantity and demand no less of our scholarship than would be expected of any other research field.\n\nSet concrete outcomes:\n\nAll LTS staff are research-active in an area that supports their professional development and the PERU strategy.\nOur work shapes educational practices and policies through the adoption of PERU research and outputs by others, nationally and internationally.\nWe achieve national and international recognition through awards, promotions, and fellowships.\nWe disseminate research findings through outlets such as peer-reviewed articles, national and international educational blogs, and external workshops and conference presentations.\nWe secure sources of internal and external funding.\n\nAnd agreed on what we needed in the form of enabling factors to achieve those outcomes, so that we could hold the School, and ourselves, to account:\n\nA fair and transparent workload model that ensures all LTS staff have time to conduct scholarship.\nInternal funding to support the development of research projects where other funding is not available, and to support dissemination and networking.\nRegular PERU sessions to help identify common themes and opportunities for collaboration.\nContinuous professional development and support, specific to the LTS track, and focused on scholarship and esteem across all grades.\n\nOur formalisation as a unit has been essential in validating the LTS team as researchers and in making the distinction between our teaching and scholarship activity (I teach, but I am not a teacher, I am an LTS academic). We are different to the other research centres in that we don’t participate in the REF and we don’t have funding targets (well, other than those pesky tuition fees and student numbers) but it’s so important that we still have a seat at the table and I’m also grateful to have a very supportive Head of School in Prof. Kate Jeffrey.\nIn many other Schools and institutions, LTS academics work in much smaller teams which can limit opportunities for collaboration, and at worst, can be isolating. In those cases, institution-wide SoTL Networks that provide breadth of expertise, professional development, and a sense of community are vital. But our size makes us different. Whilst our student numbers have presented us with challenges for teaching (everything must be designed for scale), when it comes to our scholarship, our size gives us a rare opportunity to build a strong internal community within the School where collaboration, support, and professional identity can thrive.\nAnd I think that this sense of identity and belonging is PERU’s greatest strength. It’s why when I hear people refer to themselves as part of PERU or talking about PERU to external colleagues it makes me so incredibly happy. It’s well-established that belonging is key to student success and that students tend to feel a greater sense of belonging to their School and subject rather than their University as a whole. Same goes for us. Identity and shared purpose and vision are so important, and they’re a big part of why we’ve been able to achieve so much. I will also note that you don’t have to be as big as us to organise into something like PERU, but this doesn’t detract from the point that the bigger you are, the more sense it makes to do so.\nFormalising the structure of PERU has also allowed us to take full advantage of the School-level resources available. While many LTS teams across the sector operate with little to no support, we’re lucky to have a School management team and environment that supports us with conference and travel funds, and with project funds (in addition to institutional funding for Learning and Teaching Development, Scholarship, and Staff-Student Partnerships). Our research tends to be cheap in relative terms, but it doesn’t happen for free and to have impact it needs disseminated and to thrive we need professional development and networking opportunities. If institutions create education-focused tracks that don’t/can’t apply for grants but also expect high quality scholarship then they better be dammed well willing to support it. Glasgow isn’t perfect but in this I think we lead the sector in putting our money where our expectations are.\nI’m incredibly proud of everything we’ve achieved. When PERU was first formed, I didn’t realise how much it would come to mean to me. I thought it would be just another admin role, but it’s been a source of immense joy. That said, it’s also absolutely the right time to hand it over to a fresh pair of eyes and I’m delighted to have Dr. Ashley Robertson take the lead. Her focus on supporting early-career colleagues and driving research outputs will build on what we’ve already established and I can’t wait to come along for the ride."
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html",
    "title": "Munro Tidy Tuesday",
    "section": "",
    "text": "I was immediately obsessed when I saw the Tidy Tuesday theme was Scottish Munros - we have climbed 58/282 so far. As I assume is the case for the majority of baggers, we use walkhighlands for all our munro info and routes. walkhighlands is one of the unequivocally wonderful bits of the internet and I can’t believe it’s free. We donate to keep it going, and if you’re a bagger who uses it often, I’d encourage you to do the same. When I saw the Tidy Tuesday dataset, I knew I wanted to try and combine the provided data from the Database of British and Irish Hills v18.2 with what’s available on walkhighlands.\nBecause I’ve made certain career choices, my day-to-day activity now involves a lot less coding and a lot more admin and I realised I was starting to lose some of my R so this has been a nice excuse to refresh. I’m gearing up for another semester of teaching R to students in the age of AI and it has been interesting to reflect on how I’m using AI myself. Pleasingly, many of the solutions to the many problems I had to solve came from my knowledge of Munros and Gaelic. AI sometimes provided the code but it’s a nice reminder it can only provide the answers to questions you know to ask."
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#walkhighlands-munro-info",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#walkhighlands-munro-info",
    "title": "Munro Tidy Tuesday",
    "section": "Walkhighlands munro info",
    "text": "Walkhighlands munro info\nThe reason I wanted to use walkhighlands data is that it has a bunch of route information that I could use for exploration that wasn’t contained in the Database of British and Irish Hills that is the base of the Tidy Tuesday data:\n\nRegion\nEstimated length of walk in hours\nDistance of walk\nTotal ascent of route (not just of each individual Munro)\nRoute descriptions\nUser rating of each Munro\nNumber of ascents recorded for each Munro\n\nI contacted walkhighlands to ask permission to share what I’ve done and they said yes because they’re lovely. I’ve still decided not to include the code I used for web scraping so I don’t inadvertently cause them any issues and so I’ll just describe roughly what I did instead.\nThe approach was to first scrape walkhighlands for the list of Munros, the region in which they are located, and their height from the Munro A-Z page. Then it took one route for each munro (the first listed), the min and max estimated walk time, distance in km, total ascent in metres, and used regex to look for certain words that describe walk features that might be of interest (scramble, exposed, arete, river, spate, bog). walkhighlands also provides a Grade rating for each walk as well as a bog factor, however, these are represented as images, and try as I (well, AI) might, I could not get it to parse this information.\n\nAn important note for those of you who are familiar with walkhighlands, as highlighted, I included one route per munro - the first one listed. This can make a big difference to the walk, for example, which route you take up Ben Nevis significantly changes the fear factor and technicality. Text mining is also a blunt tool and only looks at whether a word is contained in the walk report rather than its context - a route that reads “there is no scrambling required” would still have been included in the “scramble” category and exposure can relate to the weather or heights.\nIt took a long time to get the AI to provide code that worked and there were a number of issues - at one point it was matching the route to the wrong Munro, then it didn’t return all Munros, then it was missing a bunch of routes. I had to manually create a file of some routes to load in because I could not find a solution as to why these handful were failing. Because I could not have done any of this type of scraping without AI, I really have no idea why it works and why it didn’t. This is intellectually unsatisfying but also, the idea you’d be willing to trust this black box of “knowledge” to something more serious than an obsessive deep dive into your favorite mountains is madness.\nHere’s what the walkhighlands data looks like.\n\n\nmunroregionheightfirst_route_titlefirst_route_urltime_hours_mintime_hours_maxdistance_kmascentscrambleexposedspatebogriverpathlesstoiletbothypubcar_parkdeer_fenceascentsratingA' Bhuidheanach BheagCairngorms936Càrn na Caim and A' Bhuidheanach Bheag from Drumochterhttps://www.walkhighlands.co.uk/cairngorms/carn-na-caim.shtml5619824falsefalsefalsetruefalsefalsefalsefalsefalsefalsefalse10,6512.40A' Chailleach (Fannichs)Ullapool997Sgùrr Breac and a' Chailleach from near Braemorehttps://www.walkhighlands.co.uk/ullapool/sgurrbreac.shtml68161,127falsefalsefalsetruefalsetruefalsefalsefalsefalsefalse5,7293.48"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#database-of-british-and-irish-hills",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#database-of-british-and-irish-hills",
    "title": "Munro Tidy Tuesday",
    "section": "Database of British and Irish Hills",
    "text": "Database of British and Irish Hills\nNext it was time to load in the Tidy Tuesday dataset which is from the Database of British and Irish Hills. In order to be able to join this with my walkhighlands database, I had to do quite a lot of wrangling although thankfully I was not reliant on AI and mainly able to achieve it because of my existing knowledge of Munros and Gaelic.\nI wasn’t that bothered about the Munro status changes over the years as the walkhighlands database allowed me to do other more interesting analyses so I dropped these bits.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(ggthemes)\nlibrary(ggridges)\nlibrary(flextable)\nlibrary(stringi)\nlibrary(tidytext)\nlibrary(sf)       \nlibrary(plotly)\nlibrary(rnaturalearth)\n\nscottish_munros &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv')\n\nraw_data &lt;- read_csv(\"https://www.hills-database.co.uk/munrotab_v8.0.1.csv\")\n\nscottish_munros &lt;- raw_data |&gt;\n  filter(`2021` == \"MUN\") |&gt;\n  select(\n    `DoBIH Number`, Name,\n    `Height (m)`, xcoord, ycoord, \"Grid Ref\",\n  ) |&gt;\n  drop_na(`DoBIH Number`) |&gt; \n  rename(\n    munro = \"Name\",\n    height = `Height (m)`,\n    number = `DoBIH Number`,\n    grid_ref = \"Grid Ref\"\n  ) \nrm(raw_data)"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#whats-in-a-name-issues",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#whats-in-a-name-issues",
    "title": "Munro Tidy Tuesday",
    "section": "What’s in a name (issues)",
    "text": "What’s in a name (issues)\nThe first problem to solve before trying to join the two datasets was that some of the names of the Munros differ between the two - sometimes this is because there are variants in the Gaelic (Carn Eighe/Càrn Eige), sometimes the Anglicised version is used, and sometimes it’s because multiple Munros have the same name so they have additional information added in parenthesis in one of the files (Stuc an Lochain [Stuchd an Lochain]/Stùcd an Lochain).\n\n\nShow code\nscottish_munros &lt;- scottish_munros |&gt;\n  # Standardise names in the 'dobih' dataset to Walkhighlands spellings\n  mutate(\n    munro = case_when(\n      munro == \"A' Chraileag [A' Chralaig]\" ~ \"A' Chralaig\",\n      munro == \"Beinn Challuim [Ben Challum]\" ~ \"Ben Challum\",\n      munro == \"Beinn Sheasgarnaich [Beinn Heasgarnich]\" ~ \"Beinn Heasgarnich\",\n      munro == \"Beinn a' Bhuird North Top\" ~ \"Beinn a' Bhùird\",\n      munro == \"Ben Klibreck - Meall nan Con\" ~ \"Ben Klibreck\",\n      munro == \"Blabheinn [Bla Bheinn]\" ~ \"Blà Bheinn\",\n      munro == \"Cac Carn Beag (Lochnagar)\" ~ \"Lochnagar\",\n      munro == \"Carn Eighe\" ~ \"Càrn Eige\",\n      munro == \"Carn a' Choire Bhoidheach\" ~ \"Càrn a' Choire Bhòidheach\",\n      munro == \"Creag a' Mhaim\" ~ \"Creag a'Mhàim\",\n      munro == \"Glas Leathad Mor (Ben Wyvis)\" ~ \"Ben Wyvis\",\n      munro == \"Leabaidh an Daimh Bhuidhe (Ben Avon)\" ~ \"Ben Avon\",\n      munro == \"Meall Garbh\" ~ \"Meall Garbh (Ben Lawers)\",\n      munro == \"Meall na Aighean\" ~ \"Creag Mhòr (Meall na Aighean)\",\n      munro == \"Sgurr Dearg - Inaccessible Pinnacle\" ~ \"Inaccessible Pinnacle\",\n      munro == \"Sgurr Mhor (Beinn Alligin)\" ~ \"Sgùrr Mòr (Beinn Alligin)\",\n      munro == \"Sgurr na h-Ulaidh [Sgor na h-Ulaidh]\" ~ \"Sgòr na h-Ulaidh\",\n      munro == \"Sgurr nan Ceathramhnan [Sgurr nan Ceathreamhnan]\" ~ \"Sgùrr nan Ceathreamhnan\",\n      munro == \"Stob Coir' an Albannaich\" ~ \"Stob Coir an Albannaich\",\n      munro == \"Stuc an Lochain [Stuchd an Lochain]\" ~ \"Stùcd an Lochain\",\n      munro == \"Càrn nan Gobhar (Strathfarrar)\" ~ \"Càrn nan Gobhar (Loch Mullardoch)\",\n      TRUE ~ munro\n    )\n  )\n\n\nAfter standardising the names, to facilitate the join I also had to convert to lower case, remove accents (whether they’re used differs between the datasets), and any parenthesis information from the Munro names.\nAfter this cleaning, because multiple Munros have the same name, I needed to join on height to distinguish them as thankfully, there aren’t two Munros with the same name and height. However, a problem I wasn’t anticipating is that height measurements differed between the datasets. A lot of these can be put down to rounding - walkhighlands uses whole numbers whilst the DoBIH uses two decimal places. However, this doesn’t explain them all (the largest difference is 8.1 metres, that’s a lot!). I don’t know which one is “correct” or why they differ but given DoBIH is numerically more precise, I decided to use that as my measure of height in any analysis.\nMy AI-fuelled discovery was fuzzyjoin which allows you to set a tolerance level for the join and pick a best match. I’ve never needed this before but it provided itself to be extremely useful - with a bit of trial and error I set a tolerance of 10m and manually checked the output to ensure everything had lined up correctly.\nUPDATE: Because walkhighlands are great (honestly go and give them some money), they have updated the heights based on the DoBIH data so they now match (is this impact?).\n\n\nShow code\n# 0) Choose a tolerance in metres (use Inf if you want “nearest regardless”)\ntol_m &lt;- 10\n\n# 1) Normalise names in BOTH tables: remove (...) and [...], drop accents, lower-case, squish\nx &lt;- walkhighlands %&gt;%\n  mutate(\n    munro_key = munro %&gt;%\n      str_replace_all(\"\\\\s*\\\\([^)]*\\\\)\", \"\") %&gt;%   # remove text in ( )\n      str_replace_all(\"\\\\s*\\\\[[^\\\\]]*\\\\]\", \"\") %&gt;% # remove text in [ ]\n      stri_trans_general(\"Latin-ASCII\") %&gt;%\n      str_to_lower() %&gt;%\n      str_squish(),\n    height = parse_number(as.character(height)),\n    row_id_x = row_number()\n  )\n\ny &lt;- scottish_munros %&gt;%\n  mutate(\n    munro_key = munro %&gt;%\n      str_replace_all(\"\\\\s*\\\\([^)]*\\\\)\", \"\") %&gt;%\n      str_replace_all(\"\\\\s*\\\\[[^\\\\]]*\\\\]\", \"\") %&gt;%\n      stri_trans_general(\"Latin-ASCII\") %&gt;%\n      str_to_lower() %&gt;%\n      str_squish(),\n    height = parse_number(as.character(height)),\n    row_id_y = row_number()\n  )\n\n# 2) Fuzzy FULL join on exact munro_key + height within tolerance\ncandidates &lt;- fuzzy_full_join(\n  x, y,\n  by = c(\"munro_key\" = \"munro_key\", \"height\" = \"height\"),\n  match_fun = list(`==`, function(a, b) abs(a - b) &lt;= tol_m)\n) %&gt;%\n  # standardise suffixes for older fuzzyjoin that uses .x/.y\n  rename_with(~ str_replace(.x, \"\\\\.x$\", \"_wh\")) %&gt;%\n  rename_with(~ str_replace(.x, \"\\\\.y$\", \"_dobih\")) %&gt;%\n  mutate(\n    height_diff = abs(height_wh - height_dobih),\n    height_diff = if_else(is.na(height_diff), Inf, height_diff)\n  )\n\n# 3) Reduce to one nearest match per row on each side, preserving FULL-join behaviour\nbest_for_left  &lt;- candidates %&gt;% \n  group_by(row_id_x) %&gt;% \n  slice_min(height_diff, with_ties = FALSE) %&gt;% \n  ungroup()\nbest_for_right &lt;- candidates %&gt;% \n  group_by(row_id_y) %&gt;% \n  slice_min(height_diff, with_ties = FALSE) %&gt;% \n  ungroup()\n\nmunro_dat &lt;- bind_rows(best_for_left, best_for_right) %&gt;%\n  distinct(row_id_x, row_id_y, .keep_all = TRUE) %&gt;%\n  mutate(\n    site_key = coalesce(grid_ref, paste0(xcoord, \"_\", ycoord)),\n    munro_wh_clean = str_replace(munro_wh, \"\\\\s*\\\\([^)]*\\\\)$\", \"\")  # drop \"(Loch Mullardoch)\" etc.\n  ) %&gt;%\n  group_by(site_key) %&gt;%\n  slice_min(height_diff, with_ties = FALSE) %&gt;%   # keep the single closest pair for that site\n  ungroup() %&gt;%\n  mutate(munro_wh = munro_wh_clean) %&gt;%\n  select(-munro_wh_clean) |&gt;\n  mutate(time = (time_hours_min + time_hours_max) / 2) |&gt;\n  select(-munro_dobih,-munro_key_dobih) |&gt;\n  rename(munro = munro_wh)|&gt;\n  mutate(scramble_exposed = case_when(\n    scramble & exposed ~ \"Both\",\n    scramble & !exposed ~ \"Scramble\",\n    !scramble & exposed ~ \"Exposed\",\n    TRUE ~ \"Neither\"\n  )) |&gt;\n  mutate(scramble_exposed = factor(scramble_exposed,\n                                   levels = c(\"Neither\", \"Scramble\", \"Exposed\", \"Both\"))) |&gt;\n    mutate(wet = case_when(\n    spate & bog ~ \"Both\",\n    spate & !bog ~ \"Large river\",\n    !spate & bog ~ \"Boggy\",\n    TRUE ~ \"Neither\"\n  )) |&gt;\n  mutate(wet = factor(wet,\n                                   levels = c(\"Neither\", \"Large river\", \"Boggy\", \"Both\")))\n\n\n\nrm(x,y, best_for_left, best_for_right, candidates, tol_m, walkhighlands, scottish_munros)\n\nwrite_csv(x = munro_dat, \"munros_combined.csv\")\n\n\nI also created some manual colour scales on a nature theme:\n\n\nShow code\nnature_6 &lt;- c(\n  \"#355E3B\",  \n  \"#4B4F58\",  \n  \"#4682B4\",  \n  \"#8E6C88\",  \n  \"#E07B39\",\n  \"#BDB76B\"\n)\n\nnature_5 &lt;- c(\n  \"#355E3B\",  \n  \"#4B4F58\",  \n  \"#4682B4\",  \n  \"#E07B39\",\n  \"#BDB76B\"\n)\n\nnature_4 &lt;- c(\n  \"#355E3B\",  \n  \"#4B4F58\",  \n  \"#4682B4\",  \n  \"#E07B39\"   \n)\n\nnature_13 &lt;- c(\n  \"#355E3B\",  # Pine green\n  \"#6B8E23\",  # Moss\n  \"#BDB76B\",  # Dry grass\n  \"#8B5A2B\",  # Earth brown\n  \"#D2B48C\",  # Sand\n  \"#87CEEB\",  # Sky blue\n  \"#4682B4\",  # Loch blue\n  \"#191970\",  # Mountain shadow (midnight blue)\n  \"#7D7D7D\",  # Granite grey\n  \"#A9A9A9\",  # Slate grey\n  \"#8E6C88\",  # Heather purple\n  \"#E07B39\",  # Sunset orange\n  \"#FFD700\"   # Sun yellow\n)"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#routes-vs-munros",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#routes-vs-munros",
    "title": "Munro Tidy Tuesday",
    "section": "Routes vs Munros",
    "text": "Routes vs Munros\nFor some of the analyses later on, it makes more sense to do the analysis by route rather than by individual mountains - one route can have multiple Munros and the time, distance, and total ascent is for the entire route, not individually.\nThere are 282 Munros, but these are covered by 152 walkhighlands routes.\nBecause the user ratings and number of ascents are done by individual Munro rather than route, I also had to calculate the average for these for the route. It won’t be completely accurate because you don’t have to climb all the Munros on a given route, but given that I am just dicking about on the internet, that’s probably fine.\n\n\nShow code\nroute_dat &lt;-munro_dat |&gt;\n  select(first_route_url, first_route_title,\n         region, time_hours_min:deer_fence, time, scramble_exposed, wet) |&gt;\n  unique()\n\nroute_stats &lt;- munro_dat |&gt;\n  group_by(first_route_url) |&gt;\n  summarise(route_rating = janitor::round_half_up(mean(rating), 2),\n            route_ascents = janitor::round_half_up(mean(ascents),0 ))\n\nroute_dat &lt;- route_dat |&gt;\n  left_join(route_stats)"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#scrambling-and-exposure",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#scrambling-and-exposure",
    "title": "Munro Tidy Tuesday",
    "section": "Scrambling and exposure",
    "text": "Scrambling and exposure\nI have a reasonably bad fear of heights so anything that mentions scrambling or exposure worries me. First question, are higher munros scarier?\nBig thank you to Jessica Moore for the inspiration for this one.\n\n\nShow code\nggplot(munro_dat, \n       aes(height_dobih, scramble_exposed, fill = scramble_exposed)) +\n  geom_density_ridges(quantile_lines = TRUE, quantile_fun = mean,\n                      vline_linetype = \"dashed\",\n                      aes(color = \"Mean height (m)\")) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_color_manual(values = c(\"Mean height (m)\" = \"black\")) +\n  theme_economist() +\n  scale_fill_manual(values = nature_4) +  \n  labs(x = NULL, y = NULL,\n       title = \"Are higher Munros scarier?\",\n       colour = NULL,\n       subtitle = \"Routes descriptions that mention exposure tend to be on higher Munros\")+\n  guides(fill = \"none\") +\n  theme(legend.position = \"bottom\",\n        legend.position.inside = c(0.8,0.10))\n\n\n\n\n\n\n\n\n\nFor locating the scary munros, I decided that an interactive plotly map was called for so that you can easily isolate the different types - these work much better on a full browser than a phone.\nHave I mentioned that I dislike heights and exposure?\n\n\nShow code\nmunros_map &lt;- munro_dat %&gt;%\n  select(munro, region, xcoord, ycoord, height_dobih, scramble_exposed, wet, toilet, pub, bothy, deer_fence, car_park) %&gt;%\n  na.omit()\n\n# Convert OSGB36 coordinates to sf object\nmunros_sf &lt;- munros_map %&gt;%\n  st_as_sf(coords = c(\"xcoord\", \"ycoord\"), \n           crs = 27700)  # EPSG:27700 is OSGB36 / British National Grid\n\n# Transform to WGS84 (lat/long) for easier plotting\nmunros_lat_long &lt;- munros_sf %&gt;%\n  st_transform(crs = 4326)\n\n# Extract coordinates for ggplot\nmunros_coords &lt;- munros_lat_long %&gt;%\n  mutate(\n    longitude = st_coordinates(.)[,1],\n    latitude = st_coordinates(.)[,2]\n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(-height_dobih)\n\n\nuk_map &lt;- rnaturalearth::ne_countries(scale = \"large\", \n                                  country = \"United Kingdom\", \n                                    returnclass = \"sf\")\n\n# Step 2: Specify shape codes (16 = circle, 17 = triangle, etc.)\nshape_values &lt;- c(\n  \"Neither\" = 16,    # filled circle\n  \"Scramble\" = 17,   # filled triangle\n  \"Exposed\" = 15,    # filled square\n  \"Both\" = 18        # filled diamond\n)\n\np &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56.5, 58.6)) +\n  geom_jitter(data = munros_coords, \n             aes(x = longitude, \n                 y = latitude, \n                 shape = scramble_exposed, \n                 colour = scramble_exposed,\n                 text = munro), \n             size = 1,\n             height = .05,\n             width = .05) + \n  scale_x_continuous(breaks = NULL) +\n  scale_shape_manual(values = shape_values) +\n  scale_y_continuous(breaks = NULL) +\n  scale_colour_manual(values = nature_4) +\n  labs(title = \"Where are the scary Munros?\",\n       subtitle = \"Walk descriptions that reference:\",\n       colour = \"Route mentions\", shape = \"Route mentions\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#by-region",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#by-region",
    "title": "Munro Tidy Tuesday",
    "section": "By region",
    "text": "By region\nI also thought it would be fun to look into regional differences. The munros vary massively in character depending on where you are in the country (you can imagine hobbits living in Cairngorms whilst Skye would be home to dragons), but how is this reflected in the walk features?\n\nRegional map\nWhich Munros are in which region is taken from the walkhighlands A-Z but I’ve had a few people ask since posting this, so here’s an easy to read map.\n\n\nShow code\np3 &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56.5, 58.6)) +\n  geom_jitter(data = munros_coords, \n             aes(x = longitude, \n                 y = latitude, \n                 colour = region,\n                 text = munro), \n             size = 1,\n             height = .05,\n             width = .05) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  scale_colour_manual(values = nature_13) +\n  labs(title = \"Munros by region\",\n       colour = NULL) +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p3, tooltip = \"text\")\n\n\n\n\n\n\nAnd here’s the counts for how many Munros and routes are in each region. It’s highly correlated, but the ranks do change a little.\n\n\nShow code\nmunro_region &lt;- munro_dat |&gt;\n  count(region, sort = TRUE) |&gt;\n  rename(\"munros\" = \"n\") \n\nroute_region &lt;- route_dat |&gt;\n  count(region, sort = TRUE) |&gt;\n  rename(\"routes\" = \"n\")\n\ninner_join(munro_region, route_region) |&gt;\n  flextable()|&gt;\n  autofit()\n\n\nregionmunrosroutesFort William6738Cairngorms5432Perthshire2816Kintail2410Ullapool249Argyll1913Loch Ness177Torridon1712Loch Lomond149Isle of Skye1210Angus32Sutherland22Isle of Mull11\n\n\n\n\nTallest Munros\nWhich region has the tallest Munros?\n\n\nShow code\nregion_height &lt;- munro_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_height = mean(height_dobih, na.rm = TRUE), .groups = \"drop\") |&gt;\n  slice_max(avg_height, n = 5)\n\nggplot(\n  semi_join(munro_dat, region_height, by = \"region\"),\n  aes(\n    x = height_dobih,\n    y = fct_reorder(region, height_dobih, .fun = mean, .desc = FALSE),\n    fill = region\n  )\n)  +\n  geom_density_ridges(\n    quantile_lines = TRUE, quantile_fun = mean,\n    vline_linetype = \"dashed\",\n    aes(colour = \"Mean height (m)\")\n  ) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_colour_manual(values = c(\"Mean height (m)\" = \"black\")) +\n  theme_economist() +\n  scale_fill_manual(values = nature_5) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Which region has the highest Munros?\",\n    colour = NULL,\n    subtitle = \"Top 5 regions displayed. On average, Loch Ness has the highest Munros\"\n  ) +\n  guides(fill = \"none\") +\n  theme(\n    legend.position = \"inside\",\n   legend.position.inside = c(0.8, 0.1),\n    legend.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nAnd fere’s the full list in table form:\n\n\nShow code\nmunro_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_height = mean(height_dobih, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_height)) |&gt;\n  flextable() |&gt;\n  colformat_double(digits = 0)|&gt;\n  autofit()\n\n\nregionavg_heightLoch Ness1,064Cairngorms1,052Perthshire1,031Fort William1,026Argyll1,011Kintail1,004Loch Lomond989Ullapool986Torridon979Isle of Mull966Isle of Skye953Sutherland945Angus938\n\n\n\n\nLongest routes\nWhich region has the longest routes on average by distance?\nThis plot uses the route data although I am pleased that the order remains the same if you use the munro data (because I only realised I should do it by route after I shared this publicly).\n\n\nShow code\n# Top 5 regions by mean distance\nregion_distance &lt;- route_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_distance = mean(distance_km, na.rm = TRUE), .groups = \"drop\") |&gt;\n  slice_max(avg_distance, n = 5)\n\n# Keep only those regions in the raw data\ntop_dat &lt;- semi_join(route_dat, region_distance, by = \"region\")\n\n\nggplot(\n  top_dat,\n  aes(\n    x = distance_km,  # use the per-route variable here\n    y = fct_reorder(region, distance_km, .fun = mean, .desc = FALSE),\n    fill = region\n  )\n) +\n  geom_density_ridges(\n    quantile_lines = TRUE, quantile_fun = mean,\n    vline_linetype = \"dashed\",\n    aes(colour = \"Mean distance (km)\")\n  ) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_colour_manual(values = c(\"Mean distance (km)\" = \"black\")) +\n  theme_economist() +\n  scale_fill_manual(values = nature_5) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Which region has the longest walks?\",\n    colour = NULL,\n    subtitle = \"Top 5 regions displayed. On average, Loch Ness has the longest walks\"\n  ) +\n  guides(fill = \"none\") +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.82, .1),\n    legend.text = element_text(size = 10)\n    )\n\n\n\n\n\n\n\n\n\nAnd here’s the full list in table form:\n\n\nShow code\nroute_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_distance = mean(distance_km, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_distance)) |&gt;\n  flextable() |&gt;\n  colformat_double(digits = 2)|&gt;\n  autofit()\n\n\nregionavg_distanceLoch Ness22.96Cairngorms22.59Ullapool21.44Torridon17.73Fort William16.99Perthshire16.30Argyll16.21Angus16.00Kintail15.54Loch Lomond13.00Sutherland10.88Isle of Skye10.85Isle of Mull9.25\n\n\n\n\nMost ascent\nWhich region has the most ascent per route?\nAgain the order doesn’t change if you use routes or Munros as the data (the exact values do slightly but not the rank).\n\n\nShow code\n# Top 5 regions by mean distance\nregion_ascent &lt;- munro_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_ascent = mean(ascent, na.rm = TRUE), .groups = \"drop\") |&gt;\n  slice_max(avg_ascent, n = 5)\n\n# Keep only those regions in the raw data\ntop_ascent &lt;- semi_join(munro_dat, region_ascent, by = \"region\")\n\n\nggplot(\n  top_ascent,\n  aes(\n    x = ascent,  # use the per-route variable here\n    y = fct_reorder(region, ascent, .fun = mean, .desc = FALSE),\n    fill = region\n  )\n) +\n  geom_density_ridges(\n    quantile_lines = TRUE, quantile_fun = mean,\n    vline_linetype = \"dashed\",\n    aes(colour = \"Mean ascent (m)\")\n  ) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_colour_manual(values = c(\"Mean ascent (m)\" = \"black\")) +\n  theme_economist() +\n  scale_fill_manual(values = nature_5) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Which region has the most ascent?\",\n    colour = NULL,\n    subtitle = \"Top 5 regions displayed. On average, Loch Ness has the most ascent\"\n  ) +\n  guides(fill = \"none\") +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.82, .1),\n    legend.text = element_text(size = 10)  \n    )\n\n\n\n\n\n\n\n\n\nAnd here’s the full list in table form:\n\n\nShow code\nroute_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_ascent = mean(ascent, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_ascent)) |&gt;\n  flextable() |&gt;\n  colformat_double(digits = 0)|&gt;\n  autofit()\n\n\nregionavg_ascentLoch Ness1,479Ullapool1,314Kintail1,291Fort William1,261Argyll1,158Torridon1,105Loch Lomond1,080Isle of Skye1,042Perthshire969Cairngorms950Isle of Mull945Sutherland938Angus758\n\n\n\n\nMunros per route\nWhere do you get the most bang for your buck?\n\n\nShow code\nmunro_dat |&gt;\n  count(region, first_route_title, name = \"n\") |&gt;\n  group_by(region) |&gt;\n  summarise(avg_count = mean(n), .groups = \"drop\") |&gt;\n  ggplot(aes(\n    x = fct_reorder(region, avg_count),\n    y = avg_count,\n    fill = region\n  )) +\n  geom_col() +\n  scale_fill_manual(values = nature_13) +\n  coord_flip() +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = \"Number of Munros\",\n       title = \"Average number of Munros bagged per route\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\nUser ratings\nWhich region has the highest and lowest rated Munros (by user rating)?\n\n\nShow code\nregion_rating_top &lt;- munro_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_rating = mean(rating, na.rm = TRUE), \n            n = n(),\n            .groups = \"drop\") |&gt;\n  filter(n &gt;= 5)|&gt;\n  slice_max(avg_rating, n = 3)\n\n# bottom 3\n\nregion_rating_bottom &lt;- munro_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_rating = mean(rating, na.rm = TRUE), \n            n = n(),\n            .groups = \"drop\") |&gt;\n  filter(n &gt;= 5)|&gt;\n  slice_min(avg_rating, n = 3)\n\nregion_top_bottom &lt;-bind_rows(region_rating_top, region_rating_bottom)\n\nggplot(\n  semi_join(munro_dat, region_top_bottom, by = \"region\"),\n  aes(\n    x = rating,\n    y = fct_reorder(region, rating, .fun = mean, .desc = FALSE),\n    fill = region\n  )\n)  +\n  geom_density_ridges(\n    quantile_lines = TRUE, quantile_fun = mean,\n    vline_linetype = \"dashed\",\n    aes(colour = \"Mean height (m)\")\n  ) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_colour_manual(values = c(\"Mean height (m)\" = \"black\")) +\n  theme_economist() +\n  scale_fill_manual(values = nature_6) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Which region has the highest user rated Munros?\",\n    colour = NULL,\n    subtitle = \"Top & bottom 3 (regions w/ 5+ Munros)\"\n  ) +\n  guides(fill = \"none\") +\n  theme(\n    legend.position = \"inside\",\n   legend.position.inside = c(0.87, 0.1),\n    legend.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nAnd here’s the full list in table form:\n\n\nShow code\nmunro_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_rating = mean(rating, na.rm = TRUE), \n            \"no. munros\" = n(),.groups = \"drop\") |&gt;\n  arrange(desc(avg_rating)) |&gt;\n  flextable() |&gt;\n  colformat_double(digits = 2)|&gt;\n  autofit()\n\n\nregionavg_ratingno. munrosIsle of Skye4.4112Isle of Mull4.081Torridon3.9517Fort William3.7367Kintail3.6924Ullapool3.6724Sutherland3.582Argyll3.5219Loch Lomond3.4414Loch Ness3.3817Perthshire3.2628Angus3.253Cairngorms3.1154\n\n\n\n\nAscents by rating\nNumber of ascents (how many people have recorded on walkhighlands that they have submitted a particular Munro) by ratings\n\n\nShow code\nggplot(route_dat, aes(x = route_ascents, y = route_rating)) +\n  geom_point(aes(text = first_route_url)) +\n  theme_economist() +\n  scale_y_continuous(breaks = seq(1:5)) +\n  coord_cartesian(ylim = c(1,5)) +\n  labs(y = \"User rating (1-5)\",\n       x = \"Number of recorded ascents\",\n       title = \"Number of ascents by rating\")+\n  annotate(geom = \"curve\", \n          x = 31000, y = 3, \n          xend = 31800, yend = 3.75,\n          curvature = 0.3,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 31000, y = 2.8,\n           label = \"Ben Lomond\")+\n  annotate(geom = \"curve\", \n          x = 23000, y = 1.45, \n          xend = 18900, yend = 2.30,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 28000, y = 1.45,\n           label = \"The Cairnwell Munros\") +\n  annotate(geom = \"curve\", \n          x = 5559, y = 1.5, \n          xend = 5559, yend = 4.61,\n          curvature = -0.55,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 7600, y = 1.35,\n           label = \"Sgùrr nan Gillean\")\n\n\n\n\n\n\n\n\n\nAnd then as an interactive plot:\n\n\nShow code\np5 &lt;- ggplot(route_dat, aes(x = route_ascents, y = route_rating)) +\n  geom_point(aes(text = first_route_title)) +\n  theme_economist() +\n  scale_y_continuous(breaks = seq(1:5)) +\n  coord_cartesian(ylim = c(1,5)) +\n  labs(y = \"User rating (1-5)\",\n       x = \"Number of recorded ascents\",\n       title = \"Number of ascents per route by rating\")\n\nggplotly(p5)"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#ascent",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#ascent",
    "title": "Munro Tidy Tuesday",
    "section": "Ascent",
    "text": "Ascent\nAs you would expect, total ascent correlates strongly with total time, but there are some points of interest. Bidein a’ Choire Sheasgaich and Lurg Mhòr have the longest estimated walk time but their remoteness means that they’re an outlier in terms of the amount of ascent you’d expect for that time. The Fisherfield 6 claim the prize for most ascent by some distance but are bang on in terms of the ascent/time relationship. Meall Buidhe wins the award for being the quickest munro to bag, with the least ascent.\n\n\nShow code\nggplot(route_dat, aes(x = ascent, y = time)) +\n  geom_jitter(height = .05, width = .05) +\n  scale_x_continuous(breaks = seq(500, 2500, 250)) +\n  scale_y_continuous(breaks = seq(0, 20, 2)) +\n  annotate(geom = \"curve\", \n          x = 1200, y = 15, \n          xend = 1450, yend = 16,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 1150, y = 14.5,\n           label = \"Bidein a' Choire Sheasgaich\\nand Lurg Mhòr\") +\n    annotate(geom = \"curve\", \n          x = 2050, y = 13.5, \n          xend = 2220, yend = 15,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 2100, y = 13,\n           label = \"Fisherfield 6\")+\n    annotate(geom = \"curve\", \n          xend = 575, yend = 2.9, \n          x = 1250, y = 4,\n          curvature = 0.2,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 1450, y = 4.5,\n           label = \"Meall Buidhe\")+\n  theme_economist()  +\n  labs(x = \"Ascent (m)\", \n       y = \"Time (hours)\",\n       title = \"Total ascent by time\") +\n    theme(legend.position = \"inside\",\n    axis.title.x = element_text(margin = margin(t = 8)),\n    axis.title.y = element_text(margin = margin(r = 8)),\n    legend.position.inside = c(0.9, .2),\n    legend.text = element_text(size = 10)  \n  )\n\n\n\n\n\n\n\n\n\nAnd here’s an interactive version of that plot that adds in scrambling and exposure because had I mentioned, I am scared of heights.\n\n\nShow code\np1 &lt;- ggplot(route_dat, aes(x = ascent, y = time)) +\n  geom_jitter(aes(shape = scramble_exposed, \n                 colour = scramble_exposed,\n                 text = first_route_title), \n             size = 1) +\n  scale_x_continuous(breaks = seq(500,2500, 250)) +\n  scale_y_continuous(breaks = seq(0,20,2)) +\n  scale_shape_manual(values = shape_values) +\n  scale_colour_manual(values= nature_4) +\n  labs(x = \"Ascent (m)\", \n       y = \"Time (hours)\",\n       title = \"Ascent by time\",\n       shape = \"Route mentions\",\n       colour = \"Route mentions\") +\n  theme_economist() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 8)),\n    axis.title.y = element_text(margin = margin(r = 8)),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p1, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#wheres-wet",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#wheres-wet",
    "title": "Munro Tidy Tuesday",
    "section": "Where’s wet?",
    "text": "Where’s wet?\nIf the text-mining for scrambling and exposure is a blunt tool then my approach here is even blunter. There are multiple words I could have searched for regarding the presence of water - river, stream, burn - but many of those represent features that don’t make a difference to the walk if they present no difficulty (“cross the bridge over the river”). I decided to use the word “spate” because when the river is large, walkhighlands often highlights that it would be difficult or impossible to cross “in spate”.\nSo these aren’t all the rivers, just ones where the description indicates crossing them might present an issue.\n\n\nShow code\nshape_values &lt;- c(\n  \"Neither\" = 16,    # filled circle\n  \"River\" = 17,   # filled triangle\n  \"Boggy\" = 15,    # filled square\n  \"Both\" = 18        # filled diamond\n)\n\np2 &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56.5, 58.6)) +\n  geom_jitter(data = munros_coords, \n             aes(x = longitude, \n                 y = latitude, \n                 shape = wet, \n                 colour = wet,\n                 text = munro), \n             size = 1,\n             height = .05,\n             width = .05) + \n  scale_x_continuous(breaks = NULL) +\n  scale_shape_manual(values = shape_values) +\n  scale_y_continuous(breaks = NULL) +\n  scale_colour_manual(values = nature_4) +\n  guides(shape = \"none\") +\n  labs(title = \"Where is wet?\\n(Everywhere, it's Scotland)\",\n              subtitle = \"Walk descriptions that reference:\",\n       colour = \"Route mentions\", shape = \"Route mentions\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p2, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#non-natural-features",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#non-natural-features",
    "title": "Munro Tidy Tuesday",
    "section": "Non-natural features",
    "text": "Non-natural features\nWith the usual caveats about the limits of text mining, here’s some maps showing route descriptions that mention non-natural features (bothy, deer fence (because they’re scary), pub, toilet). Some of these are more useful than others.\n\nBothy\n\n\nShow code\nbothy_plot &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56, 58.6)) +\n  geom_jitter(data = filter(munros_coords, bothy == TRUE), \n             aes(x = longitude, \n                 y = latitude, \n                 colour = bothy,\n                 text = munro), \n             size = 1) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  labs(title = \"Routes that mention a bothy\") +\n  guides(colour = \"none\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(bothy_plot, tooltip = \"text\")\n\n\n\n\n\n\n\n\nDeer fences\nI really hate deer fences.\n\n\nShow code\nfence_plot &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56, 58.6)) +\n  geom_jitter(data = filter(munros_coords, deer_fence == TRUE), \n             aes(x = longitude, \n                 y = latitude, \n                 colour = deer_fence,\n                 text = munro), \n             size = 1) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  labs(title = \"Routes that mention a deer fence\") +\n  guides(colour = \"none\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(fence_plot, tooltip = \"text\")\n\n\n\n\n\n\n\n\nToilets\nWhat I learned making this plot is that I cannot spell toilet. I lost an hour of my life to realising the error was coming from “toliet”.\n\n\nShow code\ntoilet_plot &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56, 58.6)) +\n  geom_jitter(data = filter(munros_coords, toilet == TRUE), \n             aes(x = longitude, \n                 y = latitude, \n                 colour = toilet,\n                 text = munro), \n             size = 1) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  labs(title = \"Routes that mention a toilet\") +\n  guides(colour = \"none\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(toilet_plot, tooltip = \"text\")\n\n\n\n\n\n\n\n\nPubs\nThis one wasn’t really worth doing given how few routes mention a pub, but for Sandie, anything.\n\n\nShow code\npub_plot &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56, 58.6)) +\n  geom_jitter(data = filter(munros_coords, pub == TRUE), \n             aes(x = longitude, \n                 y = latitude, \n                 colour = pub,\n                 text = munro), \n             size = 1) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  labs(title = \"Routes that mention a pub\") +\n  guides(colour = \"none\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(pub_plot, tooltip = \"text\")\n\n\n\n\n\n\n\n\nCar parks\n\n\nShow code\ncar_plot &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56, 58.6)) +\n  geom_jitter(data = filter(munros_coords, car_park == TRUE), \n             aes(x = longitude, \n                 y = latitude, \n                 colour = pub,\n                 text = munro), \n             size = 1) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  labs(title = \"Routes that mention a car park\") +\n  guides(colour = \"none\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(car_plot, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#best-and-worst-munros",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#best-and-worst-munros",
    "title": "Munro Tidy Tuesday",
    "section": "Best and worst Munros",
    "text": "Best and worst Munros\nI was asked by Kate Gilliver via Bluesky “which Munro is the ultimate combination of high, exposed, scary, remote, boggy, rivery and generally to be avoided?”\nSo with the following criteria:\n\nDoesn’t mention bog, river, scramble, exposed, or a deer fence\nTakes 6 hours or less\n\nYour top 5 choices by user rating are…..\n\n\nShow code\nroute_dat |&gt;\n  filter(bog == FALSE,\n         river == FALSE,\n         scramble == FALSE,\n         exposed == FALSE,\n         deer_fence == FALSE,\n         time &lt;= 6) |&gt;\n  arrange(desc(route_rating)) |&gt;\n  select(route = first_route_title, region, route_rating) |&gt;\n  slice(1:5) |&gt;\n  flextable()|&gt;\n  autofit()\n\n\nrouteregionroute_ratingBuachaille Etive BeagFort William4.06Ben Lomond from RowardennanLoch Lomond3.82Ben Hope from StrathmoreSutherland3.81Schiehallion from the Braes of FossPerthshire3.60Ben Vorlich from Inveruglas via Loch SloyLoch Lomond3.49\n\n\nIf you want a scary but dry hike:\n\nNo bog or river\nBut scramble and exposed\nTop 5 by user rating\n\n\n\nShow code\nroute_dat |&gt;\n  filter(bog == FALSE,\n         river == FALSE,\n         scramble == TRUE,\n         exposed == TRUE) |&gt;\n  arrange(desc(route_rating)) |&gt;\n  select(route = first_route_title, region, route_rating) |&gt;\n  slice(1:5) |&gt;\n  flextable()|&gt;\n  autofit()\n\n\nrouteregionroute_ratingSgùrr Mhic Chòinnich from Coire LaganIsle of Skye4.61Sgùrr Dearg and the approach to the Inn PinnIsle of Skye4.60Liathach traverse, Glen TorridonTorridon4.57Beinn Eighe via Coire Mhic FhearchairTorridon4.50The Aonach Eagach ridge from GlencoeFort William4.47\n\n\nAnd if you want to hate yourself:\n\nBog and river\nMore than 6 hours\nMore than 1000 metres of ascent\nLowest rated by users\n\n\n\nShow code\nroute_dat |&gt;\n  filter(bog == TRUE,\n         river == TRUE,\n         time &gt;6,\n         ascent &gt; 1000) |&gt;\n  arrange(route_rating) |&gt;\n  select(route = first_route_title, region, route_rating) |&gt;\n  slice(1:5) |&gt;\n  flextable()|&gt;\n  autofit()\n\n\nrouteregionroute_ratingSgiath Chùil and Meall Glas from Glen DochartLoch Lomond2.73An Scarsoch and Càrn an Fhìdhleir from the Linn of DeeCairngorms2.84Beinn a' Chaorainn & Beinn Teallach from Glen SpeanFort William2.87Glas Tulaichean and Càrn an Rìgh from the Spittal of GlensheePerthshire2.89Maoile Lunndaidh from AchnashellachTorridon3.15\n\n\nI may have become a bit obsessed.\nI must stop.\nBut if you have any other suggestions for analysis….just ask."
  },
  {
    "objectID": "post/2025-08-31-munro-tidy-tuesday/index.en.html#ascent-m",
    "href": "post/2025-08-31-munro-tidy-tuesday/index.en.html#ascent-m",
    "title": "Munro Tidy Tuesday",
    "section": "Ascent (m)",
    "text": "Ascent (m)\nAs you would expect, total ascent correlates strongly with total time, but there are some points of interest. Bidein a’ Choire Sheasgaich and Lurg Mhòr have the longest estimated walk time but their remoteness means that they’re an outlier in terms of the amount of ascent you’d expect for that time. The Fisherfield 6 claim the prize for most ascent by some distance but are bang on in terms of the ascent/time relationship. Meall Buidhe wins the award for being the quickest munro to bag, with the least ascent.\n\n\nShow code\nggplot(route_dat, aes(x = ascent, y = time)) +\n  geom_jitter(height = .05, width = .05) +\n  scale_x_continuous(breaks = seq(500, 2500, 250)) +\n  scale_y_continuous(breaks = seq(0, 20, 2)) +\n  annotate(geom = \"curve\", \n          x = 1200, y = 15, \n          xend = 1450, yend = 16,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 1150, y = 14.5,\n           label = \"Bidein a' Choire Sheasgaich\\nand Lurg Mhòr\") +\n    annotate(geom = \"curve\", \n          x = 2050, y = 13.5, \n          xend = 2220, yend = 15,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 2100, y = 13,\n           label = \"Fisherfield 6\")+\n    annotate(geom = \"curve\", \n          xend = 575, yend = 2.9, \n          x = 1250, y = 4,\n          curvature = 0.2,\n          arrow = arrow(length = unit(0.5, \"lines\"))) +\n    annotate(\"text\",\n           x = 1450, y = 4.5,\n           label = \"Meall Buidhe\")+\n  theme_economist()  +\n  labs(x = \"Ascent (m)\", \n       y = \"Time (hours)\",\n       title = \"Total ascent by time\") +\n    theme(legend.position = \"inside\",\n    axis.title.x = element_text(margin = margin(t = 8)),\n    axis.title.y = element_text(margin = margin(r = 8)),\n    legend.position.inside = c(0.9, .2),\n    legend.text = element_text(size = 10)  \n  )\n\n\n\n\n\n\n\n\n\nAnd here’s an interactive version of that plot that adds in scrambling and exposure because had I mentioned, I am scared of heights.\n\n\nShow code\np1 &lt;- ggplot(route_dat, aes(x = ascent, y = time)) +\n  geom_jitter(aes(shape = scramble_exposed, \n                 colour = scramble_exposed,\n                 text = first_route_title), \n             size = 1) +\n  scale_x_continuous(breaks = seq(500,2500, 250)) +\n  scale_y_continuous(breaks = seq(0,20,2)) +\n  scale_shape_manual(values = shape_values) +\n  scale_colour_manual(values= nature_4) +\n  labs(x = \"Ascent (m)\", \n       y = \"Time (hours)\",\n       title = \"Ascent by time\",\n       shape = \"Route mentions\",\n       colour = \"Route mentions\") +\n  theme_economist() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 8)),\n    axis.title.y = element_text(margin = margin(r = 8)),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p1, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html",
    "href": "post/2025-09-17-schools-behaviour/index.html",
    "title": "Supporting positive behaviour",
    "section": "",
    "text": "I was grateful to have the opportunity to attend the Positive Relationships and Behaviour in Schools Scotland Conference in Edinburgh last week. My reason for attending is that as Convenor of Transitions Working Group and in partnership with our Student Conduct Team, we’re undertaking work to address an increase in behavioural issues we’ve seen over the last few years through our revised Code of Expected Behaviour. Particularly in our pre-honours classes, we’ve had more and more colleagues report students being disruptive, refusing to engage, and making unreasonable demands. We know we’re not the only ones, we know that the causes are as complex as they are varied, we also know that blaming young people or generational differences is as unhelpful as it is inaccurate, and we know that those students are still the overwhelming minority. But we also know that it’s not something we can put our head in the sand about and that for both staff and student wellbeing, we need to address it.\nGiven that the issues are concentrated (although I hasten to add not exclusive to) first year, it is important to me that we know what’s happening in schools to help better understand where our students are coming from, that our approaches align, and that schools and universities work in partnership. Whilst there were no easy answers, I did find the conference reassuring and I have a better sense of what we have to do, and why, so consider this blog me working through the thinking of my key takeaways."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#its-not-all-about-covid",
    "href": "post/2025-09-17-schools-behaviour/index.html#its-not-all-about-covid",
    "title": "Supporting positive behaviour",
    "section": "It’s not all about covid",
    "text": "It’s not all about covid\nGordon West highlighted the Behaviour in Scottish Schools Research (BISSR) 2023 which found a significant increase in both low level disruption and more serious incidents of violence and aggression. However, he also made the very good point that previous iterations of the survey were conducted in 2012 and 2016 and that there was already and upward trend and that whilst someof the 2023 increase could be explained by covid and lockdown, there is a seven year gap between the 2023 and 2016 survey and so part of the increase is simply that there is a longer time period between surveys."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#consequences",
    "href": "post/2025-09-17-schools-behaviour/index.html#consequences",
    "title": "Supporting positive behaviour",
    "section": "Consequences",
    "text": "Consequences\nThere were a number of references to the need for consequences. Gordon West presented the Scottish Advisory Group on Relationships and Behaviour in Schools Action Plan to ensure safe and consistent environments in schools. Action 2 is “Identify good practice in evidence-based relationships and behavior approaches, including consequences” whilst Louise Pirie spoken about needing clear consequences with application guidance and Stuart Clark highlighted the need for consequences to be highly predictable and offer an incentive to improve.\nThe focus of these discussion was not about taking a punitive approach but rather making sure that there is no ambiguity for either staff or students, that there are specific consequences with fair escalation, and that these expectations are written down so that everyone knows what’s expected and what happens if those expectations are not met.\nFor the HE context, we need to think through the consequences beyond reporting. For example, we are already required for disability reasons to provide lecture material in an alternative format (e.g., slides and recordings), a potential consequence for a student who is continually disruptive to the planned delivery of the class might be that they may only engage through the alternative format (i.e., asynchronous and self-directed learning). I am not sure this is necessarily the right answer but a really strong message was students learn very quickly when there are actually no consequences, so we need to think through our response in advance."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#process-matters",
    "href": "post/2025-09-17-schools-behaviour/index.html#process-matters",
    "title": "Supporting positive behaviour",
    "section": "Process matters",
    "text": "Process matters\nA common theme was the importance of having a clear mechanism and process to report incidents. The focus wasn’t on being punitive but rather that it is easier to figure out what the cause of behavioural problems might be if you have the right data to spot patterns. One of the examples given was that if you have reporting that includes the time of the incident, you can spot that a child is being disruptive at a certain point each day and this might indicate that they are having trouble with a particular transition. Louise Pirie also made the point that targeting and reporting predictable common issues and everyday disruptions can help prevent more serious issues from arising through early intervention.\nAs it relates to HE, this very much aligns with the work we’re going in trying to standardise reporting of low-level behavioral concerns. A student who is disruptive and refuses to engage in a single class because they’re having a bad day is very different to a student who is disruptive in every class they attend but because HE teaching is so spread out, without clear reporting mechanisms and joined-up data, we can’t pick up on this stuff."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#catch-a-child-being-good",
    "href": "post/2025-09-17-schools-behaviour/index.html#catch-a-child-being-good",
    "title": "Supporting positive behaviour",
    "section": "Catch a child being good",
    "text": "Catch a child being good\nThis one broke my heart. In a session on children with emotional needs, Kimberley Roan McVeigh asked the question “When was the last time we”caught” a young person being good?“. As Level 1 lead, I used to send my class emails about their attendance/engagement via Moodle logs but everyone got an email, not just the ones that were flagged, this is part of the the text the ones who were above average got:\n\nWe know that engagement in the first few weeks of term is highly predictive of achievement and wellbeing, I know you have had a lot going on so well done for working through everything. I also know that often when students just get on and do the work that you often feel like it’s not seen, so this is just to say that I see you and I see the effort you’re putting in.\n\nThe feedback I got about this email was so clear and consistent - many of them said it was the only time someone had told them they were doing a good job. And it made me think about what else we can do to not just monitor and manage disruptive behaviour, but to actively reward good behaviour. We may think that we shouldn’t need to do this in HE and that this should be left in high school, but I think that world hasn’t existed for a very long time."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#behaviour-as-communication",
    "href": "post/2025-09-17-schools-behaviour/index.html#behaviour-as-communication",
    "title": "Supporting positive behaviour",
    "section": "Behaviour as communication",
    "text": "Behaviour as communication\nDirectly related to the need to report was the theme of behaviour as communication - i.e., that all behaviour is the child/student trying to tell us something, intentionally or not. This could be that they don’t understand what is expected of them, that they have a disability or neurodivergence that is causing difficulties or not being adjusted for, or it could be cultural differences in the understanding of what’s expected in any given classroom setting. The reason that this is related to the need for clear reporting mechanism is as already stated, that data regarding patterns of behaviour makes it easier to identify what the underlying communication might be. It also reinforces that the process must be about support and development, rather than punishing poor behaviour."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#dont-forget-about-the-95",
    "href": "post/2025-09-17-schools-behaviour/index.html#dont-forget-about-the-95",
    "title": "Supporting positive behaviour",
    "section": "Don’t forget about the 95%",
    "text": "Don’t forget about the 95%\nAnother strong theme was the importance of ensuring that we don’t focus on the ones being disruptive to the detriment of the overwhelming majority who are following the rules and I know I have been guilty of this in the past. When you have one or two extremely difficult cases that cause a lot of upset, it’s hard not to see anything other than the problems. As it relates to managing classroom behaviour, this point was also related to the UN Convention on Rights of the Child in that whilst we need to be mindful of how we’re dealing with disruptive students, the other students in the room have a right to a safe environment and access to their education and so not dealing with it isn’t an option.\nStuart Clark spoke about the 95/5 Rule – what is excellent behaviour for almost all in a specific school setting? What are the highest standards 95-99% of learners could normally meet? And how do we then respond flexibly to the 1-5%? I think this translates quite well to HE in that we’re essentially saying that we want to be explicit and clear about our expectations whilst recognising that not all students can be held to those standards for a variety of reasons related to inclusion."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#the-impact-of-covid",
    "href": "post/2025-09-17-schools-behaviour/index.html#the-impact-of-covid",
    "title": "Supporting positive behaviour",
    "section": "The impact of covid",
    "text": "The impact of covid\nGordon West highlighted the Behaviour in Scottish Schools Research (BISSR) 2023 which found a significant increase in both low level disruption and more serious incidents of violence and aggression. However, he also made the very good point that previous iterations of the survey were conducted in 2012 and 2016 and that there was already and upward trend and that whilst some of the 2023 increase could be explained by covid and lockdown, there is a seven year gap between the 2023 and 2016 survey and so part of the increase is simply that there is a longer time period between surveys. I am not aware of any directly analogous work in HE although I would be surprised if the trends are not the same."
  },
  {
    "objectID": "post/2025-09-17-schools-behaviour/index.html#teachers-deserve-a-pay-rise",
    "href": "post/2025-09-17-schools-behaviour/index.html#teachers-deserve-a-pay-rise",
    "title": "Supporting positive behaviour",
    "section": "Teachers deserve a pay rise",
    "text": "Teachers deserve a pay rise\nAnd finally, a statement I don’t think anyone will disagree with. I was speaking to one of the other delegates and explaining why I was there given I work in HE and I was describing some of the worst, and most upsetting incidents we’ve had to deal with and I realised she was waiting for me to get to the truly bad bit. Everything I described as shocking to me as an HE educator she’d experienced many times before. Give them a pay rise (and some psychological support)."
  },
  {
    "objectID": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html",
    "href": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html",
    "title": "Pivot 1: Academic Serenity",
    "section": "",
    "text": "Grant me the serenity to accept the things I cannot change, courage to change the things I can, and wisdom to know the difference - Reinhold Niebuhr\nThis is the first in what I plan to be a weekly blog charting what I’ve been doing to redesign my courses for online/blended delivery. This isn’t going to be heavy on pedagogical arguments or citations, if you want that please see our preprint “10 Simple Rules for Supporting a Temporary online Pivot in Higher Education”. Instead, this is literally going to be a blog about what I’ve been doing - recording videos, updating assessment information, checking reading lists. I am a teaching-focused lecturer with experience of fully online delivery and whose research expertise is lecture capture and I still feel overwhelmed at the scale of the task in front of me so I figure that it might be helpful to others to read a concrete account of the nuts and bolts of what the pivot entails."
  },
  {
    "objectID": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html#the-course",
    "href": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html#the-course",
    "title": "Pivot 1: Academic Serenity",
    "section": "The course",
    "text": "The course\nTo give you some context, I am Year Lead for 1st year psychology at the University of Glasgow. Typically there’s about 500 students on the course, with just under half of them registered on a psychology programme and half who take it as an elective.\nIn the old normal, they would have three hour-long lectures a week delivered by a team of lecturers on various fields of psychology. In addition to the lectures, there was a two-hour practical lab once every two weeks in which the focus was on small-group work academic writing, data skills, and programming using R. The assessment was a final exam (with an essay and MCQ component), a coursework essay, and a lab portfolio of small-stakes MCQs, R worksheets, and a group presentation."
  },
  {
    "objectID": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html#teamwork-makes-the-dreamwork",
    "href": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html#teamwork-makes-the-dreamwork",
    "title": "Pivot 1: Academic Serenity",
    "section": "Teamwork makes the dreamwork",
    "text": "Teamwork makes the dreamwork\nThis is my blog and I will be writing about my experience but I want to make it clear up front that none of this is done alone. I’m lucky to have a fantastic team to work with at UofG Psychology and everything we’re doing is a result of extensive and ongoing team discussions and consultations with lecturers, tutors, GTAs and students."
  },
  {
    "objectID": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html#uncertainty",
    "href": "post/2020-07-13-pivot-1-academic-serenity/2020-07-13-pivot-1-academic-serenity.html#uncertainty",
    "title": "Pivot 1: Academic Serenity",
    "section": "Uncertainty",
    "text": "Uncertainty\nLike most (all?) of us, I find the uncertainty about what life in September will look like and what will be possible (or sensible) difficult and stressful to process. It’s like when I think about the size of the universe and the scale of it all short-circuits my brain.\nHere’s the answer we’ve given to incoming and prospective students about what teaching will be like:\n\nLike all universities, face-to-face teaching in 2020-2021 will depend on social distancing guidelines that the Scottish government has set in place. Our promise to you is that we will give you as much face-to-face contact and teaching as is possible. However, because we don’t yet know what specific guidelines will still be in place in September, it’s difficult to tell you right now exactly what will be possible. We know that this uncertainty is frustrating, believe us, we miss being in the classroom with our students so much.\n\n\nDespite the uncertainty we want to give you as much concrete information as we can given what we know now, rather than promise things we may not be able to deliver. Level 1 Psychology lectures involve a lot of people in a confined space, so at this point it is very unlikely that lectures can take place in person for the first semester. As the situation evolves, and depending on guidelines, this may be required for the second semester as well. Instead, our lectures will be pre-recorded to allow you flexibility, but there will also be a live watch party (think Netflix Party but with less Tiger King) at the regular lecture time so you can make contact directly with your lecturers and peers, ask questions, take part in interactive activities, and importantly to help establish a routine.\n\n\nIt is more difficult to give you an exact plan for labs and small-group teaching. Given that lab class sizes are smaller than lectures, for the second semester, we’re optimistic that we will be able to deliver them in person as normal. However, for the first semester the exact details of the social distancing guidelines mean we have to change how we teach labs temporarily. In both semesters you will receive labs led by a lecturer and one of our Graduate Teaching Assistants, one-to-one office hours and support, Peer-Assisted Learning (which are sessions run but students in other years to help you get support from students who have been through your course), and the opportunity to work with other students. You will learn all the content and skills and complete all the assessments that students in 2019-2020 did.\n\n\nOur priority is giving you face-to-face contact to welcome you into the University community, and give you the student experience you deserve – making friends, talking to your lecturers, getting involved in community activities. We want to make sure you really feel like you are a part of Team UofG and so that is the type of contact we will prioritise. Depending on guidelines, we would really like to prioritise the use of campus space for things that make your university experience unique and special (like study groups, student societies, and getting to know staff and other students). How we do this will depend on the guidelines, but we want you to know what our priorities are.\n\nBut how to put that into action as course lead (and lecturer)? What do I do next? I’ve found it useful, and calming, to make a list of what I know and what I don’t know and structure the work I have to do in the summer around that.\nThings I know:\n\nLectures will be delivered online\nWe cannot have a large in-person exam at the end of term\nSome assessment guidance will need updated\nThe organisation of the VLE needs to be 100% super tight and clear\nReading and course materials need to be available online and accessible\nWe are going to use Teams to communicate with our students\n\nThis isn’t everything, but it’s a good chunk of concrete, actionable tasks and a positive starting point. I still don’t know how many students are actually going to turn up, if there’s going to be a second wave or local lockdowns I’ll need to deal with, or what the distancing guidelines are going to be come September. But that’s where the serenity, courage and wisdom come in (and let’s be real, not a small amount of wine). We have a plan for how to deliver the labs remotely, blended, and fully face-to-face but that’s the bit that’s most affected by the uncertainty. So I’m just going to ignore it for a few weeks.\nThis week I started with the assessment focused tasks because regardless of the exact type of blended delivery, it’s incredibly important that any changes are considered in light of the learning outcomes and that I think about what additional guidance they may need.\nThe end-of-term exam will remain but rather than being in a drafty gym hall, the essay component will be a 24-hour open book exam and the MCQ component will be a Moodle quiz that’s also available for 24-hours but timed so that students have 60-minutes to complete it from when it’s first started. I have updated the assessment information to reflect this in addition to the ILOs and assessment guidance as the expectations of an open-book exam are slightly different (i.e., it can be more focused on evaluation and argument than rote-memorising citations).\nPreviously students completed 3 small-stakes MCQs (2% each) throughout the semester to encourage distributed practice. These remain in place but I have updated the assessment guidance and procedure to note that these quizzes will now be timed so that they can get used to the format before the bigger quiz for the end-of-term exam.\nI also started a conversation with our technicians and researchers about the best way to award credit for participating in online research. Previously this had been done manually by the researcher but now there will be much more research done online, we want to find a way to automate it.\nI transferred our Assessment Information Sheets to Sway rather than giving students pdfs. Sway is good for accessibility purposes and adapts the display for different devices whilst also allowing the reader to export a copy to pdf or Word if preferred. You can see an example of one of our assessment sheets here.\nFinally, we also set up a weekly catch-up with the Level 1 team. Not for any particular formal purpose but just to ensure that nothing comes as a surprise to anyone and that everyone has the opportunity to feedback on things as I work through the changes.\nNext week I will continue with working through the assessments (the group presentation is going to require a creative solution) before moving on to my lectures. I hope that this blog will be helpful to some of you, or at the very least, provide some reassurance that even those of us who spend our time squawking on twitter like we know what we’re doing are really just trying to do things one step at a time whilst the wine is chilling."
  },
  {
    "objectID": "post/2020-07-24-pivot-2/2020-07-24-pivot-2.html",
    "href": "post/2020-07-24-pivot-2/2020-07-24-pivot-2.html",
    "title": "Pivot 2: Academic, heal thyself",
    "section": "",
    "text": "The problem with starting your first blog with a pithy quote is that you feel pressured to do it for every one.\nI thought I was going to have to start this blog by admitting that I hadn’t actually done much pivot work this week - I spent the first half of the week working on assessment stuff that has nothing to do with the change in delivery and is just part of the usual yearly updates and tweaks. And then I realised that it’s completely the wrong way to look at it.\nIn our 10 Simple Rules* paper, we make the point (helpfully suggested by Simon Horrocks) that For courses that already involve substantial use of the VLE and online content, it can be helpful to highlight that the pivot may be more accurately described as blended to online, rather than offline to online.\nAcademic, heal thyself.\nThis week I updated the essay question and guidance** for the first year coursework essay and that’s just as much a part of the pivot as designing and recording asynchronous lab activities will be. This week was a good reminder that whilst a part of what I need to do is completely new and terrifying, the majority is the same work, or an adaptation of the same work I would usually do in the summer."
  },
  {
    "objectID": "post/2020-07-24-pivot-2/2020-07-24-pivot-2.html#group-presentation-but-make-it-covid",
    "href": "post/2020-07-24-pivot-2/2020-07-24-pivot-2.html#group-presentation-but-make-it-covid",
    "title": "Pivot 2: Academic, heal thyself",
    "section": "Group presentation, but make it COVID",
    "text": "Group presentation, but make it COVID\nOne of the few assessments we have that needed more than minor tweaks was a group presentation that would normally be given in the final lab of the semester. The topic is “What three things would you tell yourself as a new level 1 student if you could hit rewind?” and students are asked to reflect on their first semester at university and what they would do differently. The entire thing has to be evidence-based and they draw on what they’ve learned about things like memory, learning, sleep, mental health, and study strategies from the lectures in addition to doing their own research. It’s a wonderful lab for many reasons - the presentations are funny, insightful, heart-warming, heart-breaking, and academically rigorous. Given that their first semester is going to be very different to what they expected from university life, we are very keen to retain the topic to give them a chance to reflect on all they have achieved and experienced.\nThey will still work in groups and the topic will remain the same, but rather than a 10-minute presentation they will be given the choice between a 5-minute video, a blogpost, or a infographic poster. I stole this idea (and some of the learning outcomes and guidance) from my colleague Chiara Horlin’s Professional Skills course. This is a Level 3 science communication assessment so the scale and expectations are slightly different to my Level 1 adaptation but the core message here is to remember that there’s loads of good practice out there waiting to be appropriated. If you don’t know how to adapt your course or assessments, ask your colleagues, ask your educational development team, hell, ask on Twitter.\nFor this assessment I’ve given more guidance than I normally would, particularly when it comes to scaffolding the group-work because it is my firm belief that for 1st years this September, if it feels like spoon-feeding*** you’re probably doing it right. It’s still a bit of a work in-progress but the assessment information is here if you’d like to take a look.\nFrom a technological perspective I’m hyper-aware that I have to provide suggestions for collaborative and creative tools that they can access freely. Their Office 365 package solves a lot of this - OneDrive, Word, PowerPoint and Sway cover a lot of bases but I’ve also tried to find alternatives, particularly for video recording and editing (Zoom, Animaker, InShot) and infographics (Canva)."
  },
  {
    "objectID": "post/2020-07-24-pivot-2/2020-07-24-pivot-2.html#student-consultation",
    "href": "post/2020-07-24-pivot-2/2020-07-24-pivot-2.html#student-consultation",
    "title": "Pivot 2: Academic, heal thyself",
    "section": "Student consultation",
    "text": "Student consultation\nThis week I also hosted a consultation with teaching staff and GTAs where I invited 2019-2020 first year students to feedback on our plans for blended delivery for first year 2020-2021 given that they experienced the old-normal version of the course. Not too many turned up (to be fair to them, it’s July, they’re scattered all over the world, and their focus will be on what Level 2 will look like) but it was really useful to hear from those that did.\nThey were largely happy with our plans and were understanding of the constraints and uncertainty we’re working under, but the main thing I took away from the session is how important it was to be able to explain not only what we are planning, but why. It’s a level of detail regarding the underlying rationale and administration for teaching that we wouldn’t necessarily normally communicate**** but I think it was really helpful for them to understand the scope of everything we’ve had to consider when designing the course. We’ve recorded the session and I have sent it out in case those who couldn’t make it are interested.\nMy contact with incoming student is still relatively limited but we do have a Facebook group which will slowly fill up as more of them confirm their offers and finalise their decisions. They’ve already been provided with an information and FAQ pack but I’ve given them an anonymous form to ask any questions or write down anything they’re worried about with the view of doing a Q&A video in the next couple of weeks.\nNext week I will continue working on assessment information and specifications. For level 1 I need to finish a few details and then move on to doing the same for my MSc conversion Research Methods course. If I have time, I’ll also start to write the mother of all week-by-week roadmaps for all of the learning activities for both lecture and labs.\n* Yes, I am going to mention this paper in every blog because it’s great.\n** I think we’ve designed a great essay btw, you can check it out here.\n*** It is also my firm belief that what most of what is derogatively described as spoon-feeding is actually clearly signposted information that gives students the information they need to succeed without chance.\n**** We’re very open with our students and strongly believe in the power of co-creation, but room capacities and the definition of asynchronous and didactic teaching don’t often come up."
  },
  {
    "objectID": "post/2020-07-31-pivot-3/2020-07-31-pivot-3.html",
    "href": "post/2020-07-31-pivot-3/2020-07-31-pivot-3.html",
    "title": "Pivot 3: Party",
    "section": "",
    "text": "This week I finished working through the assessment information for both my Level 1 course and my MSc Research Methods courses. The latter was fairly quick as that course was already delivered both online and offline and the assessments (R worksheets, pre-registration, quant report write-up) lend themselves easily to online assessment.\nThe only thing really of note for the interested pivot reader from this work was the update to the MCQ assessments for Level 1. Last year they were given three small-stakes MCQ assessments (each worth 2% of their grade) throughout the semester, to encourage them to study and keep up with the lecture content (as well as retrieval practice being good for learning). This year we’ve added in PeerWise*, where they are also required to write one MCQ of their own. As I’m sure you’ve experienced, writing good MCQs is no easy task and to write challenging distractors requires you to understand the question you’ve set fully, so it’s a great learning activity and it also means they build up a huge bank of MCQs to revise from come exam time.\nThe decision to introduce this was nothing to do with the pivot, the School of Biology used it previously and it was extremely popular and requested by our class reps. The reason it’s in this blog is because if you’re looking for an activity to promote engagement through the semester, it’s a great choice. If you’d like more info, see our assessment information and support.\n\nWhen I went to retrieve the link for PeerWise it was currently offline, here’s hoping it comes back!\n\n\nWatch party\nThe most calming thing I did this week was to do a test run of how we intend on running the lectures. Essentially, we’re going to try something akin to a Netflix Party. The lectures will be pre-recorded and chunked into ~15 minute videos, with a max of 40 minutes of didactic pre-recorded content. However, at the usual lecture time, we’re going to have a watch party whereby there will be a Zoom call and everyone will watch the chunks together. In-between the chunks, the lecturer will do interactive activities with the class - polls, discussion, Q&A. The type of interaction with depend on the size and the level of the class but hopefully you get the idea.\nThe rationale behind this approach was to try and balance staff and student workload and to balance what we know works for online with concerns about student engagement. The lectures are pre-recorded to allow for flexible asynchronous engagement, however, there’s also a synchronous event to promote engagement with the lecturer and to help students form a routine which I am particularly concerned about with new 1st years. The 15-minute chunks are better for online but the total run-time of 40 minutes is more realistic for workload than asking everyone to completely rewrite and restructure their lectures. Instead, the workload is channelled towards increasing engagement and active learning.\nThe test run was really to test the tech and whether video sharing over Zoom would work. I tried several different methods, either streaming my video or playing a local copy and all of them worked fine. This is of course going to be hugely dependent on staff broadband speeds and bandwidth so I’ve come up with high and low bandwidth solutions.\nI am 100% aware that this approach doesn’t work for everyone. Some students may not have the bandwidth (although we’re going to offer study space and access to tech to try and address this), some may not be able to make the synchronous session. That’s why the lectures are provided asynchronously as well, why the watch party will be recorded, why there’s also going to be a Teams channel where the lecturer will engage with the class. I don’t believe that perfect is possible (with anything but particularly with this) and I think we should focus on the fact that the old normal was incredibly far from inclusive and flexible and it’s huge progress for HE that inclusivity is being considered at scale for the first time. If anyone does have a solution that works for absolutely every student, I’ll buy it from you for 3 magic beans.\n\n\nWhat’s in a name?\nThe final thing I did this week was to start to edit our data skills books for teaching R. The old Level 1 book was organised by Lab (Lab 1, Lab 2, Lab 3 etc.) and the sub-chapter headings were pre-class activities and in-class activities. In the old normal they had a 2 hour lab every two weeks, in the new normal, a lot of it will be asynchronous but they will have a 30-minute synchronous session every week.\nWhat this means for my book is that I have had to rip it apart and put it back together again. I’ve removed any reference to pre-class and in-class and I’ve renamed the chapters to reflect the content (e.g., Loading data) rather than location they would be completed in. Why I didn’t do this originally I’m not sure. Importantly, the changes I’ve made have been modality neutral so that I don’t have to go back next year and change it again, e.g., “in this pre-class activity you will…” has become “in this chapter you will…” so it works regardless of how it is delivered.\nI also need to do this for the MSc data skills book - at the moment there is a f2f and an ODL version but I will combine them into one modality-neutral copy. Again, why I didn’t do this before now is a mystery.\nThe next step with our data skills work is updating the instructions for how to access R. Previously students would have used the on-campus computers or installed on their laptops. To avoid any issues with installation and system requirements, we’re hopefully getting access to [RStudio Pro]https://rstudio.com/products/rstudio-server-pro/ which runs through a browser. This is a great move, although it does mean updating a lot of set-up instructions.\nFinally, I also got a haircut, which means I can actually start recording, although I’m still a week or so from having anything to say."
  },
  {
    "objectID": "post/2020-08-11-lec-rec-impact/2020-08-11-lec-rec-impact.html",
    "href": "post/2020-08-11-lec-rec-impact/2020-08-11-lec-rec-impact.html",
    "title": "Lec Rec impact",
    "section": "",
    "text": "List of institutions linking to lecture capture guides from Nordmann et al. (2020).\n\nUniversity of Aberystwyth, UK\nUniversity of Auckland, New Zealand\nUniversity of Birmingham\nUniversity of Cape Town South Africa\nUniversity of Cambridge\nCity University London, UK\nUniversity of Derby\nUniversity of Edinburgh, UK\nHull York Medical School UK\nUniversity of Glasgow (external link pending)\nImperial College London, UK\nRoyal Holloway, University of London, UK\nLondon School of Economics, UK\nUniversity of Sheffield"
  },
  {
    "objectID": "post/2020-08-21-pivot-6/2020-08-21-pivot-6.html",
    "href": "post/2020-08-21-pivot-6/2020-08-21-pivot-6.html",
    "title": "Pivot 6: Human after all",
    "section": "",
    "text": "This week I continued recording my lectures and asynchronous lab videos. As I’ve already said on Twitter, I largely refuse to do any editing - there were a couple of moments where the cat made a noise in the background, the door bell went off, or I stumbled over words and there’s no way I am spending my time editing them out. Our face-to-face lectures weren’t perfect, we made errors, had tech issues, had students walk in late, or building work noise in the background. I’m not saying that there’s not a minimum level of polish we should be aiming for, but I know that a lot of people worry that recordings need to have high production values and I don’t think we can say it enough enough that that isn’t the case.\nAbove anything else, I hope that this whole mess has shown us what the value of education is and why people come to university. It isn’t the content, if they wanted content the internet is full of it. They want connection, engagement, community. They want advice and guidance and understanding. That’s what is going to make or break 2020-2021, not whether you forgot what you were saying for 10 seconds in a lecture. I know there’s been some ridiculous articles floating around Twitter about making sure your Zoom background is professional and videos are up to spec and I won’t give them further airtime by linking to them but honestly. It’s an entirely positive thing for our students to realise that we are human after all. Higher education isn’t going to be made any worse for an extra shot of empathy. Whilst ensuring high educational standards, I want them to see me fluff my words occasionally so that they know when they do a presentation it’s ok not to be perfect, I want the cat to interrupt when they’re in a meeting with me so that they understand that if the reverse happens and their child/partner/pet barges in, I’m not going to care. Yes it’s all relative and this is by no means me arguing for a hash job - I am working incredibly hard to make sure everything is in place and has been thought through - but why anyone would seek to strip online teaching of its humanity is beyond me.\nThere are of course some exceptions. As I write this, I’ve just discovered that in the Course Overview video I recorded earlier in the week, I’ve got the topics of the lectures the wrong way round, I’ve said mental health is on Tuesday and developmental is on Wednesday and it’s the other way. In the grand scheme of things this is a tiny mistake, and one I could maybe rectify by adding a note. But, I also know from many years of experience that new 1st year students pick up on any inconsistency and panic about it. So, given that it’s the very first video I’m going to ask them to watch, I’m going to re-record it. It will take me half an hour, but first impressions matter.\nThis week I also had an information session with the new 1st year students and it was so great to meet them all. What was reassuring for me was that their questions and concerns were the same as they’ve always been - what optional courses should they take, what’s the difference between BSc and MA psychology, should they do any pre-reading. I can’t imagine that they’re not worried about learning online or what university in a pandemic is going to be like, but they’re certainly taking it in their stride at the moment. Also, for anyone who is working at an institution where f2f teaching is required, we’ve had absolutely no blowback about our plans that all core teaching will be online.\nIn other musings, I am trying to keep the number of tech platforms we use to a minimum (Moodle, Zoom, Teams) but I have started playing around with Gather Town to facilitate social interaction without overloading on huge Zoom rooms. I may attempt to do something for the induction lecture whereby we do 15 minutes on Zoom where I present information etc. and then invite everyone to go to Gather Town so that they can chat to people in small groups. It’s very easy to use and it doesn’t require them to create an account etc., which always helps.\nAt this point in the pivot, my main advice is to have a weekly team meeting. We set up a 30-minute Zoom with the level 1 teaching team a month or so back and it’s proved incredibly helpful. As course lead it gives me a nice way of checking where we’re all at which is calming for my anxiety but it’s also good just to talk through what we’ve been doing. There’s been a few things that people have said off the cuff that have resulted in a larger conversation, a better solution, or realising we need some extra training. I am what I would charitably describe as a control enthusiast and I do think that when you’re facing something that is as complex as pivoting first year online, there’s a benefit to having one person tying everything together so that it’s cohesivee. However, it’s also very helpful and reassuring to have my ideas and decisions questioned and examined at every turn by the people they are going to affect, and to have a set time and place for them to think about that stuff because they’re all juggling multiple other roles.\nMy goal was to have the first 5 weeks of content sorted by the end of August so that I could take a few days of annual leave before diving back in. I think I’m on track? Honestly I’m not sure, it feels like a mirage at times, like I can only see what September is going to be like out the corner of my eye and if I look directly at it, it disappears. Is it wine o’clock yet?"
  },
  {
    "objectID": "post/2020-08-27-pivot-7-grief/2020-08-27-pivot-7-grief.html",
    "href": "post/2020-08-27-pivot-7-grief/2020-08-27-pivot-7-grief.html",
    "title": "Pivot 7: Grief",
    "section": "",
    "text": "Last Saturday we were supposed to be in London for the West End production of Sister Act, with Whoopi Goldberg reprising the lead role and Jennifer Saunders as Mother Superior. I genuinely love Jennifer Saunders more than I love my own mother, and my wife, who is a choral conductor1, counts Sister Act amongst her favourite films. Just after the scene where Mary Clarence leads the choir for the first time, I looked over and realised Kathleen was crying. Now, this isn’t unusual. We discuss, feel, and express our emotions as freely, frequently, and strongly as is dictated by the Lesbian Code of Conduct so we cry at the TV most days. What was unusual is that we normally cry in sync2 and I was still dry-eyed. We talked about it afterwards and she said that it normally makes her cry anyway because it’s about community choirs being good for people and conductors being good leaders and that’s her thing but that this time it had packed an extra punch because it was a reminder of how much she misses being in the room with her choir. And that, dear reader, made me cry at the time and again whilst typing this because I viscerally get what she’s feeling because I feel the same grief about my own teaching.\nI have two firm beliefs about 2020-2021\n\nOnline learning is not only educationally powerful and effective but it can provide a full, rich, positive student experience in which meaningful lasting connections can be made.\nI am going to feel a deep personal loss for as long as this goes on.\n\nAnd this is very much about the personal. Pedagogically, now that I have hit my stride with designing the teaching materials, my course might be better than its ever been. The structure is better, the signposting is better, I’ve chucked out any resource that isn’t fully open-access, the synchronous sessions are entirely focused on active learning and group-work, and community is threaded through the core of our entire programme. If you’re a UofG student reading this, honestly, we’ve got your back and I’m so proud to be part of the team that’s pulling this off.\nBut at that personal level, being at the front of that lecture hall gives me life. I love my job. I love teaching. Lectures get a bad rap because they’re so easy to do badly but for me a great lecture is like nothing else for both sides of the podium. I need a break as much as anyone but I miss teaching during the summer. I’m going to miss seeing their faces when I tell them something that blows their minds. I’m going to miss the nervous laughter in lecture 1 of 1st year when they’re not sure if they’re allowed to laugh at my jokes turn into the confident laughter of a team that’s comfortable with each other. I’m going to miss when they desperately try to hide that they’re hungover in the labs. I’m going to miss overhearing their patter about their weekends. I’m going to miss the look of the gaybys when they realise she’s one of us.\nIt’s personal because it’s got nothing to do with the quality of education we’re providing or the experience the students themselves will have. All of this will still happen. Minds will be blown, jokes will be laughed at, hangovers will be hidden, gaybys will be represented. But I’m not going to be in the room to see it and feel it with whatever magic of our chemistry it is that makes being in the same room with other humans feel good.\nI have two firm beliefs about how I/we handle this grief (and this is where I imagine some of you reading this will disagree):\n\nIt’s important that we allow ourselves and others on our teams to feel and express this grief. Some people won’t feel it, for some, not being in a lecture hall is going to be a relief, but for those who do, recognise that it is separate to how hard they’re working to pivot their teaching online and to their beliefs about the efficacy of online learning.\nWe should be mindful that our grief does not frame our interactions with students and that it is not part of the narrative of 2020-2021. Yes, they’re adults blah blah blah. But they’re adults who are dealing with their own stuff and even in the most egalitarian classroom there’s still a power differential between students and teachers that means it’s problematic for them to be aware of my personal feelings. I don’t want them to feel like they can’t come to me with problems for fear of upsetting me, that’s not my role in their lives.\n\nAt the crux of this is that as I’ve said, their education isn’t going to suffer, and by mixing our entirely valid grief in with the pedagogical narrative, we risk that narrative becoming one of deficit, which does a disservice to the massive amount of work being put in to create engaging and effective online courses. They’re not there for my entertainment and my feels, I’m there to teach them. And I am going to teach them well. I am also going to drink wine and cry to my friends and my wife occasionally, and that’s also fine.\n\n\n1 Kathleen is the Musical Director of Loud and Proud, Scotland’s LGBT choir based in Edinburgh. They’re great, she’s great, you should check them out.\n2 Being a lesbian is hilarious"
  },
  {
    "objectID": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html",
    "href": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html",
    "title": "Using ChatGPT to create teaching materials: Data simulation & MCQs",
    "section": "",
    "text": "There’s an overwhelming amount of discussion about student uses of ChatGPT that is mainly concerned with academic misconduct. Whilst I understand that it’s a huge problem (and one I am grappling with for my own assignments), I also think it’s useful to have more examples of how we in the teaching and learning community can use tools like ChatGPT to help make our lives a little easier. So, this is the first in what will (probably, if I have time) be a series of blog posts on how I have been using ChatGPT to help design some teaching materials for next year.\nFor context, I am course lead for a large (700 students) first year introductory psychology course. There are three lectures a week that cover theory and concepts and then practical labs in which students work in smaller groups. In the first semester the labs focus on academic writing and using evidence to support their first substantive piece of coursework which is an essay on the history of psychology, and they also learn data skills and the programming language R. In the second semester, they design a research project in small groups and write a registered report as their coursework, in addition to continuing their data skills and R adventures.\nI have been using ChatGPT and GPT-4 with a paid subscription."
  },
  {
    "objectID": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#data-skills",
    "href": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#data-skills",
    "title": "Using ChatGPT to create teaching materials: Data simulation & MCQs",
    "section": "Data skills",
    "text": "Data skills\nThis blog will cover how I’ve been using ChatGPT to help rewrite some of our Level 1 data skills teaching materials. Student feedback indicated that the labs and data skills work felt a bit disconnected from the lectures and psychological theory they were learning so I wanted to link them up a bit more.\nAdditionally, we were using a single dataset for the entire semester. The rationale behind using a single dataset was to allow students to get to know the data well so that they could just focus on coding. However, an unintended consequence to this approach was that because the variable names were the same each week, it encouraged blind copying-and-pasting so when they were finally given a different dataset for the homework, I’d get code that related to the one from the workbook. I could grumble about students not paying attention or I could recognise that the way the course was taught wasn’t encouraging them to pay attention.\nThere are six chapters of the data skills book in each semester. Typically they work through one on their own and one in the lab, so I knew I wanted three different datasets that could form pairs of chapters. As part of the PsyTeachR curriculum our focus is on using real, messy data but at the very beginning of their data skills journey, I need datasets that have a specific structure, or specific things wrong with them so that I can use them to teach specific skills."
  },
  {
    "objectID": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#data-simulation",
    "href": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#data-simulation",
    "title": "Using ChatGPT to create teaching materials: Data simulation & MCQs",
    "section": "Data simulation",
    "text": "Data simulation\nI’ve been using ChatGPT to help me write code to simulate datasets that have the properties I need. I’ve done a little bit of simulation before and I could have figured it out on my own but it would have taken me so much longer:"
  },
  {
    "objectID": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#evaluation-limitations-and-considerations",
    "href": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#evaluation-limitations-and-considerations",
    "title": "Using ChatGPT to create teaching materials: Data simulation & MCQs",
    "section": "Evaluation, limitations, and considerations",
    "text": "Evaluation, limitations, and considerations\nI’ve found this hugely time-saving although the above example gives an overly simplistic impression of the process. For each simulated dataset I had a back-and-forth of multiple prompts and I’ve learned a lot about how to get it to do what I want, for example, even simple things like I now know to specify that I want lower-case variable names. I always check that the data does actually have the properties I specified and sometimes play around with the values but for the most case, it’s been pretty close to what I had in mind.\nWhat I’ve found most useful is that it has allowed me to try out several different datasets. For the first semester I’ve settled on the Stroop effect, corsi blocks, and a belonging questionnaire (I have been choosing from experiments and surveys available on the wonderful PsyToolkit). But, I also experimented with a lexical-decision task and a need for cognition scale before I decided these didn’t quite fit what I wanted them to learn about data skills.\nIn the case of data simulation, using ChatGPT has given me the opportunity to pick the best possible dataset for what I’m trying to teach, rather than being constrained by the time it would take to find or simulate it myself. I’ve also used it to teach myself more about how to perform data simulation in R. For the most part, I’ve understood the code it has produced, it’s just not been quite familiar enough to me to do it off the top of my head, but there have been a couple of times where it’s given me something I haven’t done before.\n\n\nI also use the “regenerate response” feature to see if there are alternative ways of writing the code (there always are), which has been useful for someone who generally takes a strictly tidyverse approach to coding in R.\nWhat ChatGPT hasn’t done is generate any of the ideas for me. I’ve used my knowledge of the course, my students, and what I want to teach them to decide on what each dataset should look like. I’m doing a fairly substantial rewrite of the Level 1 data skills book and it hasn’t even occurred to me to use ChatGPT to help inform what’s going in the book. It might well be able to do this to some degree, but it wouldn’t be my course or my vision and that’s really important to me."
  },
  {
    "objectID": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#practice-testing",
    "href": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#practice-testing",
    "title": "Using ChatGPT to create teaching materials: Data simulation & MCQs",
    "section": "Practice testing",
    "text": "Practice testing\nWe try to integrate practice testing throughout all our data skills materials using the webexercises package so I have also been using ChatGPT to help me create multiple-choice and fill-in-the-blank questions. I’ve done this to generate theoretical questions, for example:\n\n\nBut I’ve also given it questions I have written myself and asked it to write an explanation for the answer."
  },
  {
    "objectID": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#evaluation-limitations-and-considerations-1",
    "href": "post/2023-06-12-using-chatgpt-to-create-teaching-materials-data-simulation-mcqs/index.html#evaluation-limitations-and-considerations-1",
    "title": "Using ChatGPT to create teaching materials: Data simulation & MCQs",
    "section": "Evaluation, limitations and considerations",
    "text": "Evaluation, limitations and considerations\nI’ve found that the accuracy of the questions and explanations it provides has been pretty good - although I say this aware of the context that this is an intro psych course in which I am asking it to generate fairly concrete factual questions. The big win for me is that I can generate a lot of questions with explanations and feedback in a short amount of time and I think that the rewrite of my data skills book has more practice testing in it as a result. With that said, regardless of whether it’s a theoretical or coding question and regardless of how good it’s been overall, I wouldn’t use anything ChatGPT produces for which I can’t verify the accuracy. When I ask it to write questions or write an explanation, I use the “regenerate response” option frequently so I can pick the best option and as with the simulation code, I’ve rarely been able to take what it gives me without a little bit of editing and a back-and-forth.\nThese questions are also being used as part of a formative exercise and I’d be reticent use it to generate questions for the summative MCQ exam that assesses the lecture content because I need to be 100% certain the questions are assessing what they’ve been taught. Generating plausible incorrect responses for questions I’ve written is a different matter though, I haven’t done it yet but I think there’s a use case for this. They’re often difficult to get right (let’s be honest, we’ve all written an MCQ where one option is just clearly total nonsense) and I think ChatGPT could help with providing possible options (which you then verify the accuracy of).\nMost importantly, when I say I pick the “best option”, yes I am referring to ensuring that what it has given me is correct, but it’s much more than that. I’m also using my knowledge of my students, what they’ve been taught so far, what reading they were supposed to have done, and my knowledge of the context of the materials. These questions are there to help keep them engaged as they work through the chapter mostly independently and to reinforce what they’re learning. I don’t want the questions to be hard or abstract but I also don’t want them to be a waste of time. ChatGPT also isn’t telling me where to put the questions and how many I need.\nThere’s no prompt I could write that could substitute my knowledge as a teacher. Or I suppose more to the point, there’s no prompt I could write where I would feel that it was still my expertise leading the way. Overwhelmingly my takeaway from using ChatGPT is that if you have prior knowledge and experience, it’s a game-changer for reducing the amount of time some of the grunt work takes. But just as we’re all concerned with students skipping the learning part, I’d also be concerned with educators skipping the creative experience of designing teaching materials/a course from scratch not least because it’s probably my favourite part of my job but because I think the result would be soul-less and stifle innovation and progress (although also, there’s a lot of bad teaching in higher education there so maybe I’ve got a rose-tinted view)."
  },
  {
    "objectID": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html",
    "href": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html",
    "title": "Using ChatGPT to create teaching materials: marking criteria & rubrics",
    "section": "",
    "text": "This is the second in a series of blog posts discussing how I’m using ChatGPT to create teaching materials (see here for part 1 on using it for data simulation and creating MCQs). For technical context, I have primarily been using GPT4 through a paid subscription to ChatGPT but I also occasionally use Bing Chat on Creative mode (which uses GPT4 under the hood I believe).\nFor pedagogical context, I am course lead for a large (700 students) first year introductory psychology course. The main substantive piece of coursework students complete in the first semester is a 1500 word essay taking a critical approach to the history of psychology (the questions are focused on things like eugenics, the replication crisis, the pathologisation of LGBTQ people etc., I’ve written about the essay here).\nFor marking and feedback, we use Turnitin Feedback Studio. Students receive on-script comments, a general feedback comment, and then a rubric with a rating on six criteria from Excellent (A grade) to Very Poor (G). Our full marking scheme is on a 22-point scale. The rubric doesn’t calculate a score, but the grade and the rubric should align and the marker uses the rubric to help decide the grade, i.e., if the rubric is majority Very Goods, they should be getting some sort of B.\nImportantly, the rubric contains descriptions of what each level of attainment should look like. Part of the reason we include a rubric is to provide students with clear assessment criteria before submission, and clear feedback after, but part of the reason is also to help maintain consistency between markers. With 700 students, we have 10-11 markers on the team and using rubrics really helps with alignment. We also use a shared QuickMark comment bank and have multiple marking alignment meetings although that’s not relevant to the current blog.\nBut, writing good rubric descriptors is HARD. Getting the balance between the descriptors being specific enough to be useful to students but leaving a little room for pragmatism for markers is incredibly difficult. For example, one year, one of the rubric descriptors allowed no errors in spelling, punctuation or grammar. This seems like an entirely sensible thing to expect of an excellent essay for the writing communication criteria but if you follow it exactly (which is what you want your markers to be able to do for consistency), you end up marking down the best essay you’d ever read if it had a single typo or missing comma, which is stupid. Additionally, I particularly struggle writing the descriptions at the lower end of the criteria (is it just me?) - how do you usefully distinguish “Poor” from “Very poor”? It’s part of our marking scheme so we can and we must but putting it into words takes some effort."
  },
  {
    "objectID": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html#marking-criteria",
    "href": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html#marking-criteria",
    "title": "Using ChatGPT to create teaching materials: marking criteria & rubrics",
    "section": "Marking criteria",
    "text": "Marking criteria\nSo, enter ChatGPT. The first thing I did was to see if it could improve the six marking criteria we use. Throughout all assessments on our programme, we group assessment criteria into knowledge & research, evaluation & analysis, and communication so I knew that I needed to keep this grouping.\nMy first prompt was:\n\nAct as an expert psychology lecturer. Write marking criteria that could be used to assess the below essay questions. There should be six criteria, two about knowledge and research, two about analysis and evaluation and use of evidence, and two about academic communication. Present these in a table. [followed by essay questions]\n\nFor comparison, I present the marking criteria ChatGPT created with this relatively simple prompt, and the criteria that we used in 2022, the ones I wrote myself with my muggle brain.\nWell bugger me.\n\n\n\n\n\n\n\n\nType\nChatGPT\nOriginal criteria\n\n\n\n\nKnowledge and Research\nUnderstanding of the topic, including relevant theories, concepts, historical context, and methodological approaches. This should be demonstrated by a comprehensive discussion of key ideas, appropriate use of terminology, and an accurate representation of relevant research findings and studies.\nDescribe relevant evidence in an appropriate level of depth and breadth to demonstrate a comprehensive understanding of the topic.\n\n\nKnowledge and Research\nQuality and breadth of research sources, including primary research articles, review articles, and/or reputable secondary sources. The essay should demonstrate a comprehensive understanding of the field by referring to a diverse range of sources, and these sources should be current and relevant to the topic.\nUse a range of relevant academic sources upon which to base the essay\n\n\nAnalysis and Evaluation/Use of Evidence\nCapacity to critically evaluate and analyze the evidence, considering strengths, limitations, implications, and alternative viewpoints. The essay should demonstrate an ability to think independently, and to connect and contrast different pieces of evidence in a thoughtful and meaningful way.\nProvide analysis, interpretation, and evaluation of the evidence presented to answer the specific question asked.\n\n\nAnalysis and Evaluation/Use of Evidence\nUse of empirical evidence to support arguments. Each argument should be substantiated by empirical findings from relevant research, with appropriate citation. The essay should integrate evidence in a way that supports and enhances the overall argument, rather than simply describing individual studies.\nUse academic evidence to support your analysis, interpretation, and evaluation\n\n\nAcademic communication\nClarity and organization of the essay. The essay should have a clear structure (introduction, body, conclusion), with coherent and logically-structured paragraphs. The flow of argument should be easy to follow, and transitions between points and sections should be smooth.\nEnsure that the essay has a logical structure and that the paragraphs flow together to form a coherent narrative.\n\n\nAcademic communication\nQuality of writing, including grammar, punctuation, and referencing. The essay should be well-written, with clear and concise language. It should be free of spelling and grammatical errors, and it should adhere to the appropriate academic referencing style (e.g., APA, MLA).\nWrite clearly and succinctly with appropriate use of spelling, grammar, and APA referencing.\n\n\n\nTo make myself feel better I must highlight that the fuller criteria ChatGPT has produced essentially include the “Excellent” descriptor from the rubric, whereas mine are just the headline criteria. But wow, that’s pretty close."
  },
  {
    "objectID": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html#rubrics",
    "href": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html#rubrics",
    "title": "Using ChatGPT to create teaching materials: marking criteria & rubrics",
    "section": "Rubrics",
    "text": "Rubrics\nI then followed this up with a request for the rubric descriptors:\n\nNow write me rubric descriptions for each of these criteria. There should be a description for excellent, very good, good, satisfactory, weak, poor and very poor. Present these in a table.\n\nMe and my expertise are going to the pub.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nExcellent\nVery Good\nGood\nSatisfactory\nWeak\nPoor\nVery Poor\n\n\n\n\nKnowledge and Research\n\n\n\n\n\n\n\n\n\nUnderstanding of the topic\nDemonstrates superior comprehension of the topic, employing advanced theories and concepts. Uses precise and appropriate terminology, and accurately represents diverse and complex research findings.\nDisplays a strong understanding of the topic, including relevant theories and concepts. Utilizes terminology correctly, and mostly accurately represents research findings.\nShows a good understanding of the topic, with minor inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\nDemonstrates a basic understanding of the topic, with some inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\nExhibits an incomplete understanding of the topic, with significant inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\nLacks an adequate understanding of the topic, with numerous inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\nDemonstrates little to no understanding of the topic, with severe inaccuracies or omissions in the use of theories, concepts, terminology, or research findings.\n\n\nQuality and breadth of research sources\nExtensively uses a wide range of high-quality, current, and relevant sources. Demonstrates a superior understanding of the field.\nUses a good range of high-quality, current, and relevant sources. Demonstrates a strong understanding of the field.\nUses a reasonable range of high-quality sources, but may lack some currency or relevance. Demonstrates a good understanding of the field.\nUses a limited range of sources, which may lack quality, currency, or relevance. Demonstrates a satisfactory understanding of the field.\nUses a small range of sources, which lack quality, currency, or relevance. Demonstrates a weak understanding of the field.\nUses very few sources, which lack quality, currency, or relevance. Demonstrates a poor understanding of the field.\nUses almost no sources, or uses sources that are largely irrelevant, outdated, or of low quality. Demonstrates a very poor understanding of the field.\n\n\nAnalysis and Evaluation/Use of Evidence\n\n\n\n\n\n\n\n\n\nCapacity to critically evaluate and analyze\nDemonstrates superior critical thinking skills, with thoughtful and innovative analysis of strengths, limitations, implications, and alternative viewpoints. Connects and contrasts evidence in a deeply insightful way.\nDisplays strong critical thinking skills, with a well-developed analysis of strengths, limitations, implications, and alternative viewpoints. Connects and contrasts evidence in a insightful way.\nShows good critical thinking skills, with a reasonably well-developed analysis of strengths, limitations, implications, and alternative viewpoints. Connects and contrasts evidence in a somewhat insightful way.\nDemonstrates satisfactory critical thinking skills, but the analysis of strengths, limitations, implications, and alternative viewpoints may be shallow or undeveloped. Connects and contrasts evidence in a basic way.\nExhibits weak critical thinking skills, with a poorly developed or inaccurate analysis of strengths, limitations, implications, and alternative viewpoints. Struggles to connect and contrast evidence in a meaningful way.\nLacks critical thinking skills, with an undeveloped or largely inaccurate analysis of strengths, limitations, implications, and alternative viewpoints. Fails to connect or contrast evidence in a meaningful way.\nDemonstrates little to no critical thinking skills, with an absent or completely inaccurate analysis of strengths, limitations, implications, and alternative viewpoints. Makes no attempt to connect or contrast evidence.\n\n\nUse of empirical evidence to support arguments\nEmploys a diverse range of highly relevant empirical findings to support arguments. Integrates evidence in a sophisticated and nuanced manner that greatly enhances the argument.\nUses a good range of relevant empirical findings to support arguments. Integrates evidence in a thoughtful manner that enhances the argument.\nUses a reasonable range of empirical findings to support arguments, but integration of evidence may be somewhat superficial or inconsistent.\nUses a limited range of empirical findings to support arguments, and integration of evidence may be shallow or inconsistent.\nUses few empirical findings to support arguments, and integration of evidence is poor or inconsistent.\nUses almost no empirical findings to support arguments, and makes little to no attempt to integrate evidence.\nFails to use empirical findings to support arguments, and makes no attempt to integrate evidence.\n\n\nAcademic Communication\n\n\n\n\n\n\n\n\n\nClarity and organization of the essay\nEssay is extremely well organized and clear. Arguments flow seamlessly from one point to the next, with excellent transitions.\nEssay is well organized and clear. Arguments flow smoothly from one point to the next, with very good transitions.\nEssay is generally well organized and clear, with good flow and transitions, but may have minor inconsistencies.\nEssay is somewhat organized and clear, but the flow and transitions may be choppy or inconsistent.\nEssay is poorly organized and lacks clarity, with significant problems with flow and transitions.\nEssay is very poorly organized and lacks clarity, with severe problems with flow and transitions.\nEssay is completely disorganized and lacks any clarity, with no discernible flow or transitions.\n\n\nQuality of writing\nWriting is flawless, with perfect grammar, punctuation, and referencing.\nWriting is very good, with minor errors in grammar, punctuation, or referencing.\nWriting is good, with some errors in grammar, punctuation, or referencing.\nWriting is satisfactory, but contains several errors in grammar, punctuation, or referencing.\nWriting is weak, with numerous errors in grammar, punctuation, or referencing.\nWriting is poor, with severe errors in grammar, punctuation, or referencing.\nWriting is very poor, with pervasive errors in grammar, punctuation, or referencing."
  },
  {
    "objectID": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html#evaluation-limitations-and-considerations",
    "href": "post/2023-07-02-using-chatgpt-for-teaching-marking-criteria-rubrics/index.html#evaluation-limitations-and-considerations",
    "title": "Using ChatGPT to create teaching materials: marking criteria & rubrics",
    "section": "Evaluation, limitations, and considerations",
    "text": "Evaluation, limitations, and considerations\nOk, with less melodrama, let’s be critical about what it’s produced. It’s definitely not perfect and could not be used out the box without an editor with expertise. Its first attempt has recreated the issue I had previously of requiring perfection for an “Excellent” rating on “Quality of writing”. Again, you’d think it’s a sensible thing to suggest but such is the value of experience. There’s also a bit of an issue specific to our essay questions in that because they cover the history of psychology, the sources necessarily will not all be current. Another sensible suggestion but this needs edited to stop it causing confusion and anxiety.\nI could edit these manually or I could try to use ChatGPT to shape its responses further:\n\nRewrite quality of writing but don’t specify an exact number of errors that are allowed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nExcellent\nVery Good\nGood\nSatisfactory\nWeak\nPoor\nVery Poor\n\n\n\n\nQuality of Writing\nWriting is pristine and polished, displaying exceptional use of grammar, punctuation, and adherence to the referencing style.\nWriting is of a high standard with minimal and insignificant errors in grammar, punctuation, or referencing.\nWriting is of a good standard with some noticeable, yet not severe, errors in grammar, punctuation, or referencing.\nWriting is satisfactory but displays consistent errors in grammar, punctuation, or referencing that somewhat distract from the clarity of the content.\nWriting quality is substandard with a number of distracting errors in grammar, punctuation, or referencing that affect readability.\nWriting is poor with numerous errors in grammar, punctuation, and referencing which severely impact readability.\nWriting is riddled with pervasive errors in grammar, punctuation, or referencing, making it nearly unreadable.\n\n\n\nThat’s better. Let’s try fixing the other issue:\n\nRewrite “Quality and breadth of research sources” to allow historical sources as well as current sources, as relevant to the essay question.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria\nExcellent\nVery Good\nGood\nSatisfactory\nWeak\nPoor\nVery Poor\n\n\n\n\nQuality and Breadth of Research Sources\nUtilizes a diverse range of high-quality, relevant sources, both historical and current, providing an outstanding depth and breadth of understanding of the field.\nEmploys a broad range of quality, relevant sources, both historical and current, contributing to a strong understanding of the field.\nEngages with a variety of relevant sources, both historical and current, contributing to a solid understanding of the field, but may lack some depth or breadth.\nUses a limited range of sources, both historical and current, contributing to a satisfactory understanding of the field, though these sources may lack depth or breadth.\nEmploys a narrow range of sources that lack quality or relevance, or are not adequately balanced between historical and current, leading to a limited understanding of the field.\nRelies on a very limited range of sources, with many lacking in quality or relevance, or an imbalance between historical and current sources, leading to a poor understanding of the field.\nDemonstrates little to no engagement with relevant sources, or relies heavily on sources that are irrelevant, of low quality, or unbalanced in terms of historical and current perspectives, leading to a very poor understanding of the field.\n\n\n\nIt’s better but I still don’t like it as the balance of historical and current sources that’s right will depend on the essay question they answer. It feels unlikely I’ll get it to get this one right and will edit it myself.\nBut, as a base from which to start editing this is incredibly helpful. There’s loads of rubric examples out there but the beauty of using ChatGPT is that I’ve been able to shape these examples to make the starting point more relevant to me by giving it the essay questions, the number of criteria I want, and the categories to group them into.\nAs with my attempts to use it for data simulation and writing MCQs, I’ve simplified the process I underwent here. I used the regenerate option quite a few times and some of them were better than others. The final rubric I use next year is going to be a combination of bits of the old rubric I wrote myself that were working well, along with some edited adaptions from several versions ChatGPT produced, and I’m also going to ask the teaching team to review it before we release it to students.\nWhich brings me to the point I made in my first blog post about using ChatGPT for teaching which is that I simply don’t see how any of this would be possible or useful without prior expertise and experience.\nI’m really glad that I’ve started using it for my own work because it’s given me an insight into how students might use it for assessments and the potential repercussions. Very quickly my focus has changed from “how do we stop them from using it?” or “how do we spot if they’re using it?” to “how do we make sure they first get the prior knowledge and experience they need to use it effectively?”. And not just students, everyone, everywhere, working on any task. In some ways I feel incredibly lucky that this has appeared at this point in my career when have built up the expertise and experience to use it to facilitate my work, whilst being knowledgeable enough to know when not to follow it and to ensure my work never loses my voice and my steer.\nWithout that prior knowledge and experience guiding me, my overwhelming sense is that everything it produces would look great on the surface but would just be a little bit shit once you started looking at it properly.\nHow do we convince people of this when they have access to these tools at the start of their journey? No, really, I’m asking, because I don’t think I (or anyone else) know the answer yet."
  },
  {
    "objectID": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html",
    "href": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html",
    "title": "Using ChatGPT for learning & teaching: Correcting auto-transcripts",
    "section": "",
    "text": "Alongside Wil Tovio and Rachel O’Neill, I’ve written before in the Times Higher about the problems that requiring academics to produce corrected captions introduces. If you don’t do it, you disadvantage a wide range of students and effectively ruin the education of those who are d/Deaf and hard-of-hearing. If you do, you put a much higher workload burden on any lecturer with a “non-standard” accent, those with poorer quality recording equipment, and let’s honest, those who just care a bit more. I say all of this whilst recognising my own privilege: most recording software generally picks up what I am saying accurately and through both work and personal means, I invested in a high-quality mic at the start of covid."
  },
  {
    "objectID": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#correcting-transcripts",
    "href": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#correcting-transcripts",
    "title": "Using ChatGPT for learning & teaching: Correcting auto-transcripts",
    "section": "Correcting transcripts",
    "text": "Correcting transcripts\nBut anyway. It just occurred to me that one use of ChatGPT that falls under the category “let’s make the admin associated with teaching easier” is that we might be able to use it to edit transcriptions for us. And for once, I actually have the means to evaluate how it’s done in the form of an original automated-transcription, the corrected version a human spent hours on, and the audio. The video in question was a walkthrough of a data skills tutorial in R, and platforms like Zoom and Echo360 tend to struggle with the technical terminology.\nI downloaded the original transcript as a .txt file from Echo360 and put the following prompt into ChatGPT. I’ve got a paid subscription and access to GPT4.0 but I chose to use GPT3.5 as that’s the one most people have access to.\n\nAct like an expert in the programming language R. I have recorded a lecture which is a walkthrough video of a tutorial in R. The learning outcomes are: Be able to reshape data between long and wide formats; Separate, change, reorder, and rename columns; Use pipes to chain together functions. Recording software has automatically transcribed the recording but there are many errors. Below is the transcript, correct the transcription errors.\n\nThe full transcript is much longer than ChatGPT can take in one response so you either have to do it bit by bit, or tell it you’re going to split the document and not to proceed until you tell it you’re finished. For the purposes of this tutorial, I just copied and pasted what I thought would fit.\nOnce it produced the corrected transcript I then asked:\n\nTell me what you changed and give me the original and the corrected version so I can easily compare them.\n\nIt gave me a list of 7 changes it had made, I won’t list them all but how did it do?\n\nOriginal: “So now that we have looked that in and will open up a new a junk…”\n\nCorrected by ChatGPT: “So now that we have loaded that in, let’s open up a new chunk…”\n\nCorrected by human: “So now that we have locked that in, I will open up a new a chunk”\n\nListening to the audio, you could sensibly conclude it was either “loaded” or “locked”, (the human doing the editing wasn’t the human doing the lecture) and both make sense in context. For the second part of the sentence, the audio is a bit disfluent, both the human and ChatGPT have corrected it to what was clearer the intent of the sentence and whilst they’re different, they both work.\n\nOriginal: “Let’s compare this to a version of this table, but in completely tiny format”\nCorrected by ChatGPT: “Let’s compare this to a version of this table but in a completely tidy format”\n\nCorrected by human: “Let’s compare this to a version of this table, but in completely tidy format.”\n\nChatGPT and the human produced the same correction.\n\nOriginal:“…we now have three columns here where each column is its own variable one just four items, one just for the price per item and one just for the total price, and they are distinguished by this for scoring here.”\n\nCorrected by ChatGPT: “…we now have three columns here: one just for items, one just for the price per item, and one just for the total price, and they are distinguished by this ‘year’ column.”\n\nCorrected by human: “we now have three columns here where each column is its own variable one just for items, one just for the price per item and one just for the total price, and they are distinguished by this fourth column here - year.\n\nChatGPT has removed a few words “here each column is its own variable” which in this case doesn’t really affect the overall meaning in the context of the wider section but if it’s going to do that you’d want to make sure you’re checking each correction."
  },
  {
    "objectID": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#the-whole-truth-and-nothing-but-the-truth",
    "href": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#the-whole-truth-and-nothing-but-the-truth",
    "title": "Using ChatGPT for learning & teaching: Correcting auto-transcripts",
    "section": "The whole truth and nothing but the truth",
    "text": "The whole truth and nothing but the truth\nWhilst it gave me 7 corrections, I could see that it had made more than the initial list it gave so I followed up:\n\nThese aren’t all the changes, tell me everything you changed\n\nAnd it then produced a list of 18 changes, apologising for the oversight. So I probed further:\n\nIs this a complete list of all changes now or are there more?\n\nIt then gave me an additional extra change. Hmmm. The text file dumps the transcription into one big block of text, but Echo360 also gives the option to download the .VTT file that has it line-by-line with time-stamps e.g.:\n\n00:00:21.110 –&gt; 00:00:24.680 &lt;v Speaker 0&gt;So as usual, we’re going to start off by creating\n\n\nNOTE CONF {“raw”:[100,100,100,52,93,100,100,100,100,100]} 00:00:24.680 –&gt; 00:00:27.860 &lt;v Speaker 0&gt;a new project for this chapter so that we can\n\n\nNOTE CONF {“raw”:[100,100,94,100,100,100,100,100,100,100]} 00:00:27.860 –&gt; 00:00:29.570 &lt;v Speaker 0&gt;work through things together.\n\nI thought that maybe chunking the text a bit would help it be able to identify the changes but all it did was render the correction completely useless. Splitting up the text with the time stamps appears to stop it being able to parse it properly which is interesting in a way that makes me realise I’ve got no idea what’s going on under the hood."
  },
  {
    "objectID": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#no-single-truth",
    "href": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#no-single-truth",
    "title": "Using ChatGPT for learning & teaching: Correcting auto-transcripts",
    "section": "No single truth",
    "text": "No single truth\nMy internet acted up and I couldn’t access the chat I was having for this blog so I redid the prompt in a new chat, with the same prompt and section of the script.\nThis time it gave me 33 changes. Some of them were the same, some of them were different. Which is not surprising because that’s how ChatGPT works, it’s all prediction and you can use the regenerate response option to get a slightly different version if you’re not happy with whatever it has produced. But in the context of transcription, it’s a really useful reminder that it isn’t “correcting” it, it’s doing what it normally does which is predicting what word should come next. It doesn’t have the source audio, it’s not doing what an underpaid human would be doing.\nIt’s possible that had I initially asked it “give me a complete list of all changes, leave nothing out”, it wouldn’t have missed any. But also, it is well-known that you can “trick” ChatGPT into thinking it’s wrong just by telling it that it is:\n\nSo it could be that my follow-up prompts insisting it had missed something resulted in it making up new prompts to satisfy the monkey at its typewriter. In a nutshell, you can’t use ChatGPT to verify what ChatGPT has produced. The snake will eat its own tail.\nMore worryingly, in additional attempts both with 3.5 and GPT4.0, it started editing more than you’d want for a transcription correction. For example:\n\nBefore we go any further here, I’m going to to switch to year. As you can see, we’re going to switch to this year. So hopefully you are now seeing my internet browser,\n\nConsulting the audio, this should be “I’m going to switch to share, as you can see, I’m going to switch to this here. So hopefully you are now seeing my internet browser”. It’s not a sentence that makes a great deal of sense without the video (which is describing changing what is being shared on the screen) but that’s what you’d want the transcript to say because alongside the video it does make sense.\nThis is what ChatGPT changed it to:\n\nBefore we delve deeper, I’m going to switch screens. You should now see my internet browser.\n\nWhich makes a lot more sense except for the fact it doesn’t actually represent what was said."
  },
  {
    "objectID": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#maybe-were-asking-the-wrong-questions",
    "href": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#maybe-were-asking-the-wrong-questions",
    "title": "Using ChatGPT for learning & teaching: Correcting auto-transcripts",
    "section": "Maybe we’re asking the wrong questions",
    "text": "Maybe we’re asking the wrong questions\nThis feels like a task ChatGPT should be able to perform so I became slightly obsessed and starting trying different prompts, convinced that maybe the issue was that I wasn’t being specific enough:\n\nAct like a video editor who is an expert in the programming language R who has been asked to correct a transcript for the subtitles of a recorded lecture which is a walkthrough video of a tutorial in R.\n\n\nThe learning outcomes are: Be able to reshape data between long and wide formats; Separate, change, reorder, and rename columns; Use pipes to chain together functions.\n\n\nRecording software has automatically transcribed the recording but there are many errors where the transcription software has not accurately assessed what word has been said. Below is the transcript, edit all words that are likely to be transcription errors so that they can be used as subtitles. Do not edit anything that is not likely to be an error and do not paraphrase or change the meaning.\n\nThis seemed to keep to the brief of not changing the meaning a lot better although it was perhaps a little too conservative (but if the option is change too much or too little, perhaps that’s for the best). Additionally it didn’t get everything right (e.g., 2 and 4 aren’t right but I suppose they’re no more wrong than the original automated transcript so it is at least not changing things that aren’t wrong)."
  },
  {
    "objectID": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#is-this-any-use",
    "href": "post/2023-07-23-using-chatgpt-for-learning-teaching-correcting-auto-transcripts/index.html#is-this-any-use",
    "title": "Using ChatGPT for learning & teaching: Correcting auto-transcripts",
    "section": "Is this any use?",
    "text": "Is this any use?\nThe question is then, given all these issues, is this any use? The edits it produced on my first attempt were really very impressive and reading through the edited transcript, it all made sense and I was getting very excited. But as I kept going I got more and more cautious. In some cases it’s not necessarily problematic that it wasn’t a one-to-one correction, the human also made some choices that deviated from an exact script to make it make sense, but without a lot of work on the prompt in some cases ChatGPT was paraphrasing way beyond the original intent and meaning. I was forced to remind myself that it’s not “correcting” words and it doesn’t have access to the audio. Additionally, it’s very difficult to get it to tell you everything it changed so you absolutely couldn’t use this without verifying it.\nOn my first attempt, the amount it got right would hugely cut-down on the time it takes to correct a transcript and a more specific prompt seemed to solve some of the issues with paraphrasing. It was certainly still better than the automated transcript, so one possible option could be to take the original, run it through ChatGPT, and then get a human to correct the ChatGPT version. That way, you make the workload more manageable, but you still have human eyes on it.\nI think whether or not it is worth it probably depends on how much transcription you have to do. If you have hours and hours of recorded content to transcribe then it is probably worth training ChatGPT to do exactly what you want and to take the time to build the prompts and find a balance you’re happy with because in the long-run it will still save huge amounts of time. However, if I had a single video, I’m not sure I would currently bother as it probably takes enough work to get it right than it does just to do it manually.\nThis blog feels like a stream-of-consciousness but what this process has done is change the way I would approach using ChatGPT to edit anything. I’m currently working on guidance for essay writing for students and I think my experience here has taught me that I wouldn’t ask it to edit anything directly but instead to give suggestions alongside the original. For the purposes of transcription correction, that process makes it time-consuming but for an essay or any other piece of writing, it would ensure you’re making active choices.\nAnother consideration is privacy. If you upload your transcripts, you’re essentially giving OpenAI your lecture to help train its LLM unless you change the default settings. Whether you care about that is up to you, but make a conscious choice.\nAnd finally, none of this changes the fact that the problem with the workload involved with transcription will still be higher for people working in their second language and those who have regional accents and that academic workload modelling is a complete joke."
  },
  {
    "objectID": "post/2024-04-17-psychology-of-swearing-uws/index.html",
    "href": "post/2024-04-17-psychology-of-swearing-uws/index.html",
    "title": "Psychology of Swearing UWS",
    "section": "",
    "text": "This website accompanies the Psychology of Swearing talk held on Friday 19th April at UWS."
  },
  {
    "objectID": "post/2024-04-17-psychology-of-swearing-uws/index.html#reading-list",
    "href": "post/2024-04-17-psychology-of-swearing-uws/index.html#reading-list",
    "title": "Psychology of Swearing UWS",
    "section": "Reading list",
    "text": "Reading list\nIf you would like more information about anything presented in this talk, I recommend the following sources:\nBooks\n\nBenjamin Bergen – What the F: What Swearing Reveals About Our Language, Our Brains, and Ourselves. Amazon link.\nSteven Pinker - The Stuff of Thought. Amazon link.\nEmma Byrne - Swearing is Good For You. Amazon link.\n\nWebsites\nhttps://stronglang.wordpress.com/\nhttps://twitter.com/stronglang\nJournal papers & pre-prints\nBeattie, H., Robertson, O., Horlin, C., Spaeth, E., & Nordmann, E. (2022). Gendered attitudes towards swearing: a trans-inclusive mixed methods study of social identity, swearing norms, and behaviour.Preprint\nFeldman, G., Lian, H., Kosinski, M., & Stillwell, D. (2017) Frankly, we do give a damn: The relationship between profanity and honesty. Social Psychological and Personality Science\nHarris, C. L., Aycicegi, A., & Gleason, J. B. (2003). Taboo words and reprimands elicit greater autonomic reactivity in a first language than in a second language. Applied Psycholinguistics, 24(04), 561-579.\nJay, T. (2009). The utility and ubiquity of taboo words. Perspectives on Psychological Science, 4(2), 153-161.\nJay, T., & Janschewitz, K. (2008). The pragmatics of swearing. Journal of Politeness Research. Language, Behaviour, Culture, 4(2), 267-288.\nJay, K. L., & Jay, T. B. (2013). A child’s garden of curses: A gender, historical, and age-related evaluation of the taboo lexicon. The American Journal of Psychology, 126(4), 459-475.\nJay, K. L., & Jay, T. B. (2015). Taboo word fluency and knowledge of slurs and general pejoratives: deconstructing the poverty-of-vocabulary myth. Language Sciences, 52, 251-259.\nMackay, D. G., Shafto, M., Taylor, J. K., Marian, D. E., Abrams, L., & Dyer, J. R. (2004). Relations between emotion, memory, and attention: Evidence from taboo Stroop, lexical decision, and immediate memory tasks. Memory & Cognition, 32(3), 474-488.\nStephens, R., Atkins, J., & Kingston, A. (2009). Swearing as a response to pain. Neuroreport, 20(12), 1056-1060.\nVan Lancker, D., & Cummings, J. L. (1999). Expletives: Neurolinguistic and neurobehavioral perspectives on swearing. Brain research reviews, 31(1), 83-104."
  },
  {
    "objectID": "post/2024-04-17-psychology-of-swearing-uws/index.html#relationship-between-age-and-vocabulary",
    "href": "post/2024-04-17-psychology-of-swearing-uws/index.html#relationship-between-age-and-vocabulary",
    "title": "Psychology of Swearing UWS",
    "section": "Relationship between age and vocabulary",
    "text": "Relationship between age and vocabulary\nThe correlation between age and animals is r = -0.23, p = 0.129.\nThe correlation between age and swears is r = -0.12, p = 0.323.\nThe correlation between swears and animals is r = 0.3, p = 0.036."
  },
  {
    "objectID": "post/2024-04-17-psychology-of-swearing-uws/index.html#vocabulary-scores-by-gender",
    "href": "post/2024-04-17-psychology-of-swearing-uws/index.html#vocabulary-scores-by-gender",
    "title": "Psychology of Swearing UWS",
    "section": "Vocabulary scores by gender",
    "text": "Vocabulary scores by gender\n\nGeneral fluency\n\n\n\n\n\ngender\naverage_score\n\n\n\n\nMan\n9.30\n\n\nWoman\n10.39\n\n\nNonbinary\n12.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwearing\n\n\n\n\n\ngender\naverage_score\n\n\n\n\nMan\n7.00\n\n\nWoman\n6.61\n\n\nNonbinary\n8.50"
  },
  {
    "objectID": "post/2024-04-17-psychology-of-swearing-uws/index.html#most-frequent-swear-words",
    "href": "post/2024-04-17-psychology-of-swearing-uws/index.html#most-frequent-swear-words",
    "title": "Psychology of Swearing UWS",
    "section": "Most frequent swear words",
    "text": "Most frequent swear words"
  },
  {
    "objectID": "post/2024-04-17-psychology-of-swearing-uws/index.html#word-cloud",
    "href": "post/2024-04-17-psychology-of-swearing-uws/index.html#word-cloud",
    "title": "Psychology of Swearing UWS",
    "section": "Word cloud",
    "text": "Word cloud"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html",
    "href": "post/2025-09-25-corbetts/corbetts.html",
    "title": "Corbetts",
    "section": "",
    "text": "The nights are fair drawing in and finding Munros we can do in the available daylight and weather conditions becomes more and more difficult, particularly given we’re also unwilling to wake up before 7am. So it’s Corbett bagging season!\nThis is the best and only excuse I have for why I have spent hours on a sequel to my Munro analysis. I won’t go into as much detail as to the approach and the coding problems I faced - they are much the same as the Munro version. I will just drop two notes before we get to the graphs:"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#data-sources",
    "href": "post/2025-09-25-corbetts/corbetts.html#data-sources",
    "title": "Corbetts",
    "section": "Data sources",
    "text": "Data sources\nAs in my Munro adventure, the Corbett analysis combines two sources of data - the Database of British and Irish Hills v18.2 and walkhighlands. As before, I have walkhighlands permission and as before, if you use walkhighlands, you should give them some money.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(fuzzyjoin)\nlibrary(ggthemes)\nlibrary(ggridges)\nlibrary(flextable)\nlibrary(janitor)\nlibrary(stringi)\nlibrary(tidytext)\nlibrary(sf)       \nlibrary(plotly)\nlibrary(rnaturalearth)\n\n# read in the walkhighlands data\n\nwalkhighlands &lt;- read_csv(\"walkhighlands_corbetts.csv\") |&gt;\n  rename(\"name\" = \"corbett\")\n\n\n# read in the dobih and just select the columns i need and rename some stuff\nraw_data &lt;- read_csv(\"https://www.hills-database.co.uk/corbettab_v4.csv\")\n\nscottish_corbetts &lt;- raw_data |&gt;\n  filter(`Post 1997` == \"CORBETT\") |&gt;\n  select(\n    `DoBIH Number`, Name,\n    `Height (m)`, xcoord, ycoord, \"Grid Ref\",\n  ) |&gt;\n  drop_na(`DoBIH Number`) |&gt; \n  rename(\n    name = \"Name\",\n    height = `Height (m)`,\n    number = `DoBIH Number`,\n    grid_ref = \"Grid Ref\"\n  ) \nrm(raw_data)\n\n# rename some of the Corbets to facilitate the join\n\nscottish_corbetts &lt;- scottish_corbetts %&gt;%\n  mutate(corbett = case_when(\n    name == \"Stob a'Choin\" ~ \"Stob a' Choin\",\n    name == \"Beinn Stacach [Ceann na Baintighearna] [Stob Fear-tomhais] [Beinn Stacath]\" ~ \"Beinn Stacath\",\n    name == \"Beinn a'Choin\" ~ \"Beinn a' Choin\",\n    name == \"Stob Coire Creagach [Binnein an Fhidhleir]\" ~ \"Binnein an Fhìdhleir (or Stob Coire Creagach)\",\n    name == \"Sron a'Choire Chnapanich [Sron a'Choire Chnapanaich]\" ~ \"Sron a' Choire Chnapanich\",\n    name == \"Beinn a'Chaisteil\" & height == 886 ~ \"Beinn a' Chaisteil (Auch)\",\n    name == \"Beinn a'Chaisteil\" & height == 787 ~ \"Beinn a' Chaisteil (Strath Vaich)\",\n    name == \"Beinn a'Chrulaiste\" ~ \"Beinn a' Chrùlaiste\",\n    name == \"Beinn a'Bhuiridh\" ~ \"Beinn a' Bhuiridh\",\n    name == \"Beinn a'Chuallaich\" ~ \"Beinn a' Chuallaich\",\n    name == \"A'Chaoirnich [Maol Creag an Loch]\" ~ \"Maol Creag an Loch (A' Chaoirnich)\",\n    name == \"Meall a'Bhuachaille\" ~ \"Meall a' Bhuachaille\",\n    name == \"Carn a'Chuilinn\" ~ \"Càrn a' Chuilinn\",\n    name == \"Sgorr Craobh a'Chaorainn\" ~ \"Sgòrr Craobh a' Chaorainn\",\n    name == \"Stob Coire a'Chearcaill\" ~ \"Stob Coire a' Chearcaill\",\n    name == \"Druim nan Cnamh [Beinn Loinne]\" ~ \"Beinn Loinne\",\n    name == \"Sgurr a'Choire-bheithe\" ~ \"Sgùrr a' Choire-bheithe\",\n    name == \"Bidein a'Chabair\" ~ \"Bidein a' Chabair\",\n    name == \"Meall a'Phubuill\" ~ \"Meall a' Phùbuill\",\n    name == \"Carn a'Choire Ghairbh\" ~ \"Càrn a' Choire Ghairbh\",\n    name == \"Sgurr a'Mhuilinn\" ~ \"Sgùrr a' Mhuilinn\",\n    name == \"Beinn a'Bha'ach Ard [Beinn a'Bhathaich Ard]\" ~ \"Beinn a' Bha'ach Ard\",\n    name == \"Meall a'Ghiubhais [Meall a'Ghiuthais]\" ~ \"Meall a' Ghiubhais\",\n    name == \"Sgurr a'Chaorachain\" ~ \"Sgùrr a' Chaorachain\",\n    name == \"Beinn a'Chlaidheimh\" ~ \"Beinn a' Chlaidheimh\",\n    name == \"Beinn a'Chaisgein Mor\" ~ \"Beinn a' Chaisgein Mòr\",\n    name == \"Beinn Liath Mhor a'Ghiubhais Li [Beinn Liath Mhor a'Ghiuthais]\" ~ \"Beinn Liath Mhòr a' Ghiubhais Li\",\n    name == \"Foinaven [Foinne Bhein] - Ganu Mor\" ~ \"Foinaven\",\n    name == \"Ben Loyal - An Caisteal\" ~ \"Ben Loyal\",\n    name == \"Quinag - Sail Gorm [Sail Ghorm]\" ~ \"Quinag - Sàil Ghorm\",\n    name == \"Glamaig - Sgurr Mhairi\" ~ \"Glamaig\",\n    name == \"Goatfell [Goat Fell]\" ~ \"Goat Fell\",\n    name == \"An Cliseam [Clisham]\" ~ \"Clisham\",\n    name == \"Carn Dearg\" & height == 817 ~ \"Càrn Dearg North Eachach\",\n    name == \"Carn Dearg\" & height == 768 ~ \"Càrn Dearg South Eachach\",\n    name == \"Carn Dearg\" & height == 834 ~ \"Càrn Dearg - Glen Roy\",\n    name == \"The Cobbler [Ben Arthur]\" ~ \"The Cobbler\",\n\n    name == \"The Sow of Atholl [Meall an Dobharchain]\" ~ \"The Sow of Atholl\",\n    name == \"Druim Tarsuinn [Stob a'Bhealach an Sgriodain]\" ~ \"Druim Tarsuinn\",\n    name == \"Ben Aden [Beinn an Aodainn]\" ~ \"Ben Aden\",\n    name == \"Beinn Pharlagain [Ben Pharlagain - Meall na Meoig]\" ~ \"Beinn Pharlagain\",\n    TRUE ~ name\n  ))\n\n\nwalkhighlands &lt;- walkhighlands |&gt;\n  mutate(name = case_when(name == \"Càrn Dearg (North of Gleann Eachach)\" ~ \"Càrn Dearg North Eachach\",\n                             name == \"Càrn Dearg (South of Gleann Eachach)\" ~ \"Càrn Dearg South Eachach\",\n                              \n                              TRUE ~ name))\n\n# Normalise names for join\nwalkhighlands &lt;- walkhighlands %&gt;%\n  mutate(\n    corbett_key = name %&gt;%\n      str_replace_all(\"\\\\s*\\\\([^)]*\\\\)\", \"\") %&gt;%\n      stri_trans_general(\"Latin-ASCII\") %&gt;%\n      str_to_lower() %&gt;%\n      str_squish(),\n    height = as.numeric(height)\n  )\n\nscottish_corbetts &lt;- scottish_corbetts %&gt;%\n  mutate(\n    corbett_key = corbett %&gt;%\n      str_replace_all(\"\\\\s*\\\\([^)]*\\\\)\", \"\") %&gt;%\n      stri_trans_general(\"Latin-ASCII\") %&gt;%\n      str_to_lower() %&gt;%\n      str_squish(),\n    height = as.numeric(height)\n  )\n\n# --- Fuzzy INNER JOIN (only matches kept) ---\ntol_m &lt;- 10\n\ncorbett_dat &lt;- fuzzy_inner_join(\n  walkhighlands,\n  scottish_corbetts,\n  by = c(\"corbett_key\" = \"corbett_key\", \"height\" = \"height\"),\n  match_fun = list(`==`, function(a, b) abs(a - b) &lt;= tol_m)\n) %&gt;%\n  rename_with(~ str_replace(.x, \"\\\\.x$\", \"_wh\")) %&gt;%\n  rename_with(~ str_replace(.x, \"\\\\.y$\", \"_dobih\")) %&gt;%\n  mutate(height_diff = abs(height_wh - height_dobih),\n         time = (time_hours_min +time_hours_max)/2,\n         type = \"corbett\") |&gt;\n  select(type, \"name\" = \"name_wh\",\n         region,\n         height_wh, height_dobih, height_diff, time,\n         first_route_title,, everything(), -name_dobih, -corbett_key_wh, -corbett_key_dobih, -corbett)|&gt;\n  mutate(scramble_exposed = case_when(\n    scramble & exposed ~ \"Both\",\n    scramble & !exposed ~ \"Scramble\",\n    !scramble & exposed ~ \"Exposed\",\n    TRUE ~ \"Neither\"\n  )) |&gt;\n  mutate(scramble_exposed = factor(scramble_exposed,\n                                   levels = c(\"Neither\", \"Scramble\", \"Exposed\", \"Both\"))) |&gt;\n    mutate(wet = case_when(\n    spate & bog ~ \"Both\",\n    spate & !bog ~ \"Large river\",\n    !spate & bog ~ \"Boggy\",\n    TRUE ~ \"Neither\"\n  )) |&gt;\n  mutate(wet = factor(wet,\n                                   levels = c(\"Neither\", \"Large river\", \"Boggy\", \"Both\")))\n\n\n# make some colour palettes\n\nnature_6 &lt;- c(\n  \"#355E3B\",  \n  \"#4B4F58\",  \n  \"#4682B4\",  \n  \"#8E6C88\",  \n  \"#E07B39\",\n  \"#BDB76B\"\n)\n\nnature_5 &lt;- c(\n  \"#355E3B\",  \n  \"#4B4F58\",  \n  \"#4682B4\",  \n  \"#E07B39\",\n  \"#BDB76B\"\n)\n\nnature_4 &lt;- c(\n  \"#355E3B\",  \n  \"#4B4F58\",  \n  \"#4682B4\",  \n  \"#E07B39\"   \n)\n\nnature_2 &lt;- c(\n  \"#355E3B\",  \n  \"#4682B4\"\n)\n\nnature_13 &lt;- c(\n  \"#355E3B\",  # Pine green\n  \"#6B8E23\",  # Moss\n  \"#BDB76B\",  # Dry grass\n  \"#8B5A2B\",  # Earth brown\n  \"#D2B48C\",  # Sand\n  \"#87CEEB\",  # Sky blue\n  \"#4682B4\",  # Loch blue\n  \"#191970\",  # Mountain shadow (midnight blue)\n  \"#7D7D7D\",  # Granite grey\n  \"#A9A9A9\",  # Slate grey\n  \"#8E6C88\",  # Heather purple\n  \"#E07B39\",  # Sunset orange\n  \"#FFD700\"   # Sun yellow\n)\n\nnature_19 &lt;- c(\n  \"#355E3B\",  # Pine green\n  \"#6B8E23\",  # Moss\n  \"#BDB76B\",  # Dry grass\n  \"#8B5A2B\",  # Earth brown\n  \"#D2B48C\",  # Sand\n  \"#87CEEB\",  # Sky blue\n  \"#4682B4\",  # Loch blue\n  \"#191970\",  # Mountain shadow (midnight blue)\n  \"#7D7D7D\",  # Granite grey\n  \"#A9A9A9\",  # Slate grey\n  \"#8E6C88\",  # Heather purple\n  \"#E07B39\",  # Sunset orange\n  \"#FFD700\",  # Sun yellow\n  \"#228B22\",  # Forest green\n  \"#B22222\",  # Red clay / bracken\n  \"#FF69B4\",  # Wildflower pink\n  \"#40E0D0\",  # Turquoise (river shallows)\n  \"#C0C0C0\",  # Silver mist\n  \"#800080\"   # Deep purple (moorland heather)\n)\n\n# create route data\nroute_dat &lt;-corbett_dat |&gt;\n  select(first_route_url, first_route_title,\n         region, time_hours_min:deer_fence, time, scramble_exposed, wet, type) |&gt;\n  unique()\n\nroute_stats &lt;- corbett_dat |&gt;\n  group_by(first_route_title) |&gt;\n  summarise(route_rating = janitor::round_half_up(mean(rating), 2),\n            route_ascents = janitor::round_half_up(mean(ascents),0 ))\n\nroute_dat &lt;- route_dat |&gt;\n  left_join(route_stats)\n\nrm(route_stats)\n\n## load in munro dat and combine it\n\nmunro_dat &lt;- read_csv(\"munros_combined.csv\")|&gt;\n  mutate(type = \"munro\",\n         ycoord = as.character(ycoord))|&gt;\n  rename(\"name\" = \"munro\")\n\ncombined_dat &lt;- bind_rows(corbett_dat, munro_dat)|&gt;\n  mutate(scramble_exposed = factor(scramble_exposed,\n                                   levels = c(\"Neither\", \n                                              \"Scramble\", \n                                              \"Exposed\", \n                                              \"Both\")))\n\nroute_dat_combined &lt;-combined_dat |&gt;\n  select(first_route_url, first_route_title,\n         region, time_hours_min:deer_fence, time, scramble_exposed, wet, type) |&gt;\n  unique()\n\nroute_stats_combined &lt;- combined_dat |&gt;\n  group_by(first_route_title) |&gt;\n  summarise(route_rating = janitor::round_half_up(mean(rating), 2),\n            route_ascents = janitor::round_half_up(mean(ascents),0 ))\n\nroute_dat_combined &lt;- route_dat_combined |&gt;\n  left_join(route_stats_combined)\n\nrm(munro_dat, route_stats_combined, walkhighlands)"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#height",
    "href": "post/2025-09-25-corbetts/corbetts.html#height",
    "title": "Corbetts",
    "section": "Height",
    "text": "Height\nIn my Munro analysis, I originally found that there were some discrepancies between the heights on walkhighlands and the DoBIH (which walkhighlands then corrected, cause they’re great, give them some money) so first, I checked if the same discrepancy existed for the Corbetts.\nThe short answer is yes. Although the differences are fewer and smaller than for the Munros. Additionally, for the Munros, walkhighlands were always larger whereas there is a mix here and more of them can be explained by rounding.\n\n\nShow code\ncorbett_dat &lt;- corbett_dat |&gt;\n  mutate(height_diff = height_wh - height_dobih)\n\nggplot(corbett_dat, aes(x = height_diff)) +\n  geom_histogram() +\n  theme_economist() +\n  scale_x_continuous(breaks = seq(-3,4, 1)) +\n  labs(y = NULL, \n       x = \"Difference in metres\", \n       title =\"Height difference between walkhighlands and DoBIH\",\n       subtitle = \"\") +\n  # labels\n    annotate(\"text\",\n           x = -2, y = 50,\n           label = \"Cnoc Coinnich\") +\n  annotate(geom = \"curve\", \n          x = -2.5, y = 40, \n          xend = -2.5, yend = 5,\n          curvature = 0.3,\n          arrow = arrow(length = unit(0.5, \"lines\")),\n          , linewidth = .75)+\n  # labels\n    annotate(\"text\",\n           x = 3, y = 60,\n           label = \"Beinn Dearg Mòr\") +\n  annotate(geom = \"curve\", \n          x = 3.5, y = 45, \n          xend = 3.72, yend = 5,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\")), linewidth = .75)"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#munros-vs-corbetts",
    "href": "post/2025-09-25-corbetts/corbetts.html#munros-vs-corbetts",
    "title": "Corbetts",
    "section": "Munros vs Corbetts",
    "text": "Munros vs Corbetts\nRather than just do the same plots for Corbetts as for the Munros I thought it would be interesting to compare the two."
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#scary-corbetts",
    "href": "post/2025-09-25-corbetts/corbetts.html#scary-corbetts",
    "title": "Corbetts",
    "section": "Scary Corbetts",
    "text": "Scary Corbetts\nFirst, I looked at what percent of route descriptions mention scrambling, exposure, both, or neither for Corbetts and Munros. I don’t think it’s particularly surprising that more Munros feature scrambling and exposure but I don’t think I expected the “neither” category to be quite so high for the Corbetts.\n\n\nShow code\ncombined_dat |&gt;\n  group_by(type) |&gt;\n  count(scramble_exposed) |&gt;\n  mutate(percent = round_half_up(n/sum(n)*100, 0)) |&gt;\n  ungroup() |&gt;\n  select(-n) |&gt;\n  pivot_wider(names_from = scramble_exposed, values_from = percent) |&gt;\n  flextable() |&gt;\n  autofit()\n\n\nPercent of route descruptions that contain scary wordstypeNeitherScrambleExposedBothcorbett781534munro5427514\n\n\n\n\nShow code\ncombined_dat |&gt;\n  group_by(type) |&gt;\n  count(scramble_exposed) |&gt;\n  mutate(percent = round_half_up(n/sum(n)*100, 0)) |&gt;\n  ungroup() |&gt;\n  select(-n) |&gt;\n  ggplot(aes(x = scramble_exposed, y = percent, fill = type)) +\n  geom_col(position = \"dodge\") +\n  theme_economist() +\n  scale_fill_manual(values = nature_2) +\n  labs(x = NULL, y = NULL, fill = NULL, \n       title = \"Percent of route descriptions that mention scary features\") +\n    theme(\n    legend.position = \"inside\",\n   legend.position.inside = c(0.9, 0.85),\n    legend.text = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\nThere’s far fewer scary Corbetts but it’s good to know where they are - these maps work much better on a full browser than a phone.\nHave I mentioned that I dislike heights and exposure?\n\n\nShow code\ncorbett_map &lt;- corbett_dat %&gt;%\n  select(name, region, xcoord, ycoord, height_dobih, pathless, scramble_exposed, wet, toilet, pub, bothy, deer_fence, car_park) %&gt;%\n  na.omit()\n\n# Convert OSGB36 coordinates to sf object\ncorbett_sf &lt;- corbett_map %&gt;%\n  st_as_sf(coords = c(\"xcoord\", \"ycoord\"), \n           crs = 27700)  # EPSG:27700 is OSGB36 / British National Grid\n\n# Transform to WGS84 (lat/long) for easier plotting\ncorbett_lat_long &lt;- corbett_sf %&gt;%\n  st_transform(crs = 4326)\n\n# Extract coordinates for ggplot\ncorbett_coords &lt;- corbett_lat_long %&gt;%\n  mutate(\n    longitude = st_coordinates(.)[,1],\n    latitude = st_coordinates(.)[,2]\n  ) %&gt;%\n  st_drop_geometry() %&gt;%\n  arrange(-height_dobih)%&gt;%\n  mutate(pathless = factor(pathless,\n                           levels = c(FALSE, TRUE),\n                           labels = c(\"Pathed\", \"Pathless\")))\n\n\nuk_map &lt;- rnaturalearth::ne_countries(scale = \"large\", \n                                  country = \"United Kingdom\", \n                                    returnclass = \"sf\")\n\n# Step 2: Specify shape codes (16 = circle, 17 = triangle, etc.)\nshape_values &lt;- c(\n  \"Neither\" = 16,    # filled circle\n  \"Scramble\" = 17,   # filled triangle\n  \"Exposed\" = 15,    # filled square\n  \"Both\" = 18        # filled diamond\n)\n\np &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56.5, 58.6)) +\n  geom_jitter(data = corbett_coords, \n             aes(x = longitude, \n                 y = latitude, \n                 shape = scramble_exposed, \n                 colour = scramble_exposed,\n                 text = name), \n             size = 1,\n             height = .05,\n             width = .05) + \n  scale_x_continuous(breaks = NULL) +\n  scale_shape_manual(values = shape_values) +\n  scale_y_continuous(breaks = NULL) +\n  scale_colour_manual(values = nature_4) +\n  labs(title = \"Where are the scary Corbetts?\",\n       subtitle = \"Walk descriptions that reference:\",\n       colour = \"Route mentions\", shape = \"Route mentions\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#distance-and-length",
    "href": "post/2025-09-25-corbetts/corbetts.html#distance-and-length",
    "title": "Corbetts",
    "section": "Distance and length",
    "text": "Distance and length\nThese analyses are done using the route data rather the individual hills (because some routes have more than one hill). That the Munros routes are longer in both distance and time is not a surprise but in this case, I think I might have expected the difference to be larger than it is and also, the longest route on walkhighlands is a Corbett, not a Munro. Don’t underestimate the Corbetts!\n\n\nShow code\nroute_dat_combined |&gt;\n  group_by(type) |&gt;\n  summarise(distance = round(mean(distance_km),2),\n            time = round(mean(time),2)) |&gt;\n  flextable()\n\n\nMean route distance (km) and time (hours) by hill typetypedistancetimecorbett15.776.31munro17.707.63\n\n\n\n\nShow code\nroute_dat_combined |&gt;\n  group_by(type) |&gt;\n  summarise(distance = mean(distance_km),\n            time = mean(time)) |&gt;\n  pivot_longer(cols = distance:time,\n               names_to = \"measurement\", \n               values_to = \"values\") |&gt;\n  ggplot(aes(x = measurement, y = values, fill = type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = nature_2) +\n  theme_economist() +\n  scale_x_discrete(labels = c(\"Distance (km)\", \"Time (Hours)\")) +\n  scale_y_continuous(limits = c(0, 20)) +\n  labs(x = NULL, y = NULL, title = \"Average distance and time\",\n       fill = NULL)+\n  theme(strip.text = element_blank(),\n        legend.position = \"inside\",\n        legend.position.inside = c(.85, .8),\n        legend.text = element_text(size = 14))\n\n\n\n\n\n\n\n\n\nI no-one cares but these arrows took me bloody ages.\n\n\nShow code\nlabels &lt;- tibble::tribble(\n  ~type,     ~x,   ~y,   ~label,\n  \"munro\",    2,   26,  \"Meall Buidhe\",\n  \"munro\",   16,   28,  \"Lurg Mhòr\\n& Bidein a' Choire Sheasgaich\",\n  \"corbett\",  -1,   38,  \"Meall nam Maigheach\\n/ Sgùrr a' Chaorachain\",\n  \"corbett\", 15,   32,  \"Beinn a' Chlaidheimh\"\n)\n\narrows &lt;- tibble::tribble(\n  ~type,     ~x,   ~y,   ~xend, ~yend, ~curve,\n  \"munro\",    3,   23,    3,     5,    0.3,\n  \"munro\",   16,   20,   16,     5,   -0.3,\n  \"corbett\",  0,   28,    2,   5,    0.3,\n  \"corbett\", 15,   25,   15,     5,   -0.3\n)\n\nggplot(route_dat_combined, aes(x = time, fill = type)) +\n  geom_histogram(binwidth = 1, colour = \"black\") +\n  facet_wrap(~type, nrow = 2) +\n  scale_x_continuous(breaks = seq(0, 18, 1)) +\n  theme_economist() +\n  labs(x = \"Length of walk in hours\", \n       y = \"Count\", \n       title = \"Munros vs Corbetts\",\n       subtitle = \"Length of walk (hours)\",\n       fill = NULL) +\n  scale_fill_manual(values = nature_2) +\n  theme(strip.text = element_blank(),\n        legend.position = \"inside\",\n        legend.position.inside = c(.9, 1),\n        legend.text = element_text(size = 14)) +\n\n  # curves with curvature = 0.3\n  geom_curve(data = arrows %&gt;% filter(curve &gt; 0),\n             aes(x = x, y = y, xend = xend, yend = yend),\n             inherit.aes = FALSE,\n             curvature = 0.3,\n             arrow = arrow(length = unit(0.5, \"lines\")),\n             linewidth = .75) +\n\n  # curves with curvature = -0.3\n  geom_curve(data = arrows %&gt;% filter(curve &lt; 0),\n             aes(x = x, y = y, xend = xend, yend = yend),\n             inherit.aes = FALSE,\n             curvature = -0.3,\n             arrow = arrow(length = unit(0.5, \"lines\")),\n             linewidth = .75) +\n\n  # labels\n  geom_text(data = labels,\n            aes(x = x, y = y, label = label,\n                hjust = ifelse(x &gt; 10, 1, 0)),\n            inherit.aes = FALSE, size = 3.5)\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlabels &lt;- tibble::tribble(\n  ~type,     ~x,   ~y,   ~label,\n  \"munro\",    1,   18,  \"Ben Hope\\n/ Sgùrr na Banachdich\",\n  \"munro\",  42,   12,  \"An Scarsoch and\\nCàrn an Fhìdhleir\",\n  \"corbett\",  4.5,   20,  \"Beinn Luibhean\",\n  \"corbett\", 45,   15,  \"Beinn a' Chaisgein Mòr\"\n)\n\narrows &lt;- tibble::tribble(\n  ~type,     ~x,   ~y,   ~xend, ~yend, ~curve,\n  \"munro\",    5,  14,   6.75,     3,    0.3,\n  \"munro\",   40,   8,   42,     2,   -0.3,\n  \"corbett\",  4.5,  18,    4,   2,    0.3,\n  \"corbett\", 40,   13,   45,     2,   -0.3\n)\n\n\nggplot(route_dat_combined, aes(x = distance_km, fill = type)) +\n  geom_histogram(binwidth = 1, colour = \"black\") +\n  facet_wrap(~type, nrow = 2) +\n  scale_x_continuous(breaks = seq(0, 50, 5)) +\n  theme_economist() +\n  labs(x = \"Length of walk in km\", \n       y = \"Count\", \n       title = \"Munros vs Corbetts\",\n       subtitle = \"Length of walk (km)\",\n       fill = NULL) +\n  scale_fill_manual(values = nature_2) +\n  theme(strip.text = element_blank(),\n        legend.position = \"inside\",\n        legend.position.inside = c(.9, 1),\n        legend.text = element_text(size = 14)) +\n\n  # curves with curvature = 0.3\n  geom_curve(data = arrows %&gt;% filter(curve &gt; 0),\n             aes(x = x, y = y, xend = xend, yend = yend),\n             inherit.aes = FALSE,\n             curvature = 0.3,\n             arrow = arrow(length = unit(0.5, \"lines\")),\n             linewidth = .75) +\n\n  # curves with curvature = -0.3\n  geom_curve(data = arrows %&gt;% filter(curve &lt; 0),\n             aes(x = x, y = y, xend = xend, yend = yend),\n             inherit.aes = FALSE,\n             curvature = -0.3,\n             arrow = arrow(length = unit(0.5, \"lines\")),\n             linewidth = .75) +\n\n  # labels\n  geom_text(data = labels,\n            aes(x = x, y = y, label = label,\n                hjust = ifelse(x &gt; 10, 1, 0)),\n            inherit.aes = FALSE, size = 3.5)"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#pathless",
    "href": "post/2025-09-25-corbetts/corbetts.html#pathless",
    "title": "Corbetts",
    "section": "Pathless",
    "text": "Pathless\nThis is dedicated to Creag Mac Ranaich and Meall an t-Seallaidh that I bagged last weekend, and to my ankles that only just survived the pathless descent.\nA note here that this doesn’t mean the full route is pathless, just that it is mentioned at some point. Additionally, for the map, “Pathed” just means that the route did not mention the word pathless.\nAgain it’s not a huge surprise that there are more pathless Corbetts but again I didn’t expect the difference to be quite so large.\n\n\nShow code\ncombined_dat |&gt;\n  group_by(type) |&gt;\n  count(pathless) |&gt;\n  mutate(percent = round_half_up(n/sum(n)*100, 0)) |&gt;\n  ungroup() |&gt;\n  select(-n) |&gt;\n  filter(pathless == TRUE) |&gt;\n  ggplot(aes(x = \"\", y = percent, fill = type)) +   # &lt;--- \"\" instead of pathless\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = paste0(percent, \"%\")), \n            vjust = 2, color = \"black\", size = 5,\n            position = position_dodge(width = 1)) +\n  theme_economist() +\n  scale_y_continuous(limits = c(0,100)) +\n  scale_fill_manual(values = nature_2) +\n  labs(x = \"Hll type\", \n       y = \"Percent\", \n       fill = NULL, \n       title = \"Percent of Munro and Corbett routes that mention pathless\") +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.9, 0.85),\n    legend.text = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\n\n\n\nShow code\np &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56.5, 58.6)) +\n  geom_jitter(data = corbett_coords, \n             aes(x = longitude, \n                 y = latitude, \n                 colour = pathless,\n                 text = name), \n             size = 1,\n             height = .05,\n             width = .05) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  scale_colour_manual(values = nature_2) +\n  labs(title = \"Where are the pathless Corbetts?\",\n       subtitle = \"Walk descriptions that reference:\",\n       colour = NULL) +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(p, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#user-ratings",
    "href": "post/2025-09-25-corbetts/corbetts.html#user-ratings",
    "title": "Corbetts",
    "section": "User ratings",
    "text": "User ratings\nWhich region has the highest and lowest rated Corbetts (by user rating)? The bottom three are the same for the Munros but the top three are a little different - for Munros Isle of Skye, Torridon, and Fort William took the podium whereas Ullapool and Sutherland claim their place for Corbetts.\n\n\nShow code\nregion_rating_top &lt;- corbett_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_rating = mean(rating, na.rm = TRUE), \n            n = n(),\n            .groups = \"drop\") |&gt;\n  filter(n &gt;= 5)|&gt;\n  slice_max(avg_rating, n = 3)\n\n# bottom 3\n\nregion_rating_bottom &lt;- corbett_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_rating = mean(rating, na.rm = TRUE), \n            n = n(),\n            .groups = \"drop\") |&gt;\n  filter(n &gt;= 5)|&gt;\n  slice_min(avg_rating, n = 3)\n\nregion_top_bottom &lt;-bind_rows(region_rating_top, region_rating_bottom)\n\nggplot(\n  semi_join(corbett_dat, region_top_bottom, by = \"region\"),\n  aes(\n    x = rating,\n    y = fct_reorder(region, rating, .fun = mean, .desc = FALSE),\n    fill = region\n  )\n)  +\n  geom_density_ridges(\n    quantile_lines = TRUE, quantile_fun = mean,\n    vline_linetype = \"dashed\",\n    aes(colour = \"Mean height (m)\")\n  ) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_colour_manual(values = c(\"Mean height (m)\" = \"black\")) +\n  theme_economist() +\n  scale_fill_manual(values = nature_6) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Which region has the highest user rated Corbett?\",\n    colour = NULL,\n    subtitle = \"Top & bottom 3 (regions w/ 5+ Corbetts)\"\n  ) +\n  guides(fill = \"none\") +\n  theme(\n    legend.position = \"inside\",\n   legend.position.inside = c(0.87, 0.1),\n    legend.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nAnd here’s the full list in table form (the graphs only have those regions with 5 or more Corbetts, they are a lot of regions that have fewer than this):\n\n\nShow code\n#| tbl-cap: \"Mean Corbett user rating by region\"\n\ncorbett_dat |&gt;\n  group_by(region) |&gt;\n  summarise(avg_rating = mean(rating, na.rm = TRUE), \n            \"no. corbetts\" = n(),.groups = \"drop\") |&gt;\n  arrange(desc(avg_rating)) |&gt;\n  flextable() |&gt;\n  colformat_double(digits = 2)|&gt;\n  autofit()\n\n\nregionavg_ratingno. corbettsIsle of Rum4.662Isle of Arran4.554Outer Hebrides4.411Islay, Jura and Colonsay4.321Skye4.232Torridon4.1719Isle of Mull4.111Ullapool3.9217Sutherland3.9110Fort William3.6955Loch Lomond3.6017Dumfries and Galloway3.546Kintail3.5210Argyll3.4312Loch Ness3.2911Moray3.262Perthshire3.1724Cairngorms3.1224Angus3.053Borders2.951"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#bothy-plot",
    "href": "post/2025-09-25-corbetts/corbetts.html#bothy-plot",
    "title": "Corbetts",
    "section": "Bothy plot",
    "text": "Bothy plot\nI didn’t see the point in mapping all the non-natural features like I did for the Munros but given there’s quite a few long Corbett routes, a bothy plot is of use.\n\n\nShow code\nbothy_plot &lt;- ggplot() +\n  geom_sf(data = uk_map, fill = \"lightgray\", color = \"darkgrey\", size = 0.3) +\n  coord_sf(xlim = c(-8, -1.5), ylim = c(56, 58.6)) +\n  geom_jitter(data = filter(corbett_coords, bothy == TRUE), \n             aes(x = longitude, \n                 y = latitude, \n                 colour = bothy,\n                 text = name), \n             size = 1) + \n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  labs(title = \"Corbett routes that mention a bothy\") +\n  guides(colour = \"none\") +\n  theme_economist() +\n  theme(\n    axis.text = element_blank(),      \n    axis.ticks = element_blank(),     \n    axis.title = element_blank(),     \n    panel.grid = element_blank(),     \n    panel.border = element_blank(),\n    legend.text = element_text(size = 10)\n  )\n\nggplotly(bothy_plot, tooltip = \"text\")"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#ascents-by-rating",
    "href": "post/2025-09-25-corbetts/corbetts.html#ascents-by-rating",
    "title": "Corbetts",
    "section": "Ascents by rating",
    "text": "Ascents by rating\nNumber of ascents (how many people have recorded on walkhighlands that they have bagged a particular Corbett ) by ratings\n\n\nShow code\nggplot(route_dat, aes(x = route_ascents, y = route_rating)) +\n  geom_point(aes(text = first_route_url)) +\n  theme_economist() +\n  scale_y_continuous(breaks = seq(1:5)) +\n  scale_x_continuous(breaks = seq(0, 12000, 2000))+\n  coord_cartesian(ylim = c(1,5)) +\n  labs(y = \"User rating (1-5)\",\n       x = \"Number of recorded ascents\",\n       title = \"Number of ascents by rating\")+\n  annotate(geom = \"curve\", \n          x = 10500, y = 3, \n          xend = 11437, yend = 4.4,\n          curvature = 0.3,\n          arrow = arrow(length = unit(0.5, \"lines\")),\n          linewidth = .75) +\n    annotate(\"text\",\n           x = 10500, y = 2.8,\n           label = \"The Cobbler \") +\n    annotate(geom = \"curve\", \n          x = 9200, y = 4.6, \n          xend = 8976, yend = 4.1,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\")),\n          linewidth = .75) +\n    annotate(\"text\",\n           x = 8976, y = 4.8,\n           label = \"Ben Ledi\")+\n    annotate(geom = \"curve\", \n          x = 7329, y = 3.5, \n          xend = 7329, yend = 3.9,\n          curvature = -0.3,\n          arrow = arrow(length = unit(0.5, \"lines\")),\n          linewidth = .75) +\n    annotate(\"text\",\n           x = 7329, y = 3.3,\n           label = \"Ben Vrackie\")+\n    annotate(geom = \"curve\", \n          x = 6000, y = 4.6, \n          xend = 6800, yend = 4.4,\n          curvature = 0.3,\n          arrow = arrow(length = unit(0.5, \"lines\")),\n          linewidth = .75) +\n    annotate(\"text\",\n           x = 6000, y = 4.7,\n           label = \"Goat Fell\")"
  },
  {
    "objectID": "post/2025-09-25-corbetts/corbetts.html#rating-by-feature",
    "href": "post/2025-09-25-corbetts/corbetts.html#rating-by-feature",
    "title": "Corbetts",
    "section": "Rating by feature",
    "text": "Rating by feature\nIt’s not a surprise to me that scrambling routes are higher rated, but it is a surprise that across the board, Corbett routes are rated higher than Munros. My suspicion is that it is a combination of more people rating the Munros, and the enthusiasm for hills of the people who are likely to walk them.\n\n\nShow code\nroute_dat_combined |&gt;\n  group_by(type, pathless) |&gt;\n  summarise(rating = mean(route_rating))\n\n\n# A tibble: 4 × 3\n# Groups:   type [2]\n  type    pathless rating\n  &lt;chr&gt;   &lt;lgl&gt;     &lt;dbl&gt;\n1 corbett FALSE      3.76\n2 corbett TRUE       3.54\n3 munro   FALSE      3.60\n4 munro   TRUE       3.57\n\n\nShow code\nroute_dat_combined |&gt;\n  group_by(type, scramble_exposed) |&gt;\n  summarise(rating = mean(route_rating)) |&gt;\n  ggplot(aes(x = scramble_exposed, y = rating, fill = type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = nature_2) +\n  theme_economist() +\n  labs(x = NULL, \n       y = NULL, \n       fill = NULL, \n       title = \"User ratings by scary features\") +\n  coord_cartesian(ylim = c(0,5))+\n    theme(\n    legend.position = \"inside\",\n   legend.position.inside = c(0.1, 0.85),\n    legend.text = element_text(size = 14)\n  )"
  }
]
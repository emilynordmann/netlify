---
title: 'Using ChatGPT for learning & teaching: Correcting auto-transcripts'
author: Emily Nordmann
date: '2023-07-23'
slug: using-chatgpt-for-learning-teaching-correcting-auto-transcripts
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2023-07-23T16:02:09+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  markdown: 
    wrap: 72
---

Alongside Wil Tovio and Rachel O'Neill, I've written before in the
[Times
Higher](https://www.timeshighereducation.com/campus/new-rules-lecture-transcripts-give-academics-impossible-choice)
about the problems that requiring academics to produce corrected
captions introduces. If you don't do it, you disadvantage a wide range
of students and effectively ruin the education of those who are d/Deaf
and hard-of-hearing. If you do, you put a much higher workload burden on
any lecturer with a "non-standard" accent, those with poorer quality
recording equipment, and let's honest, those who just care a bit more. I
say all of this whilst recognising my own privilege: most recording
software generally picks up what I am saying accurately and through both
work and personal means, I invested in a high-quality mic at the start
of covid.

## Correcting transcripts

But anyway. It just occurred to me that one use of ChatGPT that falls
under the category "let's make the admin associated with teaching
easier" is that we might be able to use it to edit transcriptions for
us. And for once, I actually have the means to evaluate how it's done in
the form of an original automated-transcription, the corrected version a
human spent hours on, and the audio. The video in question was a
walkthrough of a data skills tutorial in R, and platforms like Zoom and
Echo360 tend to struggle with the technical terminology.

I downloaded the original transcript as a .txt file from Echo360 and put
the following prompt into ChatGPT. I've got a paid subscription and
access to GPT4.0 but I chose to use GPT3.5 as that's the one most people
have access to.

> Act like an expert in the programming language R. I have recorded a
> lecture which is a walkthrough video of a tutorial in R. The learning
> outcomes are: Be able to reshape data between long and wide formats;
> Separate, change, reorder, and rename columns; Use pipes to chain
> together functions. Recording software has automatically transcribed
> the recording but there are many errors. Below is the transcript,
> correct the transcription errors.

The full transcript is much longer than ChatGPT can take in one response
so you either have to do it bit by bit, or tell it you're going to split
the document and not to proceed until you tell it you're finished. For
the purposes of this tutorial, I just copied and pasted what I thought
would fit.

Once it produced the corrected transcript I then asked:

> Tell me what you changed and give me the original and the corrected
> version so I can easily compare them.

It gave me a list of 7 changes it had made, I won't list them all but
how did it do?

-   **Original**: "So now that we have looked that in and will open up a
    new a junk..."\
-   **Corrected by ChatGPT:** "So now that we have loaded that in, let's
    open up a new chunk..."\
-   **Corrected by human:** "So now that we have locked that in, I will
    open up a new a chunk"

Listening to the audio, you could sensibly conclude it was either
"loaded" or "locked", (the human doing the editing wasn't the human
doing the lecture) and both make sense in context. For the second part
of the sentence, the audio is a bit disfluent, both the human and
ChatGPT have corrected it to what was clearer the intent of the sentence
and whilst they're different, they both work.

-   **Original**: "Let's compare this to a version of this table, but in
    completely tiny format"
-   **Corrected by ChatGPT**: "Let's compare this to a version of this
    table but in a completely tidy format"\
-   **Corrected by human:** "Let's compare this to a version of this
    table, but in completely tidy format."

ChatGPT and the human produced the same correction.

-   **Original**:"...we now have three columns here where each column is
    its own variable one just four items, one just for the price per
    item and one just for the total price, and they are distinguished by
    this for scoring here."\
-   **Corrected by ChatGPT**: "...we now have three columns here: one
    just for items, one just for the price per item, and one just for
    the total price, and they are distinguished by this 'year' column."\
-   **Corrected by human:** "we now have three columns here where each
    column is its own variable one just for items, one just for the
    price per item and one just for the total price, and they are
    distinguished by this fourth column here - year.

ChatGPT has removed a few words "here each column is its own variable"
which in this case doesn't really affect the overall meaning in the
context of the wider section but if it's going to do that you'd want to
make sure you're checking each correction.

## The whole truth and nothing but the truth

Whilst it gave me 7 corrections, I could see that it had made more than
the initial list it gave so I followed up:

> These aren't all the changes, tell me everything you changed

And it then produced a list of 18 changes, apologising for the
oversight. So I probed further:

> Is this a complete list of all changes now or are there more?

It then gave me an additional extra change. Hmmm. The text file dumps
the transcription into one big block of text, but Echo360 also gives the
option to download the .VTT file that has it line-by-line with
timestamps e.g.:

> 00:00:21.110 --\> 00:00:24.680 \<v Speaker 0\>So as usual, we're going
> to start off by creating

> NOTE CONF {"raw":[100,100,100,52,93,100,100,100,100,100]} 00:00:24.680
> --\> 00:00:27.860 \<v Speaker 0\>a new project for this chapter so
> that we can

> NOTE CONF {"raw":[100,100,94,100,100,100,100,100,100,100]}
> 00:00:27.860 --\> 00:00:29.570 \<v Speaker 0\>work through things
> together.

I thought that maybe chunking the text a bit would help it be able to
identify the changes but all it did was render the correction completely
useless. Splitting up the text with the time stamps appears to stop it
being able to parse it properly which is interesting in a way that makes
me realise I've got no idea what's going on under the hood.

## No single truth

My internet acted up and I couldn't access the chat I was having for this blog so I redid the prompt in a new chat, with the same prompt and section of the script (I think, I'll be honest, I just selected a bunch of text I thought wasn't longer that it could cope with each time so my first attempt may have been a little more or less than the second).

This time it gave me 33 changes. Some of them were the same, some of
them were different. Which is not surprising because that's how ChatGPT
works, it's all prediction and you can use the regenerate response
option to get a slightly different version if you're not happy with
whatever it has produced. But in the context of transcription, it's a
really useful reminder that it isn't "correcting" it, it's doing what it
normally does which is predicting what word should come next. It doesn't
have the source audio, it's not doing what an underpaid human would be
doing, instead it's making a best guess.

It's possible that had I initially asked it "give me a complete list of
all changes, leave nothing out", it wouldn't have missed any. But also,
it is well-known that you can "trick" ChatGPT into thinking it's wrong
just by telling it that it is:

![](images/Screenshot%202023-07-23%20172451.png)

So it could be that my follow-up prompts insisting it had missed
something resulted in it making up new prompts to satisfy the monkey at
its typerwriter. In a nutshell, you can't use ChatGPT to verify what
ChatGPT has produced. The snake will eat it's own tail.

I also tried using GPT4.0 to see if it was any better and it started
editing more than you'd want for a transcription correction. For
example:

> Before we go any further here, I'm going to to switch to year. As you can see, we're going to switch to this year. So hopefully you are now seeing my internet browser,

Consulting the audio, this should be "I'm going to switch to share, as you can see, I'm going to switch to this here. So hopefully you are now seeing my internet browser". It's not a sentence that makes a great deal of sense without the video (which is describing changing what is being shared on the screen) but that's what you'd want the transcript to say because alongside the video it does make sense.

This is what ChatGPT changed it to:

> Before we delve deeper, I'm going to switch screens. You should now see my internet browser. 

Which makes a lot more sense except for the fact it doesn't actually represent what was said.

## Evaluation

The question is then, given all these issues, is this any use? The edits
it produced on my first attempt were really very impressive and reading
through the edited transcript, it all made sense and I was getting very excited. But as I kept going I got more and more cautious. I don't think it's necessarily problematic that it wasn't a one-to-one transcript, the human had also made some choices that deviated from an exact script to make it make sense, but in some cases ChatGPT was paraphrasing way beyond the original intent and meaning. I was forced to remind myself that it's not "correcting" words and it doesn't have access to the audio. Additionally, it's very difficult to get it to tell you everything it changed so you absolutely couldn't use this without verifying it.

On my first attempt, the amount it got right would hugely cut-down on the time it takes to correct a transcript and it was certainly still a lot better than the automated transcript, so one possible option could be to take the original, run it through ChatGPT, and then get a human to correct the ChatGPT version. That way, you make the workload more manageable, but you still have human eyes on it. 

However, I've now tried 5 times to replicate the success of the original edit it gave me using similar and different prompts and I can't get it to do it that well again. It might be that I am not giving it the right prompts and that a more refined request would replicate the initial approach more reliably in which case I think it would be a genuine solution to reducing the workload involved. But the versions it was giving me that were more heavily paraphrased are arguably of little use because when they go really off-script they aren't going to align with the audio so you'd be having to correct it a lot which means it might not save any more time from starting from the raw automated transcript.

Another consideration is privacy. If you upload your transcripts, you're
essentially giving OpenAI your lecture to help train its LLM [unless you
change the default
settings](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt).
Whether you care about that is up to you, but make a conscious choice.

And finally, none of this changes the fact that the problem with the
workload involved with transcription will still be higher for people
working in their second language and those who have regional accents and
that academic workload modelling is a complete joke.
